---
title: "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: hide
    dev: svg
    df_print: paged
    theme: journal
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r}
knitr::include_graphics("images/machine-learning-splitting-datasets-3-638.jpg")
```

```{r, include=FALSE}
all_data <- readr::read_csv("~/Dropbox/Nomogram/nomogram/all_data_end_of_EDA.csv")
```

# Why are we using a train and test sample data set to test the model?  
The training set contains a known output (`all_data$Match_Status`) and the model learns this data in order to be generalized to other data in the process. In this way, the model will predict values for the test data (cross validation). It is possible to determine the prediction accuracy of the model.

Overfitting is one of the biggest challenges in the machine learning process. You don’t want to build a model that fits perfectly to the data it knows the answer but doesn’t perferm well for the data that it doesn’t know the answer.  This is to prevent the over-fitting.
```{r}
knitr::include_graphics("images/overfitting.jpg")
```

Overfitting means that the model has been trained “too well”, and as a result it learns the noise present in the training data as if it was a reliable pattern. Overfitting affects the ability of the model to perform well in unseen data, which is known as generalisation.  Two well known strategies to overcome the problem of overfitting are the train/validation split and cross-validation.

# Identify training and test samples
```{r}
knitr::include_graphics("images/Train-Test-Split-Diagram.jpg")
```


Shows the data split  
```{r, train vs test, warning=FALSE, echo=TRUE, message=FALSE, include=FALSE}
#http://rpubs.com/josevilardy/crossvalidation
library(magrittr)

# WE USED THIS ONE
set.seed(seed = 1978) 
all_data$Age <- as.numeric(all_data$Age)
data_split <- rsample::initial_split(data = all_data, 
                                     strata = "Match_Status", 
                                     prop = 0.8)

train <- data_split %>% rsample::training() %>% dplyr::glimpse()  # Extract the training dataframe
readr::write_csv(x = train, 
          path = (here::here("results", "train_at_data_split_phase.csv")), 
          col_names = TRUE)
# training_data_plot_hist_facet <- plot_hist_facet(data = train)
#  training_data_plot_hist_facet

test <- data_split %>% rsample::testing() %>% dplyr::glimpse() # Extra
readr::write_csv(x = test, 
          path = (here::here("results", "test_at_data_split_phase.csv")), 
          col_names = TRUE)

# plot_hist_facet(data=test)
```

I will call the training sample `train` and the test sample `test`.  *Creative!*  There are `r nrow(train)` medical students in the training data set and `r nrow(test)` in the test data set.  The two samples should have the same proportion of students who matched and did not match.   

```{r, train vs test1, warning=FALSE, echo=TRUE, message=FALSE, include=FALSE}
#More ways to split the data.  

# Using base R
set.seed(123)  # for reproducibility
index_1 <- base::sample(1:nrow(all_data), round(nrow(all_data) * 0.8))
train_1 <- all_data[index_1, ]
test_1  <- all_data[-index_1, ]
rm(test_1)
rm(train_1)

# Using caret package
set.seed(123)  # for reproducibility
index_2 <- caret::createDataPartition(all_data$Match_Status, p = 0.8, 
                               list = FALSE)
train_2 <- all_data[index_2, ]
test_2  <- all_data[-index_2, ]
rm(train_2)
rm(test_2)

# Using rsample package
set.seed(123)  # for reproducibility
split_1  <- rsample::initial_split(all_data, prop = 0.8)
train_3  <- rsample::training(split_1)
test_3   <- rsample::testing(split_1)
rm(split_1)
rm(train_3)
rm(test_3)

# Using h2o package
# brew tap caskroom/versions
# brew cask install adoptopenjdk8
# brew tap adoptopenjdk/openjdk
# brew cask install homebrew/cask-versions/adoptopenjdk8

# h2o::h2o.init()
# all_data.h2o <- h2o::as.h2o(all_data)
# split_2 <- h2o::h2o.splitFrame(all_data.h2o, ratios = 0.8, 
#                           seed = 123)
# train_4 <- split_2[[1]]
# test_4  <- split_2[[2]]
# 
# rm(train_4)
# rm(test_4)
```

```{r, include=FALSE}
#Remove applicant names from train and test.  

# train <- train %>%
#   dplyr::select(-Applicant_Name)
# 
# test <- test %>%
#   dplyr::select(-Applicant_Name)
```


Compare the datasets of `train` and `test`using arsenal package:  
```{r, results="asis", include=TRUE, echo=FALSE}
compareddf <- summary(arsenal::comparedf(train, test))
#compareddf
# tm_write2word(compareddf, "compareddf")
# tm_write2pdf(compareddf, "compareddf")
```

# `train` data characteristics are all reported with medians and IQR.  
```{r, include = TRUE, results="asis"}
###tableby labels
mylabels <- list(white_non_white = "Race", Age = "Age, years", Gender = "Sex", Couples_Match = "Participating in the Couples Match", US_or_Canadian_Applicant = "US or Canadian Applicant", Medical_Education_Interrupted = "Medical Education Process was Interrupted", Alpha_Omega_Alpha = "Alpha Omega Alpha", Military_Service_Obligation = "Military Service Obligation", USMLE_Step_1_Score = "USMLE Step 1 Score", Military_Service_Obligation = "Military Service Obligations", Count_of_Poster_Presentation = "Count of Poster Presentations", Count_of_Oral_Presentation = "Count of Oral Presentations", Count_of_Articles_Abstracts = "Count of Published Abstracts", Count_of_Peer_Reviewed_Book_Chapter = "Count of Peer Reviewed Book Chapters", Count_of_Other_than_Published = "Count of Other Published Products", Count_of_Online_Publications = "Count of Online Publications", Visa_Sponsorship_Needed = "Visa Sponsorship is Needed", Medical_Degree = "Medical Degree Training")

tm_arsenal_table = function(df, by){
  print("Function Sanity Check: Create Arsenal Table using arsenal package")
  table_variable_within_function <- arsenal::tableby(by ~ .,
                 data=df, control = arsenal::tableby.control(test = TRUE,
                                                                total = F,
                                                                digits = 1L,
                                                                digits.p = 2L,
                                                                digits.count = 0L,
                                                                numeric.simplify = F,
                                                                numeric.stats =
                                                                  c("median",
                                                                    "q1q3"),
                                                                cat.stats =
                                                                  c("Nmiss",
                                                                    "countpct"),
                                                                stats.labels = list(Nmiss = "N Missing",
                                                                                    Nmiss2 ="N Missing",
                                                                                    meansd = "Mean (SD)",
                                                                                    medianrange = "Median (Range)",
                                                                                    median ="Median",
                                                                                    medianq1q3 = "Median (Q1, Q3)",
                                                                                    q1q3 = "Q1, Q3",
                                                                                    iqr = "IQR",
                                                                                    range = "Range",
                                                                                    countpct = "Count (Pct)",
                                                                                    Nevents = "Events",
                                                                                    medSurv ="Median Survival",
                                                                                    medTime = "Median Follow-Up")))
  final <- summary(table_variable_within_function,
          text=T,
          title = 'Table: Applicant Descriptive Variables by Matched or Did Not Match from 2015 to 2018',
          labelTranslations = mylabels, #Seen in additional functions file
          pfootnote=TRUE)
  return(final)
}

train_table_characteristics <- tm_arsenal_table(
  df=train,
  by=train$Match_Status)
train_table_characteristics
# 
# tm_write2word(train_table_characteristics, "train_table_characteristics")
# tm_write2pdf(train_table_characteristics, "train_table_characteristics")
```

# `test` data characteristics are all reported with medians and IQR. 
```{r, include = TRUE, results="asis"}
test_table_characteristics <- tm_arsenal_table(df = test, by = test$Match_Status)
test_table_characteristics

# tm_write2word(test_table_characteristics, "test_table_characteristics")
# tm_write2pdf(test_table_characteristics, "test_table_characteristics")
```

# Check Proportions of Matched/Unmatched applicants in the test and train data sets.  
Original response distribution and then showing consistent response ratio between `train` & `test` data sets.  
```{r, results="asis", echo=TRUE, include=TRUE}
# Examine the proportions of the Match_Status class lable across the datasets.
crude_summary <- 
  base::prop.table(table(all_data$Match_Status))  #Original data set proportion 

base::prop.table(table(train$Match_Status)) #Train data set proportion

base::prop.table(table(test$Match_Status))  #Test data set proportion

#knitr::kable(crude_summary, caption="2x2 Contingency Table on Matching for all_data", format="markdown")
```

# Is feature skew present?

```{r}
knitr::include_graphics("images/skewness---mean-median-mode.jpg")
```
Using the training sample, we will provide numerical summaries of each predictor variable and the outcome, as well as graphical summaries of the outcome variable. Our results should now show no missing values in any variable. We’ll need to determine whether there are any evident problems, such as substantial skew in the outcome variable.

```{r, include=FALSE}
skewed_feature_names <- train %>%
  dplyr::select_if(is.numeric) %>%
  purrr::map_df(PerformanceAnalytics::skewness) %>% #returns a single row tibble 
  tidyr::gather(factor_key = TRUE) %>% #transposes into a long data column
  dplyr::arrange(desc(value)) %>% #Look for low and high values, high values have a fat tail on right, low has fat tail on left side
  dplyr::filter(value>2.0) %>% #eyeballed cutoff for value cut off
  dplyr::pull(key) %>%
  as.character()
skewed_feature_names
```

```{r}
knitr::kable(skewed_feature_names, caption="List of skewed variables for all_data", format="html")
```

# Correlation
Get a correlation matrix for elementary feature selection and remove highly correlated features.  
```{r}
knitr::include_graphics("images/correlation1.jpg")
knitr::include_graphics("images/iu.jpg")
```
The values of correlation range from -1 to 1.  If there is a value of 0 there is no correlation between the variables.  Perfect correlation is 1.  Perfect negative correlation is -1.  

# One way to look at correlation is with a correlation funnel.
This is a great interactive plot that shows what variables are positively and negatively correlated with matching OBGYN.  The closer the dots to the zero line the less correlation there is to matching.  
```{r, include=FALSE, echo=FALSE}
all_data_binarized_tbl <- all_data %>%
  correlationfunnel::binarize(n_bins = 4, thresh_infreq = 0.01)

all_data_binarized_tbl %>% dplyr::glimpse()

all_data_correlated_tbl <- all_data_binarized_tbl %>%
  correlationfunnel::correlate(target = Match_Status__Match)
all_data_correlated_tbl

# static_correlation_funnel <- all_data_correlated_tbl %>%
#   correlationfunnel::plot_correlation_funnel(interactive = FALSE, limits = c(-1, 1))
# static_correlation_funnel

#tm_ggsave(static_correlation_funnel, "static_correlation_funnel.tiff")
```


```{r, include=TRUE, echo=FALSE}
#REALLY COOL AND INSTRUCTIVE
all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = TRUE, limits = c(-1, 1))
```

# Are there any highly correlated variables that can be removed?
No.  All the numeric variables are not correlated.  
```{r}
# get a correlation matrix for elementary feature selection and
# remove highly correlated fields
numeric_data <- dplyr::select_if(all_data, is.numeric)
thres <- 0.75
corr.m <- stats::cor(numeric_data)
highly.corr <- caret::findCorrelation(corr.m, cutoff = thres)
highly.corr
data.clean <- numeric_data[,-highly.corr]
colnames(data.clean)
```

```{r}
tm_t_test <- function(variable){
  print("Function Sanity Test: t-test")
  output <- stats::t.test(y=variable[train$Match_Status == "Match"],
                          x=variable[train$Match_Status == "Did Not Match"],
                          alternative = ("two.sided"),
                          paired = FALSE,
                          conf.level = 0.95,
                          var.equal = TRUE)
  
  #print("X is group that did not match and Y is group that did match:")
  return(output)
}

tm_t_test(train$Age)#Significant different in age for people who matched (younger) and did not match (older) shown by the t-test
#tm_t_test(train$USMLE_Step_1_Score)#Significant different in USMLE_Step_1_Score for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Poster_Presentation) #NO DIFFERENCE in Count_of_Poster_Presentation for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Oral_Presentation)#NO DIFFERENCE in Count_of_Oral_Presentation for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Peer_Reviewed_Journal_Articles_Abstracts)
tm_t_test(train$Count_of_Peer_Reviewed_Book_Chapter)#NO DIFFERENCE in Count_of_Peer_Reviewed_Book_Chapter for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Other_Articles)
```

# We compared categorical variables through the chi-square test 
The chi-squared test is a statistical test used to discover whether there is a relationship between categorical variables.
```{r}
tm_chi_square_test <- function (variable) {
  print("Function Sanity Test: chi-square test")
  chisq <- stats::chisq.test(variable, train$Match_Status, correct = FALSE)
  print(c("First five rows of variable compared to Match_Status:",(head(variable,5))))
  return(chisq)
}

tm_chi_square_test(train$white_non_white) #Race is  significant
tm_chi_square_test(train$Gender) #Gender is significant
tm_chi_square_test(train$Couples_Match)#Couples match status is significant
tm_chi_square_test(train$US_or_Canadian_Applicant)#US Applicant is significant

tm_chi_square_test(train$Medical_Education_or_Training_Interrupted)#Interrupting medical education is significant
tm_chi_square_test(train$Alpha_Omega_Alpha)#AOA is significant
tm_chi_square_test(train$Military_Service_Obligation)#Military Service obligation is NOT significant
tm_chi_square_test(train$Visa_Sponsorship_Needed)#Visa sponsorship is significant
tm_chi_square_test(train$Medical_Degree)#Medical Degree is significant
```


```{r}

# In this correlation plot we want to look for the bright, large circles which immediately show the strong correlations 

# (size and shading depends on the absolute values of the coefficients; color depends on direction).  
# This shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Anything that you would have to squint to see is usually not worth seeing! 
tm_print_save <- function (filename) {
  print("Function Sanity Check: Saving TIFF of what is in the viewer")
  dev.print(tiff, (here::here("results", filename)), compression = "lzw",width=2000, height=2000, bg="transparent", res = 200, units = "px" )
  dev.off()
}

cnum <- train[,c("Age", "Count_of_Non_Peer_Reviewed_Online_Publication", "Count_of_Oral_Presentation", "Count_of_Other_Articles", "Count_of_Peer_Reviewed_Book_Chapter", "Count_of_Peer_Reviewed_Journal_Articles_Abstracts", "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published", "Count_of_Peer_Reviewed_Online_Publication", "Count_of_Poster_Presentation", "Count_of_Scientific_Monograph", #"USMLE_Step_1_Score", 
                 "USMLE_Step_2_CK_Score", "NIH_dollars" )]
cormat <- stats::cor(cnum) # Select only numeric variables that are significant on univariate testing.
pairs <- graphics::pairs(cnum)
pairs
#tm_print_save("significant_numeric_variable_correlation.tiff")
# 
#corrplot::corrplot(cormat, method="circle")
#corrplot::corrplot(cormat, method="circle", addCoef.col="black") # With correlation
# tm_print_save("corrplot.tiff")
# rm(cnum)
# rm(cormat)
```


```{r, echo=TRUE, include = TRUE, fig.width=12, fig.height=12}
tm_ggsave <- function (object, filename, ...){  #make sure the file name has quotation marks around it.  
  print("Function Sanity Check: Saving a ggplot image as a TIFF")
  ggplot2::ggsave(here::here("results", filename), object, device = "tiff", width = 12, height = 10, dpi = 200)
}

inspect_cor_plot <- inspectdf::inspect_cor(train, method = "pearson", alpha = 0.05) %>% inspectdf::show_plot()  
inspect_cor_plot

tm_ggsave(inspect_cor_plot, "inspect_cor_plot.tiff")
```


```{r, include=FALSE}
readr::write_csv(all_data, "~/Dropbox/Nomogram/nomogram/all_data_output_of_split_correlation.csv")
readr::write_csv(train, "~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv")
readr::write_csv(test, "~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv")
```

