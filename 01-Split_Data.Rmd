---
title: "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: show
    dev: svg
    df_print: paged
    theme: journal
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---



```{r}
all_data <- readr::read_csv("~/Dropbox/Nomogram/nomogram/all_data_end_of_EDA.csv")
```

# Why are we using a train and test sample data set to test the model?  
The training set contains a known output (`all_data$Match_Status`) and the model learns this data in order to be generalized to other data in the process. In this way, the model will predict values for the test data (cross validation). It is possible to determine the prediction accuracy of the model.

Overfitting is one of the biggest challenges in the machine learning process. Overfitting means that the model has been trained “too well”, and as a result it learns the noise present in the training data as if it was a reliable pattern. Overfitting affects the ability of the model to perform well in unseen data, which is known as generalisation.

Two well known strategies to overcome the problem of overfitting are the train/validation split and cross-validation.

# Identify training and test samples


Shows four different ways to split the data  
```{r, train vs test, warning=FALSE, echo=TRUE, message=FALSE, include=TRUE}
#http://rpubs.com/josevilardy/crossvalidation
library(magrittr)

# WE USED THIS ONE
set.seed(seed = 1978) 
all_data$Age <- as.numeric(all_data$Age)
data_split <- rsample::initial_split(data = all_data, 
                                     strata = "Match_Status", 
                                     prop = 0.8)

train <- data_split %>% rsample::training() %>% dplyr::glimpse()  # Extract the training dataframe
readr::write_csv(x = train, 
          path = (here::here("results", "train_at_data_split_phase.csv")), 
          col_names = TRUE)
# training_data_plot_hist_facet <- plot_hist_facet(data = train)
# training_data_plot_hist_facet

test <- data_split %>% rsample::testing() %>% dplyr::glimpse() # Extra
readr::write_csv(x = test, 
          path = (here::here("results", "test_at_data_split_phase.csv")), 
          col_names = TRUE)

# plot_hist_facet(data=test)
```

I will call the training sample `train` and the test sample `test`.  *Creative!*  Another option is dplyr::sample_n if you want to instead specify the exact number of observations to be selected. There are `r nrow(train)` medical students in the training data set and `r nrow(test)` in the test data set.  

```{r, train vs test1, warning=FALSE, echo=TRUE, message=FALSE, include=TRUE}
#More ways to split the data.  

# Using base R
set.seed(123)  # for reproducibility
index_1 <- base::sample(1:nrow(all_data), round(nrow(all_data) * 0.8))
train_1 <- all_data[index_1, ]
test_1  <- all_data[-index_1, ]
rm(test_1)
rm(train_1)

# Using caret package
set.seed(123)  # for reproducibility
index_2 <- caret::createDataPartition(all_data$Match_Status, p = 0.8, 
                               list = FALSE)
train_2 <- all_data[index_2, ]
test_2  <- all_data[-index_2, ]
rm(train_2)
rm(test_2)

# Using rsample package
set.seed(123)  # for reproducibility
split_1  <- rsample::initial_split(all_data, prop = 0.8)
train_3  <- rsample::training(split_1)
test_3   <- rsample::testing(split_1)
rm(split_1)
rm(train_3)
rm(test_3)

# Using h2o package
# brew tap caskroom/versions
# brew cask install adoptopenjdk8
# brew tap adoptopenjdk/openjdk
# brew cask install homebrew/cask-versions/adoptopenjdk8

h2o::h2o.init()
all_data.h2o <- h2o::as.h2o(all_data)
split_2 <- h2o::h2o.splitFrame(all_data.h2o, ratios = 0.8, 
                          seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]

rm(train_4)
rm(test_4)
```

```{r}
#Remove applicant names from train and test.  

# train <- train %>%
#   dplyr::select(-Applicant_Name)
# 
# test <- test %>%
#   dplyr::select(-Applicant_Name)
```


Compare the datasets of `train` and `test`using arsenal package:  
```{r, results="asis", include=FALSE, echo=FALSE}
compareddf <- summary(arsenal::comparedf(train, test))
compareddf
# tm_write2word(compareddf, "compareddf")
# tm_write2pdf(compareddf, "compareddf")
```

`train` data characteristics are all reported with medians and IQR.  
```{r, include = TRUE, results="asis"}
# train_table_characteristics <- tm_arsenal_table(
#   df=train, 
#   by=train$Match_Status)
# 
# tm_write2word(train_table_characteristics, "train_table_characteristics")
# tm_write2pdf(train_table_characteristics, "train_table_characteristics")
```

`test` data characteristics are all reported with medians and IQR. 
```{r, include = TRUE, results="asis"}
# test_table_characteristics <- tm_arsenal_table(df = test, by = test$Match_Status)
# 
# tm_write2word(test_table_characteristics, "test_table_characteristics")
# tm_write2pdf(test_table_characteristics, "test_table_characteristics")
```

Check Proportions of Matched/Unmatched applicants in the test and train data sets.  Orginal response distribution and then showing consistent response ratio between train & test data sets.  
```{r, results="asis", echo=TRUE, include=TRUE}
# Examine the proportions of the Match_Status class lable across the datasets.
crude_summary <- 
  base::prop.table(table(all_data$Match_Status))  #Original data set proportion 

base::prop.table(table(train$Match_Status)) #Train data set proportion

base::prop.table(table(test$Match_Status))  #Test data set proportion

knitr::kable(crude_summary, caption="2x2 Contingency Table on Matching for all_data", format="markdown")
```

Summarize the outcome and the predictors
Using the training sample, we will provide numerical summaries of each predictor variable and the outcome, as well as graphical summaries of the outcome variable. Our results should now show no missing values in any variable. We’ll need to determine whether there are any evident problems, such as substantial skew in the outcome variable.

```{r}
#Is feature skew present?
skewed_feature_names <- train %>%
  dplyr::select_if(is.numeric) %>%
  purrr::map_df(PerformanceAnalytics::skewness) %>% #returns a single row tibble 
  tidyr::gather(factor_key = TRUE) %>% #transposes into a long data column
  dplyr::arrange(desc(value)) %>% #Look for low and high values, high values have a fat tail on right, low has fat tail on left side
  dplyr::filter(value>2.0) %>% #eyeballed cutoff for value cut off
  dplyr::pull(key) %>%
  as.character()
```


# Correlation
Get a correlation matrix for elementary feature selection and remove highly correlated features.  
```{r}
# get a correlation matrix for elementary feature selection and
# remove highly correlated fields
numeric_data <- dplyr::select_if(all_data, is.numeric)
thres <- 0.75
corr.m <- stats::cor(numeric_data)
highly.corr <- caret::findCorrelation(corr.m, cutoff = thres)
highly.corr
data.clean <- numeric_data[,-highly.corr]
colnames(data.clean)
```
None of the numeric data are correlated to each other.  So we can't remove any numeric features because they are not highly correlated.  

```{r}
tm_t_test <- function(variable){
  print("Function Sanity Test: t-test")
  output <- stats::t.test(y=variable[train$Match_Status == "Matched"],
                          x=variable[train$Match_Status == "Did not match"],
                          alternative = ("two.sided"),
                          paired = FALSE,
                          conf.level = 0.95,
                          var.equal = TRUE)
  
  print("X is group that did not match and Y is group that did match:")
  return(output)
}

# tm_t_test(train$Age)#Significant different in age for people who matched (younger) and did not match (older) shown by the t-test
# tm_t_test(train$USMLE_Step_1_Score)#Significant different in USMLE_Step_1_Score for people who matched and did not match shown by the t-test
# tm_t_test(train$Count_of_Poster_Presentation) #NO DIFFERENCE in Count_of_Poster_Presentation for people who matched and did not match shown by the t-test
# tm_t_test(train$Count_of_Oral_Presentation)#NO DIFFERENCE in Count_of_Oral_Presentation for people who matched and did not match shown by the t-test
# tm_t_test(train$Count_of_Articles_Abstracts)#NO DIFFERENCE in Count_of_Articles_Abstracts for people who matched and did not match shown by the t-test
# tm_t_test(train$Count_of_Peer_Reviewed_Book_Chapter)#NO DIFFERENCE in Count_of_Peer_Reviewed_Book_Chapter for people who matched and did not match shown by the t-test
# tm_t_test(train$Count_of_Other_than_Published)#NO DIFFERENCE in Count_of_Other_than_Published for people who matched and did not match shown by the t-test
```

After that, we got the train dataframe with categorization (`train`) and compared categorical variables through the chi-square test of independence (used to analyze the frequency table). The chi-squared test is a statistical test used to discover whether there is a relationship between categorical variables.
```{r}
tm_chi_square_test <- function (variable) {
  print("Function Sanity Test: chi-square test")
  chisq <- stats::chisq.test(variable, train$Match_Status, correct = FALSE)
  return(chisq)
}

tm_chi_square_test(train$white_non_white) #Race is  significant
tm_chi_square_test(train$Gender) #Gender is significant
tm_chi_square_test(train$Couples_Match)#Couples match status is significant
tm_chi_square_test(train$US_or_Canadian_Applicant)#US Applicant is significant

tm_chi_square_test(train$Medical_Education_or_Training_Interrupted)#Interrupting medical education is significant
tm_chi_square_test(train$Alpha_Omega_Alpha)#AOA is significant
tm_chi_square_test(train$Military_Service_Obligation)#Military Service obligation is NOT significant
tm_chi_square_test(train$Visa_Sponsorship_Needed)#Visa sponsorship is significant
tm_chi_square_test(train$Medical_Degree)#Medical Degree is significant
```


```{r}
tm_print_save <- function (filename) {
  print("Function Sanity Check: Saving TIFF of what is in the viewer")
  dev.print(tiff, (here::here("results", filename)), compression = "lzw",width=2000, height=2000, bg="transparent", res = 200, units = "px" )
  dev.off()
}

cnum <- train[,c("Age", "USMLE_Step_1_Score")]
cormat <- stats::cor(cnum) # Select only numeric variables that are significant on univariate testing.  
pairs <- graphics::pairs(cnum)
#tm_print_save("significant_numeric_variable_correlation.tiff")

corrplot::corrplot(cormat, method="circle")
corrplot::corrplot(cormat, method="circle", addCoef.col="black") # With correlation 
tm_print_save("corrplot.tiff")
rm(cnum)
rm(cormat)

```

In this correlation plot we want to look for the bright, large circles which immediately show the strong correlations (size and shading depends on the absolute values of the coefficients; color depends on direction).  This shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Anything that you would have to squint to see is usually not worth seeing! 

Observations:  Match_Status is most correlated to US_or_Canadian_Applicant and then to Age.  Visa_Sponsorship_Needed, Medical_Education_or_Training_Interrupted, and white_non_white are all variables that might play a secondary role.  The other features are pretty weak.  

```{r, echo=TRUE, include = TRUE}
tm_ggsave <- function (object, filename, ...){  #make sure the file name has quotation marks around it.  
  print("Function Sanity Check: Saving a ggplot image as a TIFF")
  ggplot2::ggsave(here::here("results", filename), object, device = "tiff", width = 10, height = 7, dpi = 200)
}


inspect_cor_plot <- inspectdf::inspect_cor(train, method = "pearson", alpha = 0.05) %>% inspectdf::show_plot()  
inspect_cor_plot

tm_ggsave(inspect_cor_plot, "inspect_cor_plot.tiff")
```
Correlation was found with Count_of_Poster_Presentation, Age, and USMLE_Step_1_Score to Match_Status in the train data set.  The lower the age (correlation -0.33)  the patient the more likely they are to match.  The higher the USMLE_Step_1 scores the more likely they are to match (correlation 0.344).  The younger you are the more likely that you get a higher USMLE_Step_1 score.  The number of posters and the number of abstract articles are positively correleated as well.  The strongest positive correlation was between the count of articles and the count of posters.  No shocker there.  Strong negative correlations were between Age and USMLE step 1 score.  Also match status and age were negatively correlated (the younger you are the more likely you are to match).   

Take a brief look at potential collinearity. We want to see strong correlations between our outcome and the predictors, but modest correlations between the predictors.  There are no correlations between predictors according to the above scatterplots.  If we did see signs of meaningful collinearity, we might rethink our selected set of predictors.

You can see that there is a natural separation between the Match status and USMLE socres.  See bottom row that is second from the left.  Same thing for Age and same thing for Count of posters.  Samne with article abstracts and number of peer-reviewed journals.  ALL COUNT VARIABLES SHOWED SEPARATION IN MATCHED VS. NOT MATCHED.  

The values of correlation range from -1 to 1.  If there is a value of 0 there is no correlation between the variables.  Perfect correlation is 1.  Perfect negative correlation is -1.  

Another way to look at correlation is with a correlation funnel.
```{r, include=TRUE, echo=FALSE}
all_data_binarized_tbl <- all_data %>%
  correlationfunnel::binarize(n_bins = 4, thresh_infreq = 0.01)

all_data_binarized_tbl %>% dplyr::glimpse()

all_data_correlated_tbl <- all_data_binarized_tbl %>%
  correlationfunnel::correlate(target = Match_Status__Match)
all_data_correlated_tbl

static_correlation_funnel <- all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = FALSE, limits = c(-1, 1))
static_correlation_funnel

tm_ggsave(static_correlation_funnel, "static_correlation_funnel.tiff")

#REALLY COOL AND INSTRUCTIVE
all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = TRUE, limits = c(-1, 1))
```

Another way to look at correlation is with pairs of variables plotted.   
```{r, include = TRUE, fig.width=8, fig.height=4, fig.cap="Figure: Evaluation of the variable interactions in the train data set."}
#https://jamesmarquezportfolio.com/correlation_matrices_in_r.html
colnames(train)
train_psych <- train %>%
  dplyr::select_if(is.numeric)

psych_correlation <- psych::pairs.panels(train_psych, bg=c("red","blue")[as.factor(train$Match_Status)], pch=22, jiggle = TRUE, scale = TRUE)
psych_correlation
rm(train_psych)

tm_ggsave(psych_correlation, "psych_correlation.tiff")
```

Billizionth way to review correlation.  
```{r}
#Correlation plot
M <- corrgram::corrgram(all_data)
M
```


```{r}
readr::write_csv(all_data, "~/Dropbox/Nomogram/nomogram/all_data_output_of_split_correlation.csv")
readr::write_csv(train, "~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv")
readr::write_csv(test, "~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv")
```

