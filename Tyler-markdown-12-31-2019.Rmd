---
title: "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: show
    dev: svg
    df_print: paged
    theme: journal
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console

---
*Tyler M. Muffly, Merdith Alston, Jill Liss, Georg Kropat, Janet Corral, Christine Raffaelli, J. Eric Jelovsek*
*Authorship order will be determined by the amount of work done.*
- name: Tyler M. Muffly, MD
  affiliation: Denver Health
- name: Meredith Alston, MD
  affiliation: Denver Health
- name: Jill Liss
  affiliation: University of Colorado
- name: Michael Courtois, MS
  affiliation: Denver Health
- name: Georg Kropat, PhD
  affiliation: R & D Services
- name: Janet Corall, PhD
  affiliation: University of Colorado
- name: Christine Raffaelli
  affiliation: University of Colorado
- name: J. Eric Jelovsek
  affiliation: Duke University

*Objective:  *
*We sought to construct and validate a model that predict a medical student's chances of matching into an obstetrics and gynecology residency.*

The Electronic Residency Application Service (ERAS) is a centralized solution to the medical residency application and documents distribution process. The data source to be used for the project is the Electronic Residency Application Service data that was in discrete fields in the application.  No hand-searching of data was done. The data was exported from the ERAS Program Director Work Station under the Archives menu.The data is collected by the University of Colorado OBGYN residency that has both a categorical and a preliminary position.  Medical students who applied to the preliminary position were considered to be unmatched. The data set of years 2015, 2016, 2017, and 2018 applicants to the University of Colorado OBGYN residency. The data is contained in a data frame called 'all_data'. In advance, we might anticipate that USMLE Step 1 Score and US or Canadian Applicant will be key predictors.  Match Status is the dependent Variable and shows the medical students who applied to the OBGYN residency. 

# Codebook
A codebook is a technical description of the data that was collected for a particular purpose. It describes how the data are arranged in the computer file or files, what the various numbers and letters mean, and any special instructions on how to use the data properly.

* Predictors under consideration:
1. `all_data$white_non_white` - Dichotomoized from ethnicity fields in ERAS data, 2 level categorical
2. `all_data$Age` - Age at the time of the match, numerical variable
3. `all_data$Year` - Year of participation in the match, 4 level categorical.  Will not have data of year in future so to remove.  
4. `all_data$Gender` - Male or Female, 2 level categorical
5. `all_data$Couples_Match` - PArticipating in the couples match? 2 level categorical.  We are unsure if there are any couples who are both matching into OBGYN.  
6. `all_data$US_or_Canadian_Applicant` - Are they a US/Canadian Senior "Yes" or an International medical graduate "No"?  , 2 level categorical 
7. `all_data$Medical_Education_or_Training_Interrupted` - Taking breaks, 2 level categorical
8. `all_data$Alpha_Omega_Alpha` - Membership in AOA, 3 level categorical.  Yes means elected in their junior year or elections are early.  Could combine with Sigma Sigma Phi?  Passing minimum score is 194 and theoretical maximum score is 300.  
9. `all_data$Military_Service_Obligation`
10. `all_data$USMLE_Step_1_Score` - I did not use Step 2 score because most students will not have those numbers at the time they apply, numerical variable
11. `all_data$Count_of_Poster_Presentation` - numerical variable
12. `all_data$Count_of_Oral_Presentation` - numerical variable
13. `all_data$Count_of_Articles_Abstracts` - numerical variable
14. `all_data$Count_of_Peer_Reviewed_Book_Chapter` - numerical variable
15. `all_data$Count_of_Other_than_Published` - numerical variable
16. `all_data$Visa_Sponsorship_Needed` - numerical variable
17. `all_data$Medical_Degree` - Allopathic versus Osteopathic medical school education, 2 level categorical

This data was cleaned in a separate R script with the help of exploratory.io.  

Packrat is in use to control package versions and a packrat.lock file is posted to my github repository.  This will allow for easier reproducibility.  Packrat records the exact package versions you depend on, and ensures those exact versions are the ones that get installed wherever you go.  We can also control the environment by deploying the project inside a Docker container as needed.  The project was created in R version 3.6.1 and run inside RStudio 1.2.5019.  Session info is present at the bottom of the script.  
```{r directory paths and packrat settings, include=FALSE}
#Helpful, https://www.kaggle.com/pjmcintyre/titanic-first-kernel/code
# https://bradleyboehmke.github.io/HOML/

cat("\014")  #Clear console
# I want to use packrate to make the project more: isolated, portable and reproducible.  Please see packrat/packrat.lock for a list of the precise packages used and their versions.  Be sure to check in the .Rprofile, packrat.lock files, and everything under packrat/src/. 
packrat::packrat_mode(on = TRUE)
#packrat::status()
#packrat::clean()
#packrat::restore()
#packrat::snapshot(infer.dependencies = FALSE, snapshot.sources = FALSE, ignore.stale = TRUE)  #Run each time a new library is installed.

#packrat::bundle(include.lib = TRUE, include.src = TRUE, omit.cran.src = TRUE)  #Use this for portability
#Bundle is stored in packrat/bundles/***.tar.gz

#options(tinytex.verbose = FALSE)
library(knitr)
```

```{css echo = FALSE}
#Taken from the Pudding.cool
pre code, pre, code {
  white-space: pre !important;
  overflow-y: scroll !important;
  max-height: 25vh !important;
}
```

Package installation, data download from Dropbox.com, and functions written for this project are all loaded in a separate "Additional_functions_nomogram.R" file.
```{r, echo=TRUE, include = FALSE}
here::set_here()  #set_here() creates an empty file named .here, by default in the current directory.Sets root.  
here::here()
#May need to run this file line-by-line (CRTL+Enter) by hand.  

#source(file=(here::here("Code", "Additional_functions_nomogram.R")), echo=TRUE, verbose=TRUE)
all_data <- read_csv("~/Dropbox/Nomogram/nomogram/data/All_ERAS_data_merged_output_2_1_2020.csv")
conflicts()
```

Drake shit
```{r}
file.exists("list of people who all matched into OBGYN.rds")
file.exists("~/Dropbox/Nomogram/nomogram/data/All_ERAS_data_merged_output_2_1_2020.csv")

plan <- drake::drake_plan(
  raw_data = exploratory::read_rds_file("/Users/tylermuffly/Dropbox/Nomogram/nomogram/data/list of people who all matched into OBGYN.rds") %>%
    readr::type_convert() %>%
    exploratory::clean_data_frame(),
  all_data <- read_csv("~/Dropbox/Nomogram/nomogram/data/All_ERAS_data_merged_output_2_1_2020.csv"),
  #data = 
  hist = create_plot_num(all_data), 
  create_plot_cross_plot(all_data), 
  create_profiling_num(all_data),
  tm_nomogram_prep(all_data),
  tm_rpart_plot(all_data),
  tm_arsenal_table(all_data),
  tm_chart_strength_of_variables(all_data),
  tm_variable_importance(all_data),
  density_fun_plot(all_data),
  tm_confusion_matrix_graph(all_data),
  plot_grid_plot_density(all_data),
  plot_hist_facet(all_data),
  get_cor(all_data),
  plot_cor(all_data),
  logisticPseudoR2s(all_data),
  tm_vip(all_data),
  tm_ggsave(all_data),
  tm_print_save(all_data),
  tm_write2pdf(all_data),
  tm_write2word(all_data),
  create_plot_num_plot(all_data),
  tm_t_test(all_data),
  tm_chi_square_test(all_data),
  
  
  #fit = lm(Sepal.Width ~ Petal.Width + Species, data),
  #report = rmarkdown::render(
  #  knitr_in("report.Rmd"),
  #  output_file = file_out("report.html"),
  #  quiet = TRUE
  )

drake::vis_drake_graph(plan)

drake::make(plan)


```


The first few rows of the data loaded are given below:
```{r}
head(all_data) %>% knitr::kable(format = "pandoc", caption = "Example observations (rows) and the features (columns)")
```

# Ingest Data:  Load and Tidy the Data
`all_data` is a dataframe of the independent and the dependent variables for review. Each variable is contained in a column, and each row represents a single unique medical student. If students applied in more than one year the most contemporary data was used.
```{r Download cleaned data from Dropbox, echo=TRUE, include=FALSE, cache=FALSE}
all_data
dim(all_data)
dim(all_data)
data.table::data.table(all_data)
funModeling::freq(data=all_data, plot = FALSE, na.rm = FALSE)
glimpse(all_data)

view(summarytools::dfSummary(x = all_data, justify = "l", style = "multiline", varnumbers = FALSE, valid.col = FALSE, tmp.img.dir = "./img", max.distinct.values = 5))

# orginal response distribution
tmp <- table(all_data$Match_Status)
match_rate <- (tmp[[1]]/(tmp[[2]] + tmp[[1]]))*100
match_rate
rm(tmp)
```
We can see that these data have `r nrow(all_data)` observations of `r ncol(all_data)` features, and thatabout `r round(match_rate, digits = 1)` percent of medical students applying to OB/GYN residency matched.   Let’s create a few plots to get a sense of the data. Remember, the goal here will be to predict whether a given medical student will match into OB/GYN residency, based on the variables listed in the codebook.

# Description of the Data
* A summary of the variables are listed below:
1. Eleven of the variables were a factor.  All factors had two levels except for Alpha_Omega_Alpha had three levels.  The target variable is `all_data$Match_Status`.  
2. Eight of the variables were integers. 

```{r structure of data, include=TRUE, echo=TRUE, out.width="75%"}
# examine the structure of the initial data frame
all_data$Count_of_Peer_Reviewed_Journal_Articles_Abstracts <- as.numeric(all_data$Count_of_Peer_Reviewed_Journal_Articles_Abstracts)
all_data$Age <- as.numeric(all_data$Age)
all_data$Count_of_Poster_Presentation <- as.numeric(all_data$Count_of_Poster_Presentation)
all_data$USMLE_Step_1_Score <- as.numeric(all_data$USMLE_Step_1_Score)
all_data$Count_of_Oral_Presentation <- as.numeric(all_data$Count_of_Oral_Presentation)
all_data$Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published <- as.numeric(all_data$Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published)
all_data$Count_of_Peer_Reviewed_Book_Chapter <- as.numeric(all_data$Count_of_Peer_Reviewed_Book_Chapter)
all_data$Count_of_Non_Peer_Reviewed_Online_Publication <- as.numeric(all_data$Count_of_Non_Peer_Reviewed_Online_Publication)
all_data$USMLE_Step_2_CK_Score <- as.numeric(all_data$USMLE_Step_2_CK_Score)

inspectdf::inspect_types(all_data) %>% show_plot()

all_data <- all_data %>% select(-Medical_School_of_Graduation)
```

```{r}
#https://github.com/manojmukkamala/DiabeticPatientsReadmission/blob/master/Project_V4.R
#Just writing the names of all the variables under one term to avoid re-writing when we apply model multiple times

#"Malpractice_Cases_Pending", Removed because this is due to the fact that all people had 0 cases

drivers <- c("ACLS",
                                                                     "Age",
                                                       "Alpha_Omega_Alpha",
                                                         # "Applicant_Name",
                                                                     "BLS",
                                                             "Citizenship",
                           "Count_of_Non_Peer_Reviewed_Online_Publication",
                                              "Count_of_Oral_Presentation",
                                                 "Count_of_Other_Articles",
                                    "Count_of_Peer_Reviewed_Book_Chapter",
                      "Count_of_Peer_Reviewed_Journal_Articles_Abstracts",
 "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published",
                              "Count_of_Peer_Reviewed_Online_Publication",
                                           "Count_of_Poster_Presentation",
                                          "Count_of_Scientific_Monograph",
                                                          "Couples_Match",
                                                                 "Gender",
                                                           "Match_Status",
                                                         "Medical_Degree",
                              "Medical_Education_or_Training_Interrupted",
                                              "Medical_Licensure_Problem",
                                            "Military_Service_Obligation",
                                                 "Misdemeanor_Conviction",
                                                            "NIH_dollars",
                                                                   "PALS",
                                                        "Sigma_Sigma_Phi",
                                                 "Type_of_medical_school",
                                               "US_or_Canadian_Applicant",
                                                     "USMLE_Step_1_Score",
                                                  "USMLE_Step_2_CK_Score",
                                                "Visa_Sponsorship_Needed",
                                                        "white_non_white")

#Writing the name of dependent variable seperately
dependentVar <- "Match_Status"

#Bringing all the column names under one term
formula <- paste(dependentVar, "~", paste(drivers, collapse = " + "))
formula
```


```{r Describe data, include=FALSE}
Hmisc::describe(all_data)
```
A nice data summary is available from the `skim` package.   
```{r,include=TRUE}
skimr::skim(all_data)
```

# Evaluate for missing data in multiple manners
```{r,EDA, results="asis", echo=TRUE, include=TRUE, out.width="50%"}
#plot_str(all_data) #COOL BUT USELESS HERE
DataExplorer::plot_missing(all_data)
DataExplorer::plot_intro(all_data)
```

The new data set in `all_data`, includes `r nrow(all_data)` rows and `r ncol(all_data)` columns.  The `r (english::words(nrow(all_data)))` applicants are missing `r (english::words(sum(is.na(all_data))))` values.    

Shows four different ways to check for missingness in the data 
```{r, four ways to look for missingness, include=FALSE, out.width="50%"}
#Cool but a little overkill with three ways for checking if we have missing data.  
sum(is.na(all_data))
all_data <- na.omit(all_data)
colSums(is.na(all_data))
purrr::map_df(all_data, ~ sum(is.na(.)))

naniar::gg_miss_var(all_data)
tm_print_save(filename = "na_pattern.tiff")

na_pattern <- Hmisc::na.pattern(all_data)
na_pattern

sum(is.na(all_data))
```

# Exploratory data analysis
After the data check was completed, an exploratory data analysis (EDA) was conducted to look for interesting relationships among the variables. Histograms were used to visualize distributions among predictors. Since the outcome of Matching is a classification problem, relationships between predictors and the dichotomous outcome were also performed. 

Description of `all_data$Match_Status` variable.  
```{r data check, include=TRUE, out.width="50%"}
Hmisc::describe(as.factor(all_data$Match_Status))
whomatched <- 
  ggplot2::ggplot(all_data[!is.na(all_data$Match_Status),], aes(x = Match_Status, fill = Match_Status)) +
  geom_bar(stat='count') +
  labs(x = 'How many applicants matched into OBGYN?') +
        geom_label(stat='count',aes(label=..count..), size=7) +
        theme_grey(base_size = 18)
whomatched

tm_ggsave(object = whomatched, filename = "whomatched.tiff")
```

# Data Description and *Univariate* analysis of variables. 
Categorical and numerical variable plots:
```{r DataExplorer, results='asis', echo=TRUE, include=TRUE, align = 'left', cache=FALSE, out.width="75%"}
#General Data Description
inspect_cat_plot <- inspectdf::inspect_cat(all_data) %>% show_plot()  #Please use `cols = c(data)` is the error message I get here.  But it still runs.  
inspect_cat_plot
tm_ggsave(object = inspect_cat_plot, filename = "inspect_cat_plot.tiff")

plot_histogram_plot <- DataExplorer::plot_histogram(all_data, nrow = 2L, ncol = 2L)  
plot_histogram_plot

DataExplorer::plot_bar(all_data, nrow = 2L, ncol = 2L)  #Not useful whatsoever.  

#Univariate of boxplots
inspect_num_plot <- inspectdf::inspect_num(all_data) %>% show_plot()  
inspect_num_plot

tm_ggsave(inspect_num_plot, "inspect_num_plot.tiff")
```

```{r, include=TRUE, echo=TRUE, include=TRUE, align = 'left', cache=FALSE, out.width="75%"}
#EDA,  https://ryjohnson09.netlify.com/post/caret-and-tidymodels/
# Create vector of predictors and subtract the last column with the target variable
expl <- names(all_data)[-32]  #[-19]

# Loop vector with map
expl_plots_box <- purrr::map(expl, ~box_fun_plot(data = all_data, x = "Match_Status", y = .x) )
plot_grid_plot <- cowplot::plot_grid(plotlist = expl_plots_box)
plot_grid_plot

tm_ggsave(plot_grid_plot, "plot_grid_plot.tiff")

# Loop vector with map
expl_plots_density <- purrr::map(expl, ~density_fun_plot(data = all_data, x = "Match_Status", y = .x) )
plot_grid_plot_density <- cowplot::plot_grid(plotlist = expl_plots_density)
plot_grid_plot_density

tm_ggsave(plot_grid_plot_density, "plot_grid_plot_density.tiff")
```


```{r funModeling2, echo=TRUE, message=FALSE, warning=TRUE, include=FALSE}
create_plot_num_plot <- create_plot_num(all_data)  
create_plot_num_plot

tm_ggsave(create_plot_num_plot, "create_plot_num_plot.tiff")
```

```{r funModeling1, echo=TRUE, message=FALSE, warning=TRUE, results='asis', include=FALSE}
df_status_output <- funModeling::df_status(all_data)
#desc_groups(data=all_data, group_var="Match_Status")  #Breaks the knitr for some reason
df_status_output
write_csv(df_status_output, (here::here("results", "df_status_output.csv")))
```

```{r funModeling3, echo=TRUE, message=FALSE, warning=TRUE, results='asis', include=FALSE}
#Summary stats of the numerical data showing means, medians, skew
create_profiling_num_output <- create_profiling_num(all_data)   
all_data %>% mosaic::inspect()  #another good option
write_csv(create_profiling_num_output, (here::here("results", "create_profiling_num_output.csv")))
```

# Cross plots of predictors by outcome
```{r funModeling4, echo=TRUE, message=FALSE, warning=TRUE, include=TRUE, out.width="75%"}
#Not working with markdown

#DataExplorer::plot_boxplot(all_data, by = "Match_Status", nrow = 2L, ncol = 2L)
#Shows the variable frequency charted by matching status
#create_plot_cross_plot(all_data)   
```

Here is the code that I used for the cubic splines and it pushed the VIF through the roof below.  I need help with this part.  
```{r, echo=TRUE, include=FALSE, warning = FALSE, fig.width=7, fig.asp=1, fig.cap="Figure: Relaxing Cubic Splines for Continuous Variables."}

#Using thhe splines was a mess.  

## Relaxed Cubic Splines For Continuous Variables

# #Age Splines
# Hmisc::rcspline.eval(x=all_data$Age, 
#                      nk=5, type="logistic", 
#                      inclx = TRUE, 
#                      knots.only = TRUE, 
#                      norm = 2, 
#                      fractied=0.05)  
# 
# #tells where the knots are located
# Hmisc::rcspline.plot(x = all_data$Age,
#                      y = as.numeric(all_data$Match_Status), 
#                      model = "logistic", 
#                      nk = 5, 
#                      showknots = TRUE, 
#                      plotcl = TRUE, 
#                      statloc = 11,
#                      main = "Estimated Spline Transformation for Age", 
#                      xlab = "Age (years)", 
#                      ylab = "Probability",
#                      noprint = TRUE, 
#                      m = 500) #In the model Age should have rcs(Age, 5)
# 
# #Predictions with group size of 500 patients (triangles) and location of knot (arrows).
# #USMLE_Step_1_Score Splines
# Hmisc::rcspline.eval(x=all_data$USMLE_Step_1_Score, 
#                      nk=4, 
#                      type="logistic", 
#                      inclx = TRUE, 
#                      knots.only = TRUE, 
#                      norm = 2, 
#                      fractied=0.05)  #tells where the knots are located
# 
# Hmisc::rcspline.plot(x = all_data$USMLE_Step_1_Score, 
#                      y = as.numeric(all_data$Match_Status), 
#                      model = "logistic", 
#                      nk=5, 
#                      showknots = TRUE, 
#                      plotcl = TRUE, 
#                      statloc = 11, 
#                      main = "Estimated Spline Transformation for USMLE Step 1 Score", 
#                      xlab = "USMLE Step 1 Score", 
#                      ylab = "Probability", 
#                      noprint = TRUE, 
#                      m = 500) #In the model USMLE_Step_1 should have rcs(USMLE_Step_1, 6)
# 
# #Count of Posters
# Hmisc::rcspline.eval(x=all_data$Count_of_Poster_Presentation, 
#                      nk=5, 
#                      type="logistic", 
#                      inclx = TRUE, 
#                      knots.only = TRUE, 
#                      norm = 2, 
#                      fractied=0.05)  #tells where the knots are located
# 
# Hmisc::rcspline.plot(x = all_data$Count_of_Poster_Presentation, 
#                      y = as.numeric(all_data$Match_Status), 
#                      model = "logistic", 
#                      nk=5, 
#                      showknots = TRUE, 
#                      plotcl = TRUE, 
#                      statloc = 11, 
#                      main = "Estimated Spline Transformation for Poster Presentations", 
#                      xlab = "Count of Poster Presentations", ylab = "Probability", noprint = TRUE, m = 500) #In the model Count of Poster presentations should have rcs(Count of Poster Presentations, 4)
# 
# #Count of Oral Presentations
# Hmisc::rcspline.eval(x=all_data$Count_of_Oral_Presentation, 
#                      nk=5, type="logistic",
#                      inclx = TRUE, 
#                      knots.only = TRUE, 
#                      norm = 2, 
#                      fractied=0.05)  #tells where the knots are located
# 
# Hmisc::rcspline.plot(x = all_data$Count_of_Oral_Presentation, 
#                      y = as.numeric(all_data$Match_Status), 
#                      model = "logistic", 
#                      nk = 5, 
#                      showknots = TRUE, 
#                      plotcl = TRUE, 
#                      statloc = 11, 
#                      main = "Estimated Spline Transformation for Oral Presentations", 
#                      xlab = "Count of Oral Presentations", 
#                      ylab = "Probability", 
#                      noprint = TRUE, 
#                      m = 1000) #In the model Count of Oral Presentations should have rcs(Count of Oral Presentations, 3)
```

# Table: Applicant Descriptive Variables by Matched or Did Not Match from 2015 to 2018
```{r, echo=TRUE, warning=FALSE, message=FALSE, include=TRUE, results="asis"}
tm_arsenal_table_output <- tm_arsenal_table(df = all_data, by = all_data$Match_Status)
tm_arsenal_table_output

tm_write2word(tm_arsenal_table_output, "tm_arsenal_table_output1")
tm_write2pdf(tm_arsenal_table_output, "tm_arsenal_table_output1")
```

# Why are we using a train and test sample data set to test the model?  
The training set contains a known output (`all_data$Match_Status`) and the model learns this data in order to be generalized to other data in the process. In this way, the model will predict values for the test data (cross validation). It is possible to determine the prediction accuracy of the model.

Overfitting is one of the biggest challenges in the machine learning process. Overfitting means that the model has been trained “too well”, and as a result it learns the noise present in the training data as if it was a reliable pattern. Overfitting affects the ability of the model to perform well in unseen data, which is known as generalisation.

Two well known strategies to overcome the problem of overfitting are the train/validation split and cross-validation.

# Identify training and test samples
I will call the training sample `train` and the test sample `test`.  *Creative!*  Another option is dplyr::sample_n if you want to instead specify the exact number of observations to be selected. There are `r nrow(train)` medical students in the training data set and `r nrow(test)` in the test data set.  

Shows four different ways to split the data  
```{r, train vs test, warning=FALSE, echo=TRUE, message=FALSE, include=TRUE}
#http://rpubs.com/josevilardy/crossvalidation

# WE USED THIS ONE
set.seed(seed = 1978) 
all_data$Age <- as.numeric(all_data$Age)
data_split <- rsample::initial_split(data = all_data, 
                                     strata = "Match_Status", 
                                     prop = 0.8)

train <- data_split %>% training() %>% glimpse()  # Extract the training dataframe
write_csv(x = train, 
          path = (here::here("results", "train_at_data_split_phase.csv")), 
          col_names = TRUE)
training_data_plot_hist_facet <- plot_hist_facet(data = train)
training_data_plot_hist_facet

test <- data_split %>% testing() %>% glimpse() # Extra
write_csv(x = test, 
          path = (here::here("results", "test_at_data_split_phase.csv")), 
          col_names = TRUE)

plot_hist_facet(data=test)
```


```{r, train vs test, warning=FALSE, echo=TRUE, message=FALSE, include=TRUE}
#More ways to split the data.  

# Using base R
set.seed(123)  # for reproducibility
index_1 <- base::sample(1:nrow(all_data), round(nrow(all_data) * 0.8))
train_1 <- all_data[index_1, ]
test_1  <- all_data[-index_1, ]
rm(test_1)
rm(train_1)

# Using caret package
set.seed(123)  # for reproducibility
index_2 <- caret::createDataPartition(all_data$Match_Status, p = 0.8, 
                               list = FALSE)
train_2 <- all_data[index_2, ]
test_2  <- all_data[-index_2, ]
rm(train_2)
rm(test_2)

# Using rsample package
set.seed(123)  # for reproducibility
split_1  <- rsample::initial_split(all_data, prop = 0.8)
train_3  <- rsample::training(split_1)
test_3   <- rsample::testing(split_1)
rm(split_1)
rm(train_3)
rm(test_3)

# Using h2o package
# brew tap caskroom/versions
# brew cask install adoptopenjdk8
# brew tap adoptopenjdk/openjdk
# brew cask install homebrew/cask-versions/adoptopenjdk8

h2o::h2o.init()
all_data.h2o <- h2o::as.h2o(all_data)
split_2 <- h2o::h2o.splitFrame(all_data.h2o, ratios = 0.8, 
                          seed = 123)
train_4 <- split_2[[1]]
test_4  <- split_2[[2]]
data_split

rm(train_4)
rm(test_4)
rm(data_split)
```

```{r}
#Remove applicant names from train and test.  

train <- train %>%
  dplyr::select(-Applicant_Name, -Medical_School_of_Graduation)

test <- test %>%
  dplyr::select(-Applicant_Name, -Medical_School_of_Graduation)
```


Compare the datasets of `train` and `test`using arsenal package:  
```{r, results="asis", include=FALSE, echo=FALSE}
compareddf <- summary(arsenal::comparedf(train, test))

tm_write2word(compareddf, "compareddf")
tm_write2pdf(compareddf, "compareddf")
```

`train` data characteristics are all reported with medians and IQR.  
```{r, include = TRUE, results="asis"}
train_table_characteristics <- tm_arsenal_table(
  df=train, 
  by=train$Match_Status)

tm_write2word(train_table_characteristics, "train_table_characteristics")
tm_write2pdf(train_table_characteristics, "train_table_characteristics")
```

`test` data characteristics are all reported with medians and IQR. 
```{r, include = TRUE, results="asis"}
test_table_characteristics <- tm_arsenal_table(df = test, by = test$Match_Status)

tm_write2word(test_table_characteristics, "test_table_characteristics")
tm_write2pdf(test_table_characteristics, "test_table_characteristics")
```

Check Proportions of Matched/Unmatched applicants in the test and train data sets.  Orginal response distribution and then showing consistent response ratio between train & test data sets.  
```{r, results="asis", echo=TRUE, include=TRUE}
# Examine the proportions of the Match_Status class lable across the datasets.
crude_summary <- 
  base::prop.table(table(all_data$Match_Status))  #Original data set proportion 

base::prop.table(table(train$Match_Status)) #Train data set proportion

base::prop.table(table(test$Match_Status))  #Test data set proportion

knitr::kable(crude_summary, caption="2x2 Contingency Table on Matching for all_data", format="markdown")
```

Summarize the outcome and the predictors
Using the training sample, we will provide numerical summaries of each predictor variable and the outcome, as well as graphical summaries of the outcome variable. Our results should now show no missing values in any variable. We’ll need to determine whether there are any evident problems, such as substantial skew in the outcome variable.

```{r}
#Is feature skew present?
skewed_feature_names <- train %>%
  dplyr::select_if(is.numeric) %>%
  map_df(PerformanceAnalytics::skewness) %>% #returns a single row tibble 
  tidyr::gather(factor_key = TRUE) %>% #transposes into a long data column
  dplyr::arrange(desc(value)) %>% #Look for low and high values, high values have a fat tail on right, low has fat tail on left side
  dplyr::filter(value>2.0) %>% #eyeballed cutoff for value cut off
  dplyr::pull(key) %>%
  as.character()
```

# Factor Selection: Scatterplot Matrix and Correlation
Get a correlation matrix for elementary feature selection and remove highly correlated features.  
```{r}
# get a correlation matrix for elementary feature selection and
# remove highly correlated fields
numeric_data <- select_if(all_data, is.numeric)
thres <- 0.75
corr.m <- cor(numeric_data)
highly.corr <- findCorrelation(corr.m, cutoff = thres)
highly.corr
data.clean <- numeric_data[,-highly.corr]
```
None of the numeric data are correlated to each other.  So we can't remove any numeric features because they are not highly correlated.  

```{r}
tm_t_test(train$Age)#Significant different in age for people who matched (younger) and did not match (older) shown by the t-test
tm_t_test(train$USMLE_Step_1_Score)#Significant different in USMLE_Step_1_Score for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Poster_Presentation) #NO DIFFERENCE in Count_of_Poster_Presentation for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Oral_Presentation)#NO DIFFERENCE in Count_of_Oral_Presentation for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Articles_Abstracts)#NO DIFFERENCE in Count_of_Articles_Abstracts for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Peer_Reviewed_Book_Chapter)#NO DIFFERENCE in Count_of_Peer_Reviewed_Book_Chapter for people who matched and did not match shown by the t-test
tm_t_test(train$Count_of_Other_than_Published)#NO DIFFERENCE in Count_of_Other_than_Published for people who matched and did not match shown by the t-test
```

After that, we got the train dataframe with categorization (`train`) and compared categorical variables through the chi-square test of independence (used to analyze the frequency table). The chi-squared test is a statistical test used to discover whether there is a relationship between categorical variables.
```{r}
tm_chi_square_test(train$white_non_white) #Race is  significant
tm_chi_square_test(train$Gender) #Gender is significant
tm_chi_square_test(train$Couples_Match)#Couples match status is significant
tm_chi_square_test(train$US_or_Canadian_Applicant)#US Applicant is significant

tm_chi_square_test(train$Medical_Education_or_Training_Interrupted)#Interrupting medical education is significant
tm_chi_square_test(train$Alpha_Omega_Alpha)#AOA is significant
tm_chi_square_test(train$Military_Service_Obligation)#Military Service obligation is NOT significant
tm_chi_square_test(train$Visa_Sponsorship_Needed)#Visa sponsorship is significant
tm_chi_square_test(train$Medical_Degree)#Medical Degree is significant
```


```{r}
cnum <- train[,c("Age", "USMLE_Step_1_Score")]
cormat <- cor(cnum) # Select only numeric variables that are significant on univariate testing.  
pairs <- pairs(cnum)
#tm_print_save("significant_numeric_variable_correlation.tiff")

corrplot::corrplot(cormat, method="circle")
corrplot::corrplot(cormat, method="circle", addCoef.col="black") # With correlation 
tm_print_save("corrplot.tiff")
rm(cnum)
rm(cormat)

```

Correlation plot 
```{r, warning= FALSE, message=FALSE, echo=TRUE, include=TRUE}
train_correlation <-
  train %>%
  select_if(is.numeric) %>% #selects only numeric columns to be used in the correlation plot, NICE!
dplyr::mutate(white_non_white = as.integer(train$white_non_white),
         Match_Status = as.integer(train$Match_Status),
         Gender = as.integer(train$Gender),
         Couples_Match = as.integer(train$Couples_Match),
         US_or_Canadian_Applicant = as.integer(train$US_or_Canadian_Applicant),
         Medical_Education_or_Training_Interrupted = as.integer(train$Medical_Education_or_Training_Interrupted),
         Alpha_Omega_Alpha = as.integer(train$Alpha_Omega_Alpha),
         Military_Service_Obligation = as.integer(train$Military_Service_Obligation),
         Visa_Sponsorship_Needed = as.integer(train$Visa_Sponsorship_Needed),
         Medical_Degree = as.integer(train$Medical_Degree))%>%
  stats::cor(use="complete.obs") %>%
  corrplot::corrplot(type="lower",
                     diag=FALSE,
                     addgrid.col = rgb(0, 0, 0, .05),
                     order = "hclust",
                     tl.cex = 0.5,
                     tl.col = "black",
                     sig.level = "0.01",
                     insig = "blank", #Insignficant values are left blank
                     pch = TRUE)
tm_print_save("train_correlation.tiff")
```
In this correlation plot we want to look for the bright, large circles which immediately show the strong correlations (size and shading depends on the absolute values of the coefficients; color depends on direction).  This shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Anything that you would have to squint to see is usually not worth seeing! 

Observations:  Match_Status is most correlated to US_or_Canadian_Applicant and then to Age.  Visa_Sponsorship_Needed, Medical_Education_or_Training_Interrupted, and white_non_white are all variables that might play a secondary role.  The other features are pretty weak.  

```{r, echo=TRUE, include = TRUE}
inspect_cor_plot <- inspectdf::inspect_cor(train, method = "pearson", alpha = 0.05) %>% show_plot()  
inspect_cor_plot

tm_ggsave(inspect_cor_plot, "inspect_cor_plot.tiff")
```
Correlation was found with Count_of_Poster_Presentation, Age, and USMLE_Step_1_Score to Match_Status in the train data set.  The lower the age (correlation -0.33)  the patient the more likely they are to match.  The higher the USMLE_Step_1 scores the more likely they are to match (correlation 0.344).  The younger you are the more likely that you get a higher USMLE_Step_1 score.  The number of posters and the number of abstract articles are positively correleated as well.  The strongest positive correlation was between the count of articles and the count of posters.  No shocker there.  Strong negative correlations were between Age and USMLE step 1 score.  Also match status and age were negatively correlated (the younger you are the more likely you are to match).   

Take a brief look at potential collinearity. We want to see strong correlations between our outcome and the predictors, but modest correlations between the predictors.  There are no correlations between predictors according to the above scatterplots.  If we did see signs of meaningful collinearity, we might rethink our selected set of predictors.

You can see that there is a natural separation between the Match status and USMLE socres.  See bottom row that is second from the left.  Same thing for Age and same thing for Count of posters.  Samne with article abstracts and number of peer-reviewed journals.  ALL COUNT VARIABLES SHOWED SEPARATION IN MATCHED VS. NOT MATCHED.  

The values of correlation range from -1 to 1.  If there is a value of 0 there is no correlation between the variables.  Perfect correlation is 1.  Perfect negative correlation is -1.  

Another way to look at correlation is with a correlation funnel.
```{r, include=TRUE, echo=FALSE}
all_data_binarized_tbl <- all_data %>%
  correlationfunnel::binarize(n_bins = 4, thresh_infreq = 0.01)

all_data_binarized_tbl %>% tibble::glimpse()

all_data_correlated_tbl <- all_data_binarized_tbl %>%
  correlationfunnel::correlate(target = Match_Status__Match)
all_data_correlated_tbl

static_correlation_funnel <- all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = FALSE, limits = c(-1, 1))
static_correlation_funnel

tm_ggsave(static_correlation_funnel, "static_correlation_funnel.tiff")

#REALLY COOL AND INSTRUCTIVE
all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = TRUE, limits = c(-1, 1))
```

Another way to look at correlation is with pairs of variables plotted.   
```{r, include = TRUE, fig.width=8, fig.height=4, fig.cap="Figure: Evaluation of the variable interactions in the train data set."}
#https://jamesmarquezportfolio.com/correlation_matrices_in_r.html
psych_correlation <- psych::pairs.panels(train[, c(2, 9:15, 18)], bg=c("red","blue")[as.factor(train$Match_Status)], pch=21, jiggle = TRUE, scale = TRUE)

psych_correlation

tm_ggsave(psych_correlation, "psych_correlation.tiff")
```

Billizionth way to review correlation.  
```{r}
#Correlation plot
M <- corrgram::corrgram(all_data)
M
```

# Fitting and Summarizing the Kitchen Sink Model: (aka throw everything at it)
Create a Kitchen Sink or a "large" model with all factors in the `train` data set first. This is essentially a screening model with all variables. 

Logistic regression model from the `rms` package on the `kitchen.sink` model
```{r, echo=TRUE, include = TRUE}
train$Match_Status <- as.factor(train$Match_Status)
class(train$Match_Status)  
#train <- train %>% select(-Match_Status1)
require(rms)

train1 <- train %>% select(-Type_of_medical_school, -USMLE_Step_2_CK_Score, -USMLE_Step_1_Score, -US_or_Canadian_Applicant, -Medical_Degree)

d <- 
  rms::datadist(train1)

options(datadist = "d")

#singular information matrix in lrm.fit (rank= 30 ).  Offending variable(s):
# USMLE_Step_2_CK_Score Type_of_medical_school=U.S. Public School USMLE_Step_1_Score US_or_Canadian_Applicant=US senior Medical_Degree=MD 

kitchen.sink <- 
  rms::lrm(formula = Match_Status~., 
      data = train1, 
      method = "lrm.fit",
      x = T, 
      y = T)

kitchen.sink
anova(kitchen.sink, test="Chisq")
invisible(gc())

#This is a nice view of the model formula:  
kitchen.sink[[26]]
```

```{r, include=TRUE}
#https://thepoliticalmethodologist.com/2013/11/25/making-high-resolution-graphics-for-academic-publishing/
#See custom-made function in Additional_functions_nomogram.R for specific settings on nomogram build. 
tm_nomogram_prep(kitchen.sink)
tm_print_save("tm_nomogram_prep_kitchen_sink.tiff")
```

Create a Confusion Matrix
```{r}
test$Match_Status <- as.factor(test$Match_Status)

Predprob <- stats::predict.glm(object = kitchen.sink, type = "response")
#plot(Predprob, jitter(as.numeric(Match_Status)))

# test model
###Not working!!!
prediction_test <- rms::Predict(x = kitchen.sink)
prediction_categories <- ifelse(prediction_test > 0.5, 1, 0)
confusion <- table(prediction_categories, test$Match_Status)
(confusion.limited.vif.model.kitchen.sink <- caret::confusionMatrix(confusion, positive = "1"))
```

Calculates the Brier score for the `kitchen.sink` model.  
```{r, warning=FALSE, echo=TRUE, include = TRUE}
#Shows the C-statistic and the Brier score.  
tmp <- 
  as.data.frame(kitchen.sink$stats)

knitr::kable(tmp, 
             caption = "Performance statistics of the Kitchen Sink Model Using All Variables", 
             digits=2)
```

This `kitchen.sink` model accounts for just over `r kitchen.sink$stats[[10]]*100`% (r.squared) of the variation in Match_Status in our training sample of `r nrow(train)` medical students in `train`.  The C-statistic is `r round(kitchen.sink$stats[[6]], digit =2)`.  The c-statistic, also known as the concordance statistic, is equal to to the AUC (area under curve) and has the following interpretations:
* A value below 0.5 indicates a poor model.
* A value of 0.5 indicates that the model is no better out classifying outcomes than random chance.
* The closer the value is to 1, the better the model is at correctly classifying outcomes.
* A value of 1 means that the model is perfect at classifying outcomes.

The c-statistic is equal to the AUC (area under the curve), and can also be calculated by taking all possible pairs of individuals consisting of one individual who experienced a positive outcome and one individual who experienced a negative outcome. Then, the c-statistic is the proportion of such pairs in which the individual who experienced a positive outcome had a higher predicted probability of experiencing the outcome than the individual who did not experience the positive outcome.  The closer a c-statistic is to 1, the better a model is able to classify outcomes correctly.

Brier score for `kitchen.sink` is `r round(kitchen.sink$stats[[11]], digits = 2)`.  The best possible Brier score is 0, for total accuracy.  A Brier score is a way to verify the accuracy of a probability forecast.

The `kitchen.sink` p.value (`r kitchen.sink$stats[[5]]`, which is zero for all reasonable purposes) indicates a highly statistically significant amount of predictive value is accounted for by the model. This predictive value is no surprise given the moderate R2 value (`r kitchen.sink$stats[[10]]*100`%) and reasonably large (n = `r nrow(train)`) size of this training sample.

Effect Sizes: Interpreting Coefficient Estimates
Specify the size, magnitude and meaning of all coefficients, and identify appropriate conclusions regarding effect sizes with 90% confidence intervals.

This is messy (and maybe unncecessary) but I have to create a linear model using stats::lm to pull out values like R squared into the Rmarkdown inline values.
```{r, echo=TRUE, include=FALSE}
## Fitting a linear model of the kitchen sink using lm from the stats package
#train$Match_Status
class(train$Match_Status) #For lm models you MUST have the outcome be numeric class and a number!!!!
train$Match_Status <- as.numeric(train$Match_Status) - 1  #To make lm work you need to change the Match_status to 1 vs. 0
train$Match_Status
str(train)

lm.fit2 <- 
  stats::lm(formula = formula,  
     data = train)

summary(lm.fit2)
summary.lmfit2<- summary(lm.fit2) #Do this so that we can pull out the r squared values showing model performance
```

```{r, include = TRUE}
broom::glance(lm.fit2)
```


```{r}
coefficients <- broom::tidy(lm.fit2, conf.int = TRUE, conf.level = 0.9) 
coefficients
```
y = mx + b
Our model formula is intercept of `r round(coefficients[[1, 2]], digit=1)`+`r round(coefficients[[2, 2]], digit = 2)`(`r coefficients[[2, 1]]`) `r round(coefficients[[3, 2]], digit =2)`(`r coefficients[[3, 1]]`) `r round(coefficients[[4, 2]], digit =2)`(`r coefficients[[4, 1]]`) `r round(coefficients[[5, 2]], digit =2)`(`r coefficients[[5, 1]]`) `r round(coefficients[[6, 2]], digit =2)`(`r coefficients[[6, 1]]`) `r round(coefficients[[7, 2]], digit =2)`(`r coefficients[[7, 1]]`) `r round(coefficients[[8, 2]], digit=3)` (`r coefficients[[8, 1]]`) `r round(coefficients[[9, 2]], digit=2)`(`r coefficients[[9, 1]]`) `r round(coefficients[[10, 2]], digit=2)`(`r coefficients[[10, 1]]`).  

The r squared for model `lmfit2` is: `r round(summary.lmfit2[[8]], digit=3)`.  

```{r}
# Not working with R Markdown ????
# prediction_test <- rms::Predict(x = lm.fit2)
# prediction_categories <- ifelse(prediction_test > 0.5, 1, 0)
# confusion <- table(prediction_categories, test$Match_Status)
# confusion
```


# Does collinearity in the kitchen sink model have a meaningful impact?
Logistic regression models should be free of multicollinearity so we used the variance inflation factor (VIF).   The VIF may be calculated for each predictor by doing a linear regression of that predictor on all the other predictors.  It’s called the variance inflation factor because it estimates how much the variance of a coefficient is “inflated” because of linear dependence with other predictors. Thus, a VIF of 1.8 tells us that the variance (the square of the standard error) of a particular coefficient is 80% larger than it would be if that predictor was completely uncorrelated with all the other predictors.
* VIF = 1, no correlation
* VIF between 1 and 5 , moderately correlated
* VIF greater than 5, highly correlated

```{r, echo=TRUE, include=TRUE}
car::vif(kitchen.sink)
#https://statisticalhorizons.com/multicollinearity
#https://campus.datacamp.com/courses/human-resources-analytics-in-r-predicting-employee-churn/model-validation-hr-interventions-and-roi?ex=1
# I removed the splines from the Age, USMLE step 1 variable, etc because the VIF was too high with the spolines in place and re-ran the model.  
```

I removed variables one at a time from `kitchen.sink` until VIF is <5.  Here I removed `train$Medical_Degree` and all collinearity dropped out based on VIF readings.  Now I rebuilt the model named `limited.vif.model.kitchen.sink` without the multicollinear factor of `train$Medical_Degree`.  
```{r}
limited.vif.model.kitchen.sink <- stats::glm(Match_Status ~ . - Medical_Degree,   #Nice trick to remove one variable at a time
                 family = "binomial", data = train)

rms::vif(limited.vif.model.kitchen.sink)
```

Perform In Sample Prediction - Run train model on the `train` data
```{r}
prediction_train <- rms::Predict(x = limited.vif.model.kitchen.sink)

hist(prediction_train)
tm_print_save("hist_prediction_train.tiff")
```

Out of data set prediction, predicting probability on test data set with collinear variable of `train$Medical_Degree` removed.  
```{r}
#https://campus.datacamp.com/courses/human-resources-analytics-in-r-predicting-employee-churn/model-validation-hr-interventions-and-roi?ex=1
colnames(test)
prediction_test <- predict(limited.vif.model.kitchen.sink, newdata = test, 
                           type = "response")

hist(prediction_test)
tm_print_save("hist_prediction_test.tiff")
```

```{r}
# Classify predictions using a cut-off of 0.5
prediction_categories <- ifelse(prediction_test > 0.5, 1, 0)
```

Create a confusion matrix 
```{r}
## Creating confusion matrix
test$Match_Status <- as.numeric(test$Match_Status)  -1 
#test$Match_Status

confusion <- table(prediction_categories, test$Match_Status)
knitr::kable(confusion, caption = "Confusion Matrix of non-colinear Variables", digits=2)
```
True negative is `r confusion[1]`.
False negative is `r confusion[1,2]`.  These people were predicted not to match but did match.  
True positive is `r confusion[2,2]`.  These are the people predicted to match who did match.  
False positive is `r confusion[2]`.  These are the people who were predicted to match and did not match.  ??

# Calculate accuracy. 
```{r}
(confusion.limited.vif.model.kitchen.sink <- caret::confusionMatrix(confusion, positive = "1"))
```
The accuracy of the `limited.vif.model.kitchen.sink` model is `r round(confusion.limited.vif.model.kitchen.sink$overall[[1]], digit=3)`.  The sensitivity of the `limited.vif.model.kitchen.sink` model is `r round(confusion.limited.vif.model.kitchen.sink$byClass[[1]], digit=3)`. The specificity of the `limited.vif.model.kitchen.sink` model is `r round(confusion.limited.vif.model.kitchen.sink$byClass[[2]], digit=3)`.

Evaluating the signficance of kitchen.sink variables 
```{r, echo=TRUE, message=FALSE,fig.width=7, fig.asp=1, fig.cap="Figure: Variance-inflation factors for matching into OBGYN."}
tm_chart_strength_of_variables(limited.vif.model.kitchen.sink)
```

# Factor Selection using Forward/Backwards Regression via Variable Importantce
https://livebook.datascienceheroes.com/selecting-best-variables.html

A predictive model with a high number of variables will tend to do overfitting. While on the other hand, a model with a low number of variables will lead to doing underfitting.

After splitting the model in training and validation data, it is necessary to evaluate the variables with a major predictive power. The best method is reviewing the p-values in the regression model, however there are many variables in the model so it will take considerable time to do so manually. In this case, the LASSO regularization algorithm is implemented to identify the variables to be significant on the model. 

Using Regression to Calculate Variable Importance: The summary function in regression also describes features and how they affect the dependent feature through significance.

Start with Stepwise Regression  
Backwards is when you start with a model of all the predictors and then check what happens when each of the predictors is removed.  If removing a variable does not change the ability to predict then the predictor is safely deleted.  This continues step by step until only important predictors remain.    This is just one tool.  

Try forwards stepwise regression here:  
```{r, echo=TRUE, include = FALSE}
#Forward regression!!  #https://campus.datacamp.com/courses/supervised-learning-in-r-classification/chapter-3-logistic-regression?ex=15
# Specify a null model with no predictors
null_model <- glm(formula = Match_Status ~ 1, data = train, family = "binomial")

# Specify the full model using all of the potential predictors
full_model <- glm(formula = formula, data = train, family = "binomial")

# Use a forward stepwise algorithm to build a parsimonious model
step_model <- stats::step(null_model, scope = list(lower = null_model, upper = full_model), direction = "forward")

#Quitting from lines 941-942 (Tyler-markdown-12-31-2019.Rmd) 
# Error in vapply(x, format_sci_one, character(1L), ..., USE.NAMES = FALSE) : 
#   values must be length 1,
#  but FUN(X[[2]]) result is length 3
# Calls: <Anonymous> ... paste -> hook -> .inline.hook -> format_sci -> vapply
# In addition: There were 48 warnings (use warnings() to see them)
# Execution halted
#step_model$formula[[c(3)]]

# The chosen features by forward stepwise:  
# US_or_Canadian_Applicant + USMLE_Step_1_Score + Medical_Degree + 
#     Age + white_non_white + Medical_Education_or_Training_Interrupted + Gender + 
#     Visa_Sponsorship_Needed + Alpha_Omega_Alpha + Couples_Match + 
#     Count_of_Other_than_Published

# Estimate the stepwise matching probability
step_prob <- predict(step_model, type = "response")

# Plot the ROC of the stepwise model
#forwards_ROC <- pROC::roc(train$Match_Status, step_prob,
   # levels=c("1", "0"), direction = "auto")
#plot(forwards_ROC, col = "red", main="Forwards Stepwise Regression \n for Factor Selection")
#pROC::auc(forwards_ROC) 

#tm_print_save("forward_ROC.tiff")
```


Try BACKWARDS stepwise regression here:
```{r, echo=TRUE, include = FALSE}
#BACKWARDS regression!!  #https://campus.datacamp.com/courses/supervised-learning-in-r-classification/chapter-3-logistic-regression?ex=15
# Specify a null model with no predictors
null_model <- glm(formula = formula, data = train, family = "binomial")

# Specify the full model using all of the potential predictors
full_model <- glm(formula = formula, data = train, family = "binomial")

# Use a forward stepwise algorithm to build a parsimonious model
back_step_model <- stats::step(full_model, scope = list(lower = null_model, upper = full_model), direction = "backward")

# Estimate the stepwise matching probability
back_step_prob <- predict(back_step_model, data = test, type = "response")  #train or test data here?

# Plot the ROC of the stepwise model
backwards_ROC <- pROC::roc(train$Match_Status, back_step_prob,
    levels=c("1", "0"), direction = "auto")
plot(backwards_ROC, col = "red", main="Backwards Stepwise Regression \n for Feature Selection")
pROC::auc(backwards_ROC) 

# BACKWARDS step-wise regression model `step_model` has an AUC of `r round(pROC::auc(backwards_ROC)[[1]], digits=2) ` using these predictors `r back_step_model$formula[[c(3)]]`
```

Not working with HTML knitr

#Factor Selection:  Principal Components Analysis
Principal Component Analysis (PCA) is a very powerful technique that has wide applicability in data science, bioinformatics, and further afield. It was initially developed to analyse large volumes of data in order to tease out the differences/relationships between the logical entities being analysed. It extracts the fundamental structure of the data without the need to build any model to represent it. This 'summary' of the data is arrived at through a process of reduction that can transform the large number of variables into a lesser number that are uncorrelated (i.e. the ‘principal components'), whilst at the same time being capable of easy interpretation on the original data (Blighe, Lewis, and Lun 2018) (Blighe 2013).

In order to reduce 17 dimensions or knowing the significant impact of our reduction we will do PCA.

Feature Selection: Principal Component Analysis - Use principal components analysis to pick what predictors to use 
```{r}
#From nomogram_matching_medical_students.R

#Method 1:  Principal Component Analysis - Use principal components analysis to pick what predictors to use 
train_pca <- preProcess(select(all_data, - Match_Status), 
                        method = c("center", "scale", "nzv", "pca"))
train_pca
train_pca$method
train_pca$rotation
train_pca$method$pca
```


Approach 2
```{r}
numeric_data <- select_if(all_data, is.numeric)
numeric_data <- base::scale(numeric_data, center=TRUE, scale = TRUE)
pcaObj <- stats::princomp(numeric_data, cor = TRUE, scores = TRUE, covmat = NULL)
summary(pcaObj)
print(pcaObj)
names(pcaObj)
plot(pcaObj, main="Principal Component Analysis \n Reduction in Number of Variables")
pcaObj$loadings[,1]
#I have no idea what this plot means
stats::biplot(pcaObj, cex = 1.0, main="Biplot of Principal Component Analysis \n Reduction in Number of Variables")
tm_print_save("Principal_Components_for_numeric_data.tiff")

final_data <- as.data.frame(pcaObj$scores)
```
According to PCA,the columns that are of utmost importance are as follows:
***

```{r}
#http://www.sthda.com/english/articles/31-principal-component-methods-in-r-practical-guide/118-principal-component-analysis-in-r-prcomp-vs-princomp/
res.pca <- prcomp(numeric_data, scale = TRUE)
library(factoextra)
factoextra::fviz_eig(res.pca)  #Visualize eigenvalues (scree plot). Show the percentage of variances explained by each principal component.

#tm_print_save("scree_plot_PCA.tiff")

#Graph of individuals. Individuals with a similar profile are grouped together.
factoextra::fviz_pca_ind(res.pca,
             col.ind = "cos2", # Color by the quality of representation
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
#tm_print_save("individuals_plot_PCA.tiff")

# #Graph of variables. Positive correlated variables point to the same side of the plot. Negative correlated variables point to opposite sides of the graph.
factoextra::fviz_pca_var(res.pca,
             col.var = "contrib", # Color by contributions to the PC
             gradient.cols = c("#00AFBB", "#E7B800", "#FC4E07"),
             repel = TRUE     # Avoid text overlapping
             )
#tm_print_save("graph_of_vars_plot_PCA.tiff")

#Biplot of individuals and variables
factoextra::fviz_pca_biplot(res.pca, repel = TRUE,
                col.var = "#2E9FDF", # Variables color
                col.ind = "#696969"  # Individuals color
                )
#tm_print_save("barpot_plot_PCA.tiff")

# Eigenvalues
eig.val <- get_eigenvalue(res.pca)
eig.val

# Results for Variables
res.var <- get_pca_var(res.pca)
res.var$coord          # Coordinates
res.var$contrib        # Contributions to the PCs
res.var$cos2           # Quality of representation
# Results for individuals
res.ind <- get_pca_ind(res.pca)
res.ind$coord          # Coordinates
res.ind$contrib        # Contributions to the PCs
res.ind$cos2           # Quality of representation
```

```{r}
#https://bradleyboehmke.github.io/HOML/pca.html

# convert data to h2o object
h2o::h2o.init()
my_basket.h2o <- as.h2o(all_data)

# run PCA
my_pca <- h2o.prcomp(
  training_frame = my_basket.h2o,
  pca_method = "GramSVD",
  k = ncol(my_basket.h2o), 
  transform = "STANDARDIZE", 
  impute_missing = TRUE,
  max_runtime_secs = 1000
)

my_pca@model$importance
```

Naturally, the first PC (PC1) captures the most variance followed by PC2, then PC3, etc. We can identify which of our original features contribute to the PCs by assessing the loadings. 
```{r}
#https://bradleyboehmke.github.io/HOML/pca.html
my_pca@model$eigenvectors %>% 
  as.data.frame() %>% 
  mutate(feature = row.names(.)) %>%
  ggplot(aes(pc1, reorder(feature, pc1))) +
  geom_point() + 
  ggtitle("Principal Components 1 \n Feature Selection")
```

```{r}
#https://bradleyboehmke.github.io/HOML/pca.html

# Compute eigenvalues
eigen <- my_pca@model$importance["Standard deviation", ] %>%
  as.vector() %>%
  .^2
  
# Sum of all eigenvalues equals number of variables
sum(eigen)
## [1] 42

# Find PCs where the sum of eigenvalues is greater than or equal to 1
which(eigen >= 1)
#Consequently, using this criteria would have us retain the first 3 PCs in my_basket.  

#Scree Plot
data.frame(
  PC  = my_pca@model$importance %>% seq_along,
  PVE = my_pca@model$importance %>% .[2,] %>% unlist()
) %>%
  ggplot(aes(PC, PVE, group = 1, label = PC)) +
  geom_point() +
  geom_line() +
  geom_text(nudge_y = -.002)

#Needs to be saved.  
```
So how many principal components should we use in the my_basket example? The frank answer is that there is no one best method for determining how many components to use. In this case, differing criteria suggest to retain *** (scree plot criterion), and *** (eigenvalue criterion).  

# Feature Selection: MARS model (Multivariate Adaptive Regression Splines)
MARS allows for non-linear models.  The MARS method and algorithm can be extended to handle classification problems and GLMs in general.
https://bradleyboehmke.github.io/HOML/mars.html
http://uc-r.github.io/mars
```{r}
# cross validated model
tuned_mars <- caret::train(
  x = subset(train, select = -Match_Status),
  y = train$Match_Status,
  method = "earth",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)
tuned_mars
tm_print_save("Degree_of_products.tiff")

# best model
tuned_mars$bestTune
##   nprune degree
## 2     12      1
```

Separate approach
```{r}
# Fit a basic MARS model
mars1 <- earth(
  formula = Match_Status ~ .,  
  data = train   
)
mars1
summary(mars1) %>% .$coefficients %>% head(20)
```

```{r}
plot(mars1, which = 1, , main="Model summary capturing GCV  R2 (left-hand y-axis and solid black line) based on the number of terms retained (x-axis) which is based on the number of predictors used to make those terms (right-hand side y-axis). For this model, 11 non-intercept terms were retained which are based on 18 predictors. Any additional terms retained in the model, over and above these 11, result in less than 0.001 improvement in the GCV R2")
tm_print_save(filename = "mars1_model_selection.tiff")
```

In addition to pruning the number of knots, earth::earth() allows us to also assess potential interactions between different hinge functions. The following illustrates this by including a degree = 2 argument. 

You can see that now our model includes interaction terms between a maximum of two hinge functions (e.g., h(Age-34.1890410958904)*US_or_Canadian_Applicantinternational represents an interaction effect for those applicants less than 34 and was not a US_or_Canadian_Applicant).
```{r}
# Fit a basic MARS model
mars2 <- earth(
  Match_Status ~ .,  
  data = train,
  degree = 2
)

# check out the first 10 coefficient terms
summary(mars2) %>% .$coefficients %>% head(10)
```

There are two important tuning parameters associated with our MARS model: the maximum degree of interactions and the number of terms retained in the final model. We need to perform a grid search to identify the optimal combination of these hyperparameters that minimize prediction error (the above pruning process was based only on an approximation of CV model performance on the training data rather than an exact k-fold CV process). As in previous chapters, we’ll perform a CV grid search to identify the optimal hyperparameter mix. Below, we set up a grid that assesses 30 different combinations of interaction complexity (degree) and the number of terms to retain in the final model (nprune).
```{r}
# https://bradleyboehmke.github.io/HOML/mars.html
# create a tuning grid
hyper_grid <- expand.grid(
  degree = 1:3, 
  nprune = seq(2, 100, length.out = 10) %>% floor()
)

head(hyper_grid)
```

Cross-validated RMSE for the 30 different hyperparameter combinations in our grid search. The optimal model retains *** terms and includes up to ***3rd degree interactions.

The  grid search helps to focus where we can further refine our model tuning. As a next step, we could perform a grid search that focuses in on a refined grid space for nprune (e.g., comparing 5–20 terms retained).
```{r}
# Cross-validated model
# https://bradleyboehmke.github.io/HOML/mars.html
set.seed(123)  # for reproducibility
cv_mars <- caret::train(
  x = subset(train, select = -Match_Status),  #Cool
  y = train$Match_Status,
  method = "earth",
  metric = "Accuracy",
  trControl = trainControl(method = "cv", number = 10),
  tuneGrid = hyper_grid
)

# View results
cv_mars$bestTune
##    nprune degree
## 22     12      3  #The model that provides the optimal combination includes third degree interaction effects and retains 12 terms.
cv_mars_plot <- ggplot(cv_mars) + ggtitle("The model that provides the optimal combination \n includes third degree interaction effects \n and retains 12 terms.")
cv_mars_plot
tm_ggsave(cv_mars_plot, "cv_mars_plot.tiff") 
```

GCV = Generalized cross-validation (GCV) procedure,
RSS = residual sums of squares

Variable importance based on impact to GCV (left) and RSS (right) values as predictors are added to the model. Both variable importance measures will usually give you very similar results.
```{r}
# variable importance plots
# https://bradleyboehmke.github.io/HOML/mars.html

p1 <- tm_vip(object = cv_mars, value = "gcv", title = "Generalized cross-validation \n(GCV) procedure")
p2 <- tm_vip(object = cv_mars, value = "rss", title = "Residual Sums of Squares")
gridExtra::grid.arrange(p1, p2, ncol = 2)
tm_print_save(filename = "p1p2.tiff")
```

Its important to realize that variable importance will only measure the impact of the prediction error as features are included; however, it does not measure the impact for particular hinge functions created for a given feature.
```{r}
# extract coefficients, convert to tidy data frame, and
# filter for interaction terms
# https://bradleyboehmke.github.io/HOML/mars.html
optimal_combinations <- cv_mars$finalModel %>%
  coef() %>%  
  broom::tidy() %>%  
  filter(stringr::str_detect(names, "\\*")) 
optimal_combinations
```
Interaction between features show that younger Age and international/US_or_Canadian_Applicant interact positively.  A medical education interruption and USMLE Step 1 score are negative interaction.  There is a three way interaction between (Age-34.6), international applicants (US_or_Canadian_Applicantinternational), Visa_Sponsorship_needed_yes (Visa_Sponsorship_NeededYes) that is negative.  

```{r}
#Method 5:  Variable important from ML algorithm 
# Train the model using randomForest and predict on the training data itself.
model_mars = caret::train(Match_Status ~ ., data=train, method='earth')
model_mars
fitted <- predict(model_mars)
fitted

plot(model_mars, main="Model Accuracies based on \n the number of terms with MARS") 
tm_print_save(filename = "model_mars_plot.tiff")

varimp_mars <- varImp(model_mars)
plot(varimp_mars, main="Variable Importance with MARS") #Visual of the most important factors
tm_print_save("varimp_mars.tiff")
```
The MARS model shows that *** terms were likely to improve accuracy (`model_mars`).  Variable importance shows that *** US_or_Canadian_applicants, USMLE_Step_1_Score, Age, Medial_Degree, and Visa_Sponsorship_Needed all show significant importance (`varimp_mars`).  

#Feature Selection: Recursive Feature Elimination 
So at first the model is fit on the data. Then we have coefficients of each feature or feature importance. We drop the feature with least coefficient or importance. Then the model is fit on the remaining features. The process is repeated until we have a necessary number of features (or some other criteria is fulfilled).  
```{r}
#Method 2: Recursive Feature Elimination
options(warn=-1)
subsets <- c(1:17)
ctrl <- rfeControl(functions = rfFuncs,
                   method = "repeatedcv",
                   repeats = 2,
                   verbose = FALSE)

# nrow(drivers)
# lmProfile <- rfe(x=train[, 1:17], y=train$Match_Status,
#                  sizes = subsets,
#                  rfeControl = ctrl)
# lmProfile 


lmProfile <- rfe(x=train[, 1:17], y=train$Match_Status,
                 sizes = subsets,
                 rfeControl = ctrl)
lmProfile  #Picked 5 variables that were able to predict 

#https://www.datacamp.com/community/tutorials/feature-selection-R-boruta
```

# Feature Selection: Boruta search
```{r}
#Method 3:  Boruta search
#You'll see how you can use it to perform a top-down search for relevant features by comparing original attributes' importance with importance achievable at random, estimated using their permuted copies, and progressively elliminating irrelevant features.
set.seed(123)
boruta_output <- Boruta::Boruta(Match_Status ~ ., data=na.omit(train), doTrace=0)  
names(boruta_output)
Boruta::getConfirmedFormula(boruta_output)
Boruta::attStats(boruta_output) 

# Get significant variables including tentatives
boruta_signif <-Boruta::getSelectedAttributes(boruta_output, withTentative = TRUE)
print(boruta_signif)  
boruta_signif1 <- as_tibble(boruta_signif)
write_csv(boruta_signif1, (here::here("results", "boruta_signif.csv")))

roughFixMod <- TentativeRoughFix(boruta_output)  # Do a tentative rough fix
boruta_signif <- getSelectedAttributes(roughFixMod)

imps <- attStats(roughFixMod)
imps2 = imps[imps$decision != 'Rejected', c('meanImp', 'decision')]
head(imps2[order(-imps2$meanImp), ])  # descending sort
BorutaImportance <- plot(boruta_output, cex.axis=0.35, las=2, xlab="", main="Boruta Variable Importance")  # Plot variable importance
plot(BorutaImportance)
tm_print_save("Boruta_Variable_Importance.tiff")
```

# Feature Selection: GLMNet to do factor selection with the previously made LASSO model
Regularization methods provide a means to constrain or regularize the estimated coefficients, which can reduce the variance and decrease out of sample error.

The glmnet package is extremely efficient and fast, even on very large data sets (mostly due to its use of Fortran to solve the lasso problem via coordinate descent); note, however, that it only accepts the non-formula XY interface so prior to modeling we need to separate our feature and target sets.

```{r, echo=TRUE}
# https://bradleyboehmke.github.io/HOML/regularized-regression.html#attrition-data
#https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net

# Create custom trainControl: myControl
set.seed(1978)
myControl <- 
  trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 5,
    summaryFunction = twoClassSummary,
    classProbs = TRUE, # IMPORTANT!
    verboseIter = FALSE)

dim(train)
#train$Match_Status
train$Match_Status <-
  as.factor(train$Match_Status)

test$Match_Status <-
  as.factor(test$Match_Status)
#test$Match_Status

#Levels of the target outcome variable for glmnet need to be words and not numbers.
levels(train$Match_Status) <-
  c("No.Match", "Matched")

levels(test$Match_Status) <-
  c("No.Match", "Matched")

#train$Match_Status
levels(train$Match_Status)
class(train$Match_Status)

levels(all_data$Match_Status)
class(all_data$Match_Status)

dim(train)
sum(is.na(train))
```

Create the LASSO using glmnet within the caret package.  Here we are solely using the train dataset to determine what varaiables predict the outcome.  

The alpha parameter tells glmnet to perform a ridge (alpha = 0), lasso (alpha = 1), or elastic net (0 < alpha < 1) model. 

```{r}
# https://bradleyboehmke.github.io/HOML/regularized-regression.html

# Train glmnet with custom trainControl and tuning: model
set.seed(1978)
glm.kitchen.sink <- glm(formula = formula, family = "binomial", train)
glm.kitchen.sink

lasso.mod <- 
  caret::train(
    Match_Status ~ .,
    data = train,
    family = "binomial",
    tuneGrid = expand.grid(
      alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)
    ),
    method = "glmnet",
    metric = "ROC",
    trControl = myControl)
```

```{r, echo=TRUE, include=FALSE}
lasso.mod[["results"]]
lasso.mod$bestTune #Final model is more of a ridge and less of a LASSO model

best <- 
  lasso.mod$finalModel

coef(best, s=lasso.mod$bestTune$lambda) ###Look for the largest coefficient
```
Final model is more of a ridge and less of a LASSO model:  `r lasso.mod$bestTune `

Plot the results of the lasso.mod so we can see if this is more ridge or more lasso.  0 = ridge regression and 1 = LASSO regression, here ridge is better.

```{r, fig.width=7, fig.asp=1, fig.cap="Figure: Plotting the results of ridge or lasso in regression"}
plot(lasso.mod, main = "More ridge regression than LASSO")
tm_print_save("lasso_mod.tiff")
```
  
Plot LASSO factors - Plot the individual variables by lambda.  Saves the lasso.mod to an RDS file for later use.  Coefficients for our ridge regression model as λ grows from  0 → ∞
 
```{r,  fig.asp=1}
lasso.mod.plot <- plot(lasso.mod$finalModel, xvar = 'lambda', label = TRUE, main = "Plot of the LASSO factors")
tm_print_save(filename = "lasso_mod_plot.tiff")

print("Coefficients for our ridge regression model as λ grows from  0 → ∞")
#legend("topright", lwd = 1, col = 1:5, legend = colnames(train), cex = .6)
#https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net

colnames(train[1:17])
saveRDS(lasso.mod, "best.LASSO.rds")  #save the model
```

Makes predictions of matching based on the lasso.mod using the training data.  
```{r, echo=TRUE, warning=FALSE, include=FALSE}
predict(lasso.mod, newx = x[1:5,], type = "prob", s = c(0.05, 0.01))
```

And we use the glmnet library to determine the optimal penalization parameter. Note that this must be assigned through cross validation; here, we use 50-fold cross validation (only suitable in small datasets).  GLMnet accepts data in a matrix format so the data format was changed before giving it to glmnet.cv. Match_Status ~ . is shorthand for all predictors.

```{r, echo=TRUE, message=FALSE, include=TRUE, fig.cap="Figure: Determining optimal penalization parameters for factor selection with the least absolute shrinkage and selection operator (LASSO) model."}
`%nin%`<-Negate(`%in%`)
# save the outcome for the glmnet model, could use dummyVars with fullRan=FALSE can remove collinearity by removing male.gender so you are either male or female

# Create training  feature matrices
# we use model.matrix(...)[, -1] to discard the intercept
x <- model.matrix(train$Match_Status~., data=train)

class(x)

x <- x[,-1]  #Removes intercept

set.seed(356)
glmnet1 <- 
  cv.glmnet(x=x,
            y=train$Match_Status,
            nfolds=10,
            alpha=.5, 
            family="binomial")

misclassification_error <- plot(glmnet1,main = "Misclassification Error")
misclassification_error

tm_ggsave(misclassification_error, "misclassification_error.tiff")
```
The left vertical line represents the minimum error, and the right vertical line represents the cross-validated error within 1 standard error of the minimum. LASSO, least absolute shrinkage and selection operator

If you look at this graph we ran the model with a range of values for lambda and saw which returned the lowest cross-validated error. You'll see that our cross-validated error remains consistent until we hit the dotted lines, where we start to see our model perform very poorly due to underfitting with misclassification error.  Cross validation is an essential step in studies to help up us not only calibrate the parameters of our model but estimate the prediction accuracy with unseen data.

CV ridge regression with alpha at 0.0

Coefficients for our ridge regression model as λ  grows from 0 → ∞.
```{r}
# https://bradleyboehmke.github.io/HOML/regularized-regression.html
# Apply CV ridge regression to Matching data
ridge <- cv.glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 0, 
  family="binomial")
tm_print_save("ridge.tiff")
```

We can also access the coefficients for a particular model using coef(). glmnet stores all the coefficients for each model in order of largest to smallest λ. 
```{r}
# lambdas applied to penalty parameter
ridge$lambda %>% head()
```


CV lasso regression with alpha at 1.0
```{r}
# Apply CV lasso regression to Matching data
lasso <- cv.glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 1,
  family="binomial")

# plot results
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
tm_print_save("ridgeandlassopenalty.tiff")
```

```{r}
# Ridge model
min(ridge$cvm)       # minimum MSE
ridge$lambda.min     # lambda for this min MSE

ridge$cvm[ridge$lambda == ridge$lambda.1se]  # 1-SE rule
ridge$lambda.1se  # lambda for this MSE

# Lasso model
min(lasso$cvm)       # minimum MSE
lasso$lambda.min     # lambda for this min MSE

lasso$cvm[lasso$lambda == lasso$lambda.1se]  # 1-SE rule
lasso$lambda.1se  # lambda for this MSE
```

Coefficients for our ridge and lasso models. First dotted vertical line in each plot represents the λ with the smallest MSE and the second represents the λ with an MSE within one standard error of the minimum MSE.
```{r}
# Ridge model
ridge_min <- glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 0,
  family="binomial")

# Lasso model
lasso_min <- glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 1,
  family="binomial")

par(mfrow = c(1, 2))
# plot ridge model
plot(ridge_min, xvar = "lambda", main = "Ridge penalty\n\n")
abline(v = log(ridge$lambda.min), col = "red", lty = "dashed")
abline(v = log(ridge$lambda.1se), col = "blue", lty = "dashed")
tm_print_save(filename = "tidge_abline.tiff")

# plot lasso model
plot(lasso_min, xvar = "lambda", main = "Lasso penalty\n\n")
abline(v = log(lasso$lambda.min), col = "red", lty = "dashed")
abline(v = log(lasso$lambda.1se), col = "blue", lty = "dashed")
tm_print_save(filename = "lasso_abline.tiff")
```

So far we’ve implemented a pure ridge and pure lasso model. However, we can implement an elastic net the same way as the ridge and lasso models, by adjusting the alpha parameter. Any alpha value between 0–1 will perform an elastic net. 

When alpha = 0.5 we perform an equal combination of penalties whereas alpha <0.5 will have a heavier ridge penalty applied and alpha  > 0.5 will have a heavier lasso penalty.
```{r}
elastic_net_one_quarter <- glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 0.25,  #
  family="binomial")
plot(elastic_net_one_quarter, main = "Elastic net with alpha 0f 0.25")
tm_print_save(filename = "elastic_net_one_quarter.tiff")

elastic_net_three_quarter <- glmnet(
  x = x,
  y=train$Match_Status,
  alpha = 0.75,  #
  family="binomial")
plot(elastic_net_three_quarter, main = "Elastic net with alpha 0f 0.75")
tm_print_save("elastic_net_three_quarter.tiff")
```


```{r}
# for reproducibility
set.seed(123)

# grid search across 
cv_glmnet <- caret::train(
  x = x,
  y=train$Match_Status,
  family = "binomial",
  method = "glmnet",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)

# model with lowest RMSE
cv_glmnet$bestTune

# plot cross-validated RMSE
cv_glmnet_plot <- ggplot(cv_glmnet) + ggtitle("The 10-fold cross valdation accuracy of cv_glmnet_plot \n across 10 alpha values (x-axis) \n and 10 lambda values (line color)")
cv_glmnet_plot

tm_ggsave(cv_glmnet_plot, "cv_glmnet_plot.tiff")
```
So how does this compare to our previous best model for the train data set? Introducing a penalty parameter to constrain the coefficients provided some improvement over our previously obtained dimension reduction approach.  See figure with The 10-fold cross valdation accuracy across 10 alpha values (x-axis) and 10 lambda values (line color).  

```{r}
# predict matching_status on training data
pred <- stats::predict(cv_glmnet, x)
pred
```

# Feature Selection : Magnitude Feature Interpretation
Variable importance for regularized models provides a similar interpretation as in logistic regression. Importance is determined by magnitude of the standardized coefficients and we can see in figure below.  

```{r}

feature_interpretation_regularized_regression <- tm_vip(object = cv_glmnet, title = "Variable Importance for \n Regularized Regression Model")
feature_interpretation_regularized_regression

tm_ggsave(object = feature_interpretation_regularized_regression, filename = "feature_interpretation_regularized_regression.tiff")
```
Penalized regression model shows that USMLE Step 1 Score is the most important followed by IMG and followed by Age.  


# Review of all feature Selection Results
```{r}
#Forward regression
step_model$formula[[c(3)]] #Important variables

#MARS
plot(tuned_mars)
summary(mars1) %>% .$coefficients %>% head(20)
gridExtra::grid.arrange(p1, p2, ncol = 2)

cv_mars$bestTune
##    nprune degree
## 22     12      3  #The model that provides the optimal combination includes third degree interaction effects and retains 12 terms.
plot(model_mars, main="Model Accuracies based on \n the number of terms with MARS")  # Number of terms to use to optimize accuracy

optimal_combinations

plot(varimp_mars, main="Variable Importance with MARS") #Visual of the most important factors



#Principal Components Analysis foudn thhat six factors were needed to capture 95% of the variance:
train_pca$method$pca
#Alternative PCA
pcaObj$loadings[,1]

#Using recursive feature eliminiation found eight variables were needed:
lmProfile

#Boruta
boruta_output
boruta_signif

#Variable Importance of Each Variable in the Regularized Regression Model
feature_interpretation_regularized_regression

#Boruta is nice because it tells you what is rejected and what was included.  
boruta_output
boruta_output$finalDecision
plot(BorutaImportance)
boruta_signif

# plot results, create grid of all importance graphs?
par(mfrow = c(1, 2))
plot(ridge, main = "Ridge penalty\n\n")
plot(lasso, main = "Lasso penalty\n\n")
```



Comparing logistic regression model with penalized regression model. 
```{r}
# train logistic regression model
set.seed(123)
glm_mod <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
  )

# train regularized logistic regression model
set.seed(123)
penalized_mod <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
  )
```


```{r}
# extract out of sample performance measures
summary(resamples(list(
  logistic_model = glm_mod, 
  penalized_model = penalized_mod, 
  mars_model = tuned_mars
  )))$statistics$Accuracy
```
The output shows minimal improvement with the penalized model (median accuracy = 0.809) compared to the logistic model (median accuracy = 0.803). 


#Factor Selection: LASSO (Least Absolute Shrinkage and Selection Operator)
Also, I like lasso because some people will find that using predictors of age, race, gender as predictors will be discriminatory.  In short, LASSO eliminates the need for an author to be a subject expert on matching when selecting variables.  

Here, we use Lasso for simplicity and interpretability. The aim is to avoid over-parametrization and unnecessary model bias by carrying feature selection on-the-go. Key to this task will be cross-validation.  Start by creating a custom train control providing the number of cross-validations and setting the classProbs to TRUE for logistic regression. 

Factor Selection: Variable selection using LASSO in the train dataset
```{r, echo=TRUE, warning=FALSE, message=FALSE}
c <- 
  coef(glmnet1,s='lambda.min',exact=TRUE)  #Bring in the coefficients from LASSO

inds <-
  which(c!=0)   #Pick which coefficients are not zero

variables <-    #Select the row names for the coefficients that are not zero by subsetting
  row.names(c)[inds]

variables <-    #List out the variables LASSO chose exempting the intercept variable
  variables[variables %nin% '(Intercept)']

variables
```
This feature selection with LASSO was not very helpful because it eliminated zero features (see `variables`).  See VIP figure above (`feature_interpretation_regularized_regression`) for a clearer answer as to what features to select.  

```{r, results="asis"}
knitr::kable(variables, caption = "Variables Chosen by LASSO to Predict Matching into OBGYN based on the Train Data")
```
  
## Factor Selection: Revise GLM Model with factors selected by LASSO
Creating a more parsiomonious model using the variables selected by LASSO in the train dataset. 

```{r}
limited.to.lasso.variables <- glm(formula = formula, 
                 family = "binomial", data = train)

rms::vif(limited.to.lasso.variables)
```

```{r, echo=TRUE, results="asis", warning=FALSE, include=TRUE}
#lrm
#Print out variables that LASSO found were helpful.  
print(variables)  # Include these variables into the new model called lrm.with.lasso.variables

d <- 
  rms::datadist(test)

options(datadist = "d")

lrm.with.lasso.variables <- 
  rms::lrm(Match_Status ~ 
                                    white_non_white + 
                                    Age +
                                    Gender +
                                    Couples_Match +
                                    US_or_Canadian_Applicant +
                                    Medical_Education_or_Training_Interrupted + 
                                    Alpha_Omega_Alpha + 
                                    USMLE_Step_1_Score + 
                                    Count_of_Oral_Presentation + 
                                    Visa_Sponsorship_Needed,
                                    #Medical_Degree, #Removed due to VIF issues of collinearity
      data = train, 
      x = T, 
      y = T)

#lrm.with.lasso.variables$stats  #Shows the C-statistic and the Brier score.  
knitr::kable(broom::tidy(lrm.with.lasso.variables$stats), digits =2, caption = "Performance statistics of the Training Model")

round(lrm.with.lasso.variables$stat[[6]], digits = 2)  #C-statistic
```

C-statistics of the `lrm.with.lasso.variables` model with variables chosen by LASSO is: `r round(lrm.with.lasso.variables$stat[[6]], digits = 2)`.

```{r, echo=TRUE,  fig.width=7, fig.asp=1, fig.cap="Figure: Charting the strength of the variables chosen using LASSO.", include=TRUE}
lrm.with.lasso.variables
tm_chart_strength_of_variables(lrm.with.lasso.variables)
```

```{r, include=F}
summary(lrm.with.lasso.variables)
```

# Odds ratios of the `train` dataset

Odds ratios in graph form in the train dataset.  
```{r, echo=TRUE,  include = TRUE, fig.width=7, fig.asp=1, fig.cap="Figure: Odds ratios of the training data set to predict matching into OBGYN residency."}
plot(summary(lrm.with.lasso.variables), cex=1.2, cex.lab=0.7, cex.axis = 0.7)
#https://rstudio-pubs-static.s3.amazonaws.com/283447_fd922429e1f0415c89b93b6da6dc1ccc.html

tm_print_save("odds_ratio_image.tiff")
```

```{r, results="asis"}
#For example, increase one unit in age will decrease the log odd of survival by 0.039; being a male will decrease the log odd of survival by 2.7 compared to female; and being in class2 will decrease the log odd of survival by 0.92, being in class3 will decrease the log odd of survival by 2.15. 
oddsratios <- 
  as.data.frame(exp(cbind("Adjusted Odds ratio" = coef(lrm.with.lasso.variables),
                          confint.default(lrm.with.lasso.variables, level = 0.95))))

knitr::kable(oddsratios, digits = 2)
```

Annotation for Manuscript Table:  A:  Nonlinear component A of the function describing the variable and the probability of matching into OBGYN.  B:  Nonlinear component B of the function describing the variable and the probability of matching into OBGYN.  C:  Nonlinear component C of the function describing the variable and the probability of matching into OBGYN.  


# 1) Model: General logistic regression model Use Model to predict match for Test Data
Shift Gears: Test Accuracy of Model on Training Data, Use glmnet model on 207 and 2018 `test` data.   Run the 2017, 2018 data through the train model.  

Build both a glm (train.glm.with.lasso.variables) and a lrm model (train.lrm.with.lasso.variables) here with the same predictor variables.  

```{r, echo=TRUE, warning=FALSE}
#It would be nice to create a character variable that we drop features from.  
train.glm.with.lasso.variables  <- 
  stats::glm(Match_Status ~ white_non_white + Alpha_Omega_Alpha + ACLS + BLS + PALS + Citizenship + Gold_Humanism_Honor_Society + Misdemeanor_Conviction + Sigma_Sigma_Phi + Age + Medical_Education_or_Training_Interrupted + Gender + Couples_Match + US_or_Canadian_Applicant + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Peer_Reviewed_Journal_Articles_Abstracts + Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published + Count_of_Peer_Reviewed_Online_Publication + Count_of_Non_Peer_Reviewed_Online_Publication + Count_of_Scientific_Monograph + Count_of_Other_Articles + Visa_Sponsorship_Needed + Medical_School_Type + Medical_Degree + USMLE_Step_2_CK_Score,
      data = train, 
      family = "binomial"(link=logit))  

summary(train.glm.with.lasso.variables)  #check the model
exp(train.glm.with.lasso.variables$coefficients)
confint(train.glm.with.lasso.variables)

#Zhang book page 75
Predprob <- predict(train.glm.with.lasso.variables, type = "response")
library(Deducer)
plot(train.glm.with.lasso.variables, main = "ROC curve", colorize = T)
```


```{r, echo=TRUE, warning=FALSE}
train.lrm.with.lasso.variables <- 
  rms::lrm(formula = Match_Status ~ 
                                    white_non_white + 
                                    Age +
                                    Gender +
                                    Couples_Match +
                                    US_or_Canadian_Applicant +
                                    Medical_Education_or_Training_Interrupted + 
                                    Alpha_Omega_Alpha + 
                                    USMLE_Step_1_Score + 
                                    Count_of_Oral_Presentation + 
                                    Visa_Sponsorship_Needed,
                                    #Medical_Degree, #Removed due to VIF issues of collinearity
              data = train,
           x=TRUE, y=TRUE)
```

First, we need to fit lrm.with.lasso.variables in GLM, rather than rms, to get the AUC.  There is probably a better way to do this.  Using the test data set.  Also built the same model in lrm.  

The Receiver Operating Characteristic (ROC) curve is plotted below for false positive rate (FPR) in the x-axis vs. the true positive rate (TPR) in the y-axis. It shows the detection of true positive while avoiding the false positive. This is the same as measuring the unspecificity (1 - specificity) in x-axis, against the sensitivity in y-axis. This ROC curve in particular shows that its very closed to the perfect classifier meaning that its better at identifying the positive values. 
Use Model to predict match Status for Test Data
```{r}
#Use Model to predict match Status for Test Data
##For prediction
# predict class
pred_class <- rms::Predict(train.glm.with.lasso.variables)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class, ref = "Matched"), 
  reference = relevel(train$Match_Status, ref = "Matched")
)

prob <- 
  predict(train.glm.with.lasso.variables, newdata = test, type="response")
dim(test)
plot(prob, main = "Probability of Matching based on \n train.glm.with.lasso.variables model")

pred <- 
  prediction(prob, test$Match_Status)  #removed na.omit
```

```{r}
#https://github.com/mufflyt/Diabetes-Patient-Readdmission-Prediction/blob/master/Patient%20Readmission%20Risk%20Prediction.r

# Accuracy of Model
GLM_Pred <- ifelse(prob > 0.5, 1, 0)
pred2 <- ifelse(GLM_Pred == 1, "Match", "No.Match")
#Number True and Number False
table(pred2)["Match"]

# test$Match_Status1 <- as.numeric(test$Match_Status) - 1
#In GLM_Pred == test$Match_Status1 :
  # longer object length is not a multiple of shorter object length

#Accuracy Equation
# GLM_Accuracy <- ifelse(GLM_Pred == test$Match_Status1,1,0) #Not working

#table for accuracy
#table(GLM_Accuracy)

#Calculating True vs. total
# Accuracy_GLM <- (sum(GLM_Accuracy)/count(test))*100
# Accuracy_GLM

#confusion matrix  #Not working from here
#confusionMatrix(as.factor(GLM_Pred),as.factor(test$Match_Status1), positive = "1")  ##????NOT WORKING

#ROC curve for GLM
library(ROCR)
#pred_glm <- prediction(as.numeric(as.character(prob)),  ###NOT WORKING??? as.numeric(as.character(test$Match_Status)))
#perf_glm <- performance(pred, "tpr", 'fpr')    ###NOT WORKING???
# plot(perf_glm, main = "ROC curve", colorize = T)
# abline(0,1, col='gray60')
# 
# summary(perf_glm)

#auc_ROCR <- performance(pred_glm, measure = "auc") ###NOT WORKING???
# auc_ROCR <- auc_ROCR@y.values[[1]]
# auc_ROCR
```


ROC: ROC ggplot with nice controls
```{r, include = TRUE}
# rest of this doesn't need much adjustment except for titles
perf <-
  performance(pred, measure = "tpr", x.measure = "fpr")

auc <- 
  performance(pred, measure="auc")

auc <- 
  round(auc@y.values[[1]],3)

roc.data <- 
  data.frame(fpr=unlist(perf@x.values),
             tpr=unlist(perf@y.values),
             model="GLM")

roc_plot <- ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  labs(title = paste0("ROC Curve with area under the curve = ", auc),
       subtitle = "Model: train.glm.with.lasso.variables")
roc_plot

tm_ggsave(roc_plot, "roc_plot.tiff")
```

ROC: ROC with nice labels on the x and y
```{r, include = TRUE}
pred <- 
  prediction(prob, test$Match_Status)

perf <- 
  performance(pred, measure = "tpr", x.measure = "fpr")

plot(perf)

auc <- 
  performance(pred, measure = "auc")

auc <- 
  auc@y.values[[1]]

auc  
```

ROC: ROC in color
```{r, include = TRUE}
perf <- 
  performance(pred, 'tpr','fpr')

plot(perf, colorize = TRUE, text.adj = c(-0.2,1.7), main="Receiver-Operator Curve for Model A")

#Plots of Sensitivity and Specificity
perf1 <- 
  performance(pred, "sens", "spec")

plot(perf1, colorize = TRUE, text.adj = c(-0.2,1.7), main="Sensitivity and Specificity for Model A")

## precision/recall curve (x-axis: recall, y-axis: precision)
perf2 <- 
  performance(pred, "prec", "rec")

plot(perf2, colorize = TRUE, text.adj = c(-0.2,1.7), main="Precision and Recall for Model A")
```

Exploratory random forest was also performed. The variable importance for the random forest model was summarized in the figure below. 
```{r}
#https://bradleyboehmke.github.io/HOML/logistic-regression.html

model3 <- glm(
  formula = formula,
  family = "binomial", 
  data = train
  )

tidy(model3)
```

```{r}
set.seed(123)
cv_model3 <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```

```{r}
#Error in resamples.default(list(model3 = cv_model3)) : 
  #at least two train objects are needed

# summary(
#   resamples(
#     list(
#       model3 = cv_model3
#     )
#   )
# )$statistics$Accuracy
```

```{r}
# predict class
pred_class <- stats::predict(cv_model3, train)
pred_class


factor(train$Match_Status, levels= c("Matched", "Did not match"))
levels(train$Match_Status)
class(train$Match_Status)

pred_class
levels(pred_class)

#create confusion matrix
caret::confusionMatrix(
  data = stats::relevel(pred_class, ref = "Matched"),
  reference = stats::relevel(train$Match_Status, ref = "Matched")
)
```

We can perform a partial least squares (PLS) logistic regression to assess if reducing the dimension of our numeric predictors helps to improve accuracy. There are 17 numeric features in our data set so the following code performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1–17. The optimal model uses 1 or 5 principal components, which is not reducing the dimension. However, the mean accuracy of 0.876 is no better than the average CV accuracy of cv_model3 (0.876).
```{r}
# Perform 10-fold CV on a PLS model tuning the number of PCs to 
# use as predictors
set.seed(123)
cv_model_pls <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "pls",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 10
)

# Model with lowest AUC
cv_model_pls$bestTune

# Plot cross-validated AUC
cv_model_pls_plot <- ggplot2::ggplot(cv_model_pls) + ggtitle(label = "The 10-fold cross-validation AUC \n obtained using PLS with \n 1–10 principal components.")
cv_model_pls_plot

tm_ggsave(cv_model_pls_plot, "cv_model_pls_plot.tiff")
```
The 10-fold cross-validation AUC obtained using partial least squares with 1–10 principal components.

```{r}
tm_vip(object = cv_model3, title = "CV_model3")
tm_print_save("cv_model3.tiff")
```
Using vip::vip() we can extract our top 10 influential variables. The figures illustrates that 'USMLE_Step_1_Score' is the most influential followed by 'US_or_Canadian_Applicant - International', and 'Age'.

```{r}
# https://github.com/tobiolatunji/Readmission_Prediction/blob/master/diabetes_readmission.R
# pseudo R-squared for logistic regression model
logisticPseudoR2s(train.glm.with.lasso.variables)
```


**(2) Regression Tree model**
# 2)  Model: A CART model was fit using the rpart package. 

Trees can handle both factors and continuous variables and do not need to create dummy variables.   Use the divide-and-conquer (aka recursive partinioning e.g. rpart) technique to create two homogenous groups.  Leaf nodes are at the bottom and denote final decisions on matching success. Random Forest is a more powerful algorithm over just a single tree. However, the Decision Tree classification preserve the interpretability which the random forest algorithm lacks.  A simple decision tree model was used for exploration. The Decision Tree does not require feature scaling but can overfit data by fitting the noise instead of the whole data.  It is very important to evaluate decision trees on data it has not seen before.

Here we intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.
```{r rpart EDA}
#https://bradleyboehmke.github.io/HOML/DT.html
#https://www.datacamp.com/community/tutorials/decision-trees-R 
t.model <-
  rpart(as.factor(Match_Status) ~.,
        data = train,     
        method = "class", #Builds a classification tree
        control = rpart.control(cp = 0, maxdepth = 6), #Decreaseing this CP control made the tree much more complicated
        minsplit = 20) 

t.model$variable.importance
```

```{r fancyR plot rpart EDA, include=TRUE}
# Tree visualization
tm_rpart_plot(t.model)
tm_print_save(filename = "tm_rpart_plot.tiff")
```
Using the model to make Match_Status predictions on the test dataframe
```{r}
#using the model to make Match_Status predictions on the test dataframe
solution_tree_probability <- predict(t.model, newdata = test, type="prob")  #predict(model made with training data, test data)

solution_tree_class <- predict(t.model, newdata = test, type="class")  #predict(model made with training data, test data)

table(solution_tree_class, test$Match_Status)
# Compute the accuracy on the test dataset
round(mean(solution_tree_class == test$Match_Status), digits = 3)
```
The accuracy or correct classification rate of the decision tree called `t.model` is `r round(mean(solution_tree_probability == test$Match_Status), digits = 3)`.  

Look at pruning the tree branches to determine the optimal complexity vs. accuracy point for stopping branch growth.  Pruning complexity parameter (cp) plot illustrating the relative cross validation error (y-axis) for various cp values (lower x-axis). Smaller cp values lead to larger trees (upper x-axis). Using the 1-SE rule, a tree size of 10-12 provides optimal cross validation results.
```{r}
plotcp(t.model)
tm_print_save(filename = "plotcp.tiff")

printcp(t.model)
tm_print_save(filename = "tree_sizes.tiff")
```

The figure shows the pruning complexity parameter plot for a fully grown tree. Significant reduction in the cross validation error is achieved with tree sizes 88-100 and then the cross validation error levels off with minimal or no additional improvements.
```{r}
t.model2 <- rpart(
    formula = formula,
    data    = train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(t.model2)
abline(v = 34, lty = "dashed")  #Not sure if 11 is ideal.  
#https://bradleyboehmke.github.io/HOML/DT.html
tm_print_save("plotcp_for_t_model2.tiff")
```

```{r}
# rpart cross validation results
t.model2$cptable

# caret cross validation results
t.model3 <- caret::train(
  Match_Status ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

t.model3.plot <- ggplot(t.model3)
t.model3.plot

tm_ggsave(t.model3.plot, "t_model3_plot.tiff")
```

Feature interpretation of Decision Trees
Variable importance based on the total reduction in accuracy for the Match_Status decision tree.
```{r}
tm_vip(t.model3)
tm_print_save("vip_for_t.model3.plot.tiff")
```

```{r}
#########NOT WORKING


# # Construct partial dependence plots
# p1 <- pdp::partial(t.model3, pred.var = "Age") %>% autoplot() #Not working
# p2 <- pdp::partial(t.model3, pred.var = "USMLE_Step_1_Score") %>% autoplot()
# p3 <- partial(t.model3, pred.var = c("Gr_Liv_Area", "Year_Built")) %>% 
#   plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
#               colorkey = TRUE, screen = list(z = -20, x = -60))
# 
# # Display plots side by side
# gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```


```{r}
pruned.t.model <- rpart::prune(t.model, cp = 0.005) #Set the cp where the complexity to accuraty rate plateaued.  
tm_rpart_plot(pruned.t.model)
tm_print_save("pruned_t_model.tiff")

summary(pruned.t.model)
```
Based on the complexity plot, prune the tree to a complexity of 0.005 using the prune() function with the tree and the complexity parameter.  Now Compute the accuracy of the pruned tree.  

```{r}
pred = predict(pruned.t.model, test, type = "class")

caret::confusionMatrix(as.factor(test$Match_Status), pred)  #In Ops.factor(predictedScores, threshold) : ‘<’ not meaningful for factors

(pruned.t.model)

#Not working
# pred_ct <- prediction(as.numeric(as.character(pred)), as.numeric(as.character(test$Match_Status)))
# perf_ct <- performance(pred_ct, "tpr", 'fpr')
# plot(perf_ct, main = "ROC curve", colorize = T)
# abline(0,1, col='gray60')
```

```{r}
# Compute the accuracy of the pruned tree
solution_tree <- predict(pruned.t.model, newdata = test, type = "class")
mean(solution_tree == test$Match_Status)
```
The accuracy or correct classification rate of the PRUNED decision tree called `pruned.t.model` is `r round(mean(solution_tree == test$Match_Status), digits = 3)`.  Not a huge benefit in accuracy but the model will be more understandable and be faster to run now that it is pruned.  


\pagebreak

# 3) Model: a Support Vector Machine model

Is the data linearly separable?
In order to use SVM, we need to remember to do one thing - Feature Scaling! Because the SVM classifier predicts the class of a given test observation by identifying the observations that are nearest to it, the scale of the variables matters.

The next algorithm that I want to use is SVM, as it is known to work well with small datasets. SVM is a data classification method that separates data using hyperplanes. SVM can be used to generate multiple separating hyperplanes such that the data space is divided into segments and each segment contains only one kind of data. SVM technique is generally useful for data which has non-regularity which means, data whose distribution is unknown.

We want to find the “most optimal” solution. What will then be the characteristic of this most optimal line? We have to remember that this is just the training data and we can have more data points which can lie anywhere in the subspace. If our line is too close to any of the datapoints, noisy test data is more likely to get classified in a wrong segment. 

```{r, include=TRUE}
#SVM with caret, method #1
set.seed(2017)

caret_svm <- caret::train(Match_Status ~ ., 
                          data=train, 
                          method='svmLinear', 
                          preProcess= c('center', 'scale'), trControl=trainControl(method="cv", number=5))

caret_svm
caret_svm$results
caret_svm$finalModel
```
The in-sample accuracy of the supported vector machine model is `r round(caret_svm$results[[2]], digits=3)`.  

```{r}
#using the model to make Survival predictions on the test set
solution_svm <- predict(caret_svm, newdata = test, type = "raw")
mean(solution_svm == test$Match_Status)

#ROC Curve
SVM_ROC <-roc(response=test$Match_Status, predictor= factor(solution_svm,ordered = TRUE), plot=TRUE) 
plot(SVM_ROC, main = "ROC Curve for Supported \n Vector Machines Model")
tm_print_save("SVM_ROC.tiff")
```

```{r}
#https://bradleyboehmke.github.io/HOML/svm.html

# Tune an SVM with radial basis kernel, method #2
set.seed(1854)  # for reproducibility
churn_svm <- caret::train(
  Match_Status ~ ., 
  data = train,
  method = "svmRadial",               
  preProcess = c("center", "scale"),  
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
)
```

```{r}
# Plot results
#https://bradleyboehmke.github.io/HOML/svm.html
churn_svm_plot <- ggplot(churn_svm) + theme_light()
churn_svm_plot

tm_ggsave(churn_svm_plot, "churn_svm_plot.tiff")
```
Plotting the results, we see that smaller values of the cost parameter (C≈0-1) provide better cross-validated accuracy scores for these training data.

```{r}
# Print results
#https://bradleyboehmke.github.io/HOML/svm.html
churn_svm$results
```
Similar to before, we see that smaller values of the cost parameter ( 
C≈0-1) provide better cross-validated AUC scores on the training data. 

```{r}
#https://bradleyboehmke.github.io/HOML/svm.html
class.weights = c("No" = 1, "Yes" = 10)
```

```{r}
#Takes FOREVER!

#https://bradleyboehmke.github.io/HOML/svm.html
# Control params for SVM
ctrl <- trainControl(
  method = "cv",
  number = 10,
  classProbs = TRUE,
  summaryFunction = twoClassSummary  # also needed for AUC/ROC
)

# Tune an SVM
 set.seed(5628)  # for reproducibility
 churn_svm_auc <- caret::train(
   Match_Status ~ .,
   data = train,
   method = "svmRadial",
   preProcess = c("center", "scale"),
   metric = "ROC",  # area under ROC curve (AUC)
   trControl = ctrl,
   tuneLength = 10
 )

 # Print results
churn_svm_auc$results
```

```{r}
#https://bradleyboehmke.github.io/HOML/svm.html
# caret::confusionMatrix(churn_svm_auc)
```

Feature interpretation
```{r}
#https://bradleyboehmke.github.io/HOML/svm.html
prob_yes <- function(object, newdata) {
  predict(object, newdata = newdata, type = "prob")[, "Yes"]
}
```

```{r}
#https://bradleyboehmke.github.io/HOML/svm.html
#Variable importance plot
set.seed(2827)  # for reproducibility
vip::vip(churn_svm_auc, method = "permute", nsim = 5, train = train,
    target = "Match_Status", metric = "auc", reference_class = "Yes",
    pred_wrapper = prob_yes)
```



# 4) Model: Random Forest model
  
A random forest model is a forest of decision trees. Think of random forest as an ensemble of multiple trees.  

```{r Random Forest Model}
# train a default random forest model
#https://bradleyboehmke.github.io/HOML/random-forest.html
# number of features
n_features <- length(setdiff(names(train), "Match_Status"))

# By default, ranger sets the mtry parameter to floor (√number of features); however, for regression problems the preferred mtry to start with is floor  number of features divided by 3)

train_rf1 <- ranger::ranger(
  Match_Status ~ ., 
  data = train,
  mtry = floor(n_features / 3),
  respect.unordered.factors = "order",
  seed = 123
)
train_rf1

rf_model <- randomForest::randomForest(Match_Status~., data = train)
# Show model error
plot(rf_model, ylim=c(0,0.36), main = "Random Forest, an mtry \n value slightly lower (25) \n improves performance.", legend = TRUE, subtitles = TRUE)
legend('topright', colnames(rf_model$err.rate), col=1:3, fill=1:3)

tm_print_save("rf_model_plot.tiff")
```

Tuning
```{r}
# create hyperparameter grid
#https://bradleyboehmke.github.io/HOML/random-forest.html
hyper_grid <- expand.grid(
  mtry = floor(n_features * c(.05, .15, .25, .333, .4)),
  min.node.size = c(1, 3, 5, 10), 
  replace = c(TRUE, FALSE),                               
  sample.fraction = c(.5, .63, .8),                       
  rmse = NA                                               
)

# execute full cartesian grid search
for(i in seq_len(nrow(hyper_grid))) {
  # fit model for ith hyperparameter combination
  fit <- ranger(
    formula         = Match_Status ~ ., 
    data            = train, 
    num.trees       = n_features * 10,
    mtry            = hyper_grid$mtry[i],
    min.node.size   = hyper_grid$min.node.size[i],
    replace         = hyper_grid$replace[i],
    sample.fraction = hyper_grid$sample.fraction[i],
    verbose         = FALSE,
    seed            = 123,
    respect.unordered.factors = 'order',
  )
  # export OOB error 
  hyper_grid$rmse[i] <- sqrt(fit$prediction.error)
}
```

```{r}
# re-run model with impurity-based variable importance
#https://bradleyboehmke.github.io/HOML/random-forest.html
rf_impurity <- ranger(
  formula = formula, 
  data = train, 
  num.trees = 2000,          # 10 times number of features
  mtry = 5,
  min.node.size = 1,
  sample.fraction = .63,
  replace = FALSE,
  importance = "impurity",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)

# re-run model with permutation-based variable importance
rf_permutation <- ranger(
  formula = formula, 
  data = train, 
  num.trees = 2000,
  mtry = 5,
  min.node.size = 1,
  sample.fraction = .63,
  replace = FALSE,
  importance = "permutation",
  respect.unordered.factors = "order",
  verbose = FALSE,
  seed  = 123
)
```

Feature importance for Random Forest
```{r}
#https://bradleyboehmke.github.io/HOML/random-forest.html
p1 <- vip::vip(rf_impurity, num_features = 25, bar = FALSE)
p2 <- vip::vip(rf_permutation, num_features = 25, bar = FALSE)

gridExtra::grid.arrange(p1, p2, nrow = 1)
tm_print_save("Feature_importance_for_Random_Forest.tiff")
```


```{r Random Forest}
#https://github.com/mufflyt/Diabetes-Patient-Readdmission-Prediction/blob/master/Patient%20Readmission%20Risk%20Prediction.r

train$Match_Status <- as.factor(train$Match_Status)
levels(train$Match_Status)

#Using caret
caret_matrix <- caret::train(x=train[,c('white_non_white', 'Age', 'Gender', 'Couples_Match', 'US_or_Canadian_Applicant', 'Medical_Education_or_Training_Interrupted', 'Alpha_Omega_Alpha', 'Military_Service_Obligation', 'USMLE_Step_1_Score', 'Count_of_Poster_Presentation', 'Count_of_Oral_Presentation', 'Count_of_Articles_Abstracts', 'Count_of_Peer_Reviewed_Book_Chapter', 'Count_of_Other_than_Published', 'Count_of_Online_Publications', 'Visa_Sponsorship_Needed', 'Medical_Degree')],
y=train$Match_Status, 
method='rf', 
trControl=trainControl(method="repeatedcv", 
                       number=5, 
                       summaryFunction = twoClassSummary, 
                       classProbs = TRUE, 
                       allowParallel = TRUE))

caret_matrix
caret_matrix$results

#extracting variable importance and make graph with ggplot (looks nicer that the standard varImpPlot)
#Special function!  
tm_variable_importance(caret_matrix)
tm_print_save("tm_variable_importance_caret_matrix.tiff")
```
The random Forest classification suffers in terms of interpretability. We are unable to visualize the 500 trees and identify important features of the model. However, we can assess the Feature Importance using the Gini index measure. Let’s plot mean Gini index across all trees and identify important features.

```{r}
#using the model to make Survival predictions on the test set
solution_rf <- stats::predict(caret_matrix, newdata = test, type = "raw")
head(solution_rf)
mean(solution_rf == test$Match_Status)  #Accuracy

#for confusion Matrix
test$Match_Status
#y_test <- test[test, "Matched"]  #Not sure what is wrong here.  #https://github.com/mufflyt/Diabetes-Patient-Readdmission-Prediction/blob/master/Patient%20Readmission%20Risk%20Prediction.r
#confusionMatrix(solution_rf, y_test)
```
Random forest has accuracy of `r round(mean(solution_rf == test$Match_Status), digits=2)`.  
\pagebreak

# 5) Model: Gradient Boosting Machine (GBM) model

```{r run a basic GBM model}
#https://bradleyboehmke.github.io/HOML/gbm.html
# run a basic GBM model
set.seed(123)  # for reproducibility
library(gbm)

ctrl <- trainControl(method = "repeatedcv",   # 10fold cross validation
                     # number = 5,							# do 5 repititions of cv
                     summaryFunction=twoClassSummary,	# Use AUC to pick the best model
                     classProbs=TRUE,
                     allowParallel = TRUE)

grid <- expand.grid(interaction.depth=c(2,3), # Depth of variable interactions
                    n.trees=c(100,1000),	        # Num trees to fit
                    shrinkage=c(0.01,0.2),		# Try 2 values for learning rate 
                    n.minobsinnode = 20)

train$Match_Status <- (as.numeric(train$Match_Status) - 1)
train$Match_Status <- as.factor(train$Match_Status)
class(train$Match_Status)
levels(train$Match_Status)

#   Not working?????????
# gbmFit<- caret::train(Match_Status~., data = train,
#                method = "gbm", #n.trees = 1000,
#   #shrinkage = c(0.01,0.2),
#   #interaction.depth = c(2,3),
#   #n.minobsinnode = 20,
#   #cv.folds = 10,
#   #distribution = "gaussian",
#                tuneGrid = grid,
#                metric = "ROC",
#                verbose = FALSE,
#                trControl = ctrl)
# 
# gbmPred <- predict(gbmFit,type = "raw")
# head(gbmPred)
# 
# confusionMatrix(train$Match_Status,gbmPred)
# 
# library(ROCR)
# gbmPred <- predict(gbmFit, test)
# pred <- prediction(as.numeric(gbmPred), as.numeric(y_test))
# perf <- performance(pred, "tpr", 'fpr')
# plot(perf, main = "ROC curve", colorize = T)
# abline(0,1, col='gray60')
# 
# auc_ROCR <- performance(pred, measure = "auc")
# auc_ROCR <- auc_ROCR@y.values[[1]]
# auc_ROCR
# 
# # find index for number trees with minimum CV error
# best <- which.min(gbmFit$cv.error)
```

```{r plot error curve}
# plot error curve
#gbm.perf(gbmFit, method = "cv")  #Not working
```

Tuning
```{r create grid search}
#Not working

# # create grid search
# hyper_grid <- expand.grid(
#   learning_rate = c(0.3),
#   #  learning_rate = c(0.3, 0.1, 0.05, 0.01, 0.005),  #original
#   RMSE = NA,
#   trees = NA,
#   time = NA
# )
# 
# # execute grid search
# for(i in seq_len(nrow(hyper_grid))) {
# 
#   # fit gbm
#   set.seed(123)  # for reproducibility
#   train_time <- system.time({
#     m <- gbm::gbm(
#       formula = Match_Status ~ .,
#       data = train,
#       distribution = "gaussian",
#       n.trees = 5000, 
#       shrinkage = hyper_grid$learning_rate[i], 
#       interaction.depth = 3, 
#       n.minobsinnode = 10,
#       cv.folds = 10 
#    )
#   })
#   
#   # add SSE, trees, and training time to results
#   hyper_grid$trees[i] <- which.min(m$cv.error)
#   hyper_grid$Time[i]  <- train_time[["elapsed"]]
# 
# }
```

```{r search grid}
#Not working.  

# # search grid
# hyper_grid <- expand.grid(
#   n.trees = 1000,
#   shrinkage = 0.01,
#   interaction.depth = c(3),
#   n.minobsinnode = c(5)
#   #   n.trees = 6000, #original
#   # shrinkage = 0.01, #original
#   # interaction.depth = c(3, 5, 7), #original
#   # n.minobsinnode = c(5, 10, 15) #original
#   
# )
# 
# # create model fit function
# model_fit <- function(n.trees, shrinkage, interaction.depth, n.minobsinnode) {
#   set.seed(123)
#   m <- gbm(
#     formula = Match_Status ~ .,
#     data = train,
#     distribution = "gaussian",
#     n.trees = n.trees,
#     shrinkage = shrinkage,
#     interaction.depth = interaction.depth,
#     n.minobsinnode = n.minobsinnode,
#     cv.folds = 10
#   )
#   # compute RMSE
#   sqrt(min(m$cv.error))
# }
# 
# # perform search grid with functional programming
# hyper_grid$rmse <- purrr::pmap_dbl(
#   hyper_grid,
#   ~ model_fit(
#     n.trees = ..1,
#     shrinkage = ..2,
#     interaction.depth = ..3,
#     n.minobsinnode = ..4
#     )
# )
# 
# # results
# arrange(hyper_grid, rmse)
```


```{r, include = T}
set.seed(2017)
caret_boost <- caret::train(Match_Status~ ., 
                     data=train, method='gbm', preProcess= c('center', 'scale'), trControl=trainControl(method="cv", number=7), verbose=FALSE)
print(caret_boost)
```

```{r}
#using the model to make Survival predictions on the test set
solution_boost <- predict(caret_boost, newdata = test, type = "raw")
#caret::confusionMatrix(solution_boost, test$Match_Status) #not working
```


\pagebreak
# 6) Model: Naïve Bayes with WOE Binning model

?????????????Finally, a Naïve Bayes model was fit. Similar to the previous models, the top 5 WOE binned variables were also included in this model. Cross-validation demonstrated that the tuning parameter 'laplace' was held constant at a value of 0 and tuning parameter 'adjust' was held constant at a value of 1.

```{r NB model, cache=FALSE}
# NAIVE BAYES 
# e1071 implementation # https://github.com/tobiolatunji/Readmission_Prediction/blob/master/diabetes_readmission.R

nbayesmodel <- naiveBayes(Match_Status~., 
                          data = train)
print(nbayesmodel)

pred <- predict(nbayesmodel, test, type = "class")
test$pred_Match_Status <- pred
prop.table(table(test$Match_Status, test$pred_Match_Status),1)
caret::confusionMatrix(test$Match_Status, test$pred_Match_Status)
```

\pagebreak

**(7) xgboost model**

# 7) Model: XGboost model
XGBoost (which stands for eXtreme Gradient Boosting) is an especialy efficent implimentation of gradient boosting. In practice, XGBoost is a very powerful tool for classification and regression. 

Data prep for xgboost
How can we convert these categories to a matrix? One way to do this is using one-hot encoding. One-hot encoding takes each category and makes it its own column. Then, for each observation, it puts a "0" in that column if that observation doesn't belong to that column and "1" if it does.

```{r}
train$Match_Status <- as.factor(train$Match_Status)
model_04_xgboost <- parsnip::boost_tree(mode = "classification", 
        mtry = 30, 
        trees = 500, 
        min_n = 2, 
        tree_depth = 6,
        learn_rate = 0.35, 
        loss_reduction = 0.0001) %>%
    set_engine("xgboost") %>%
    fit(Match_Status~., data = train)

model_04_xgboost

calc_metrics
### Not working !!!
#model_04_xgboost %>% calc_metrics(test, truth = Match_Status) #Not working!!!!

#model_04_xgboost %>% plot_predictions(new_data = test_tbl)

# Explanation
model_04_xgboost$fit %>%
    xgb.importance(model = .) %>%
    xgb.plot.importance(main = "XGBoost Feature Importance")
tm_print_save("XGBoost_Feature_Importance.tiff")
```

#Not working.  

<!-- ```{r} -->
<!-- library(recipes) -->
<!-- xgb_prep <- recipe(x = formula, data = train) %>% -->
<!--   step_integer(all_nominal()) %>% -->
<!--   prep(training = train, retain = TRUE) %>% -->
<!--   juice() -->

<!-- X <- as.matrix(xgb_prep[setdiff(names(xgb_prep), "Match_Status")]) -->
<!-- Y <- xgb_prep$Match_Status -->
<!-- ``` -->

<!-- Tuning -->
<!-- ```{r} -->
<!-- set.seed(123) -->
<!-- train_xgb <- xgb.cv( -->
<!--   data = X, -->
<!--   label = Y, -->
<!--   nrounds = 6000, -->
<!--   objective = "reg:linear", -->
<!--   early_stopping_rounds = 50,  -->
<!--   nfold = 10, -->
<!--   params = list( -->
<!--     eta = 0.1, -->
<!--     max_depth = 3, -->
<!--     min_child_weight = 3, -->
<!--     subsample = 0.8, -->
<!--     colsample_bytree = 1.0), -->
<!--   verbose = 0 -->
<!-- )   -->

<!-- # minimum test CV RMSE -->
<!-- min(train_xgb$evaluation_log$test_rmse_mean) -->
<!-- ``` -->

<!-- Grid Search -->
<!-- ```{r} -->
<!-- #This takes FOREVER!  Oh man! -->

<!-- # hyperparameter grid -->
<!-- hyper_grid <- expand.grid( -->
<!--   eta = 0.01, -->
<!--   max_depth = 3,  -->
<!--   min_child_weight = 3, -->
<!--   subsample = 0.5,  -->
<!--   colsample_bytree = 0.5, -->
<!--   gamma = c(0, 1, 10, 100, 1000), -->
<!--   lambda = c(0, 1e-2, 0.1, 1, 100, 1000, 10000), -->
<!--   alpha = c(0, 1e-2, 0.1, 1, 100, 1000, 10000), -->
<!--   rmse = 0,          # a place to dump RMSE results -->
<!--   trees = 0          # a place to dump required number of trees -->
<!-- ) -->

<!-- #This takes FOREVER!  Oh man! -->
<!-- # grid search -->
<!-- for(i in seq_len(nrow(hyper_grid))) { -->
<!--   set.seed(123) -->
<!--   m <- xgb.cv( -->
<!--     data = X, -->
<!--     label = Y, -->
<!--     nrounds = 4, #4000 -->
<!--     objective = "reg:linear", -->
<!--     early_stopping_rounds = 50,  -->
<!--     nfold = 10, -->
<!--     verbose = 0, -->
<!--     params = list(  -->
<!--       eta = hyper_grid$eta[i],  -->
<!--       max_depth = hyper_grid$max_depth[i], -->
<!--       min_child_weight = hyper_grid$min_child_weight[i], -->
<!--       subsample = hyper_grid$subsample[i], -->
<!--       colsample_bytree = hyper_grid$colsample_bytree[i], -->
<!--       gamma = hyper_grid$gamma[i],  -->
<!--       lambda = hyper_grid$lambda[i],  -->
<!--       alpha = hyper_grid$alpha[i] -->
<!--     )  -->
<!--   ) -->
<!--   hyper_grid$rmse[i] <- min(m$evaluation_log$test_rmse_mean) -->
<!--   hyper_grid$trees[i] <- m$best_iteration -->
<!-- } -->

<!-- # results -->
<!-- hyper_grid %>% -->
<!--   filter(rmse > 0) %>% -->
<!--   arrange(rmse) %>% -->
<!--   glimpse() -->
<!-- ``` -->

<!-- ```{r Optimal parameters} -->
<!-- #Optimal parameters -->
<!-- # optimal parameter list -->
<!-- params <- list( -->
<!--   eta = 0.01, -->
<!--   max_depth = 3, -->
<!--   min_child_weight = 3, -->
<!--   subsample = 0.5, -->
<!--   colsample_bytree = 0.5 -->
<!-- ) -->

<!-- # train final model -->

<!-- #Error in UseMethod("xgboost") :  -->
<!--   #no applicable method for 'xgboost' applied to an object of class "list" -->


<!-- # xgb.fit.final <- xgboost( -->
<!-- #   params = params, -->
<!-- #   data = X, -->
<!-- #   label = Y, -->
<!-- #   nrounds = 3944, -->
<!-- #   objective = "reg:linear", -->
<!-- #   verbose = 0 -->
<!-- # ) -->
<!-- ``` -->

```{r}
# variable importance plot,   Error in vip::vip(xgb.fit.final) : object 'xgb.fit.final' not found
# vip::vip(xgb.fit.final) 
# xgb.importance(xgb.fit.final)
# xgboost::xgb.ggplot.importance(xgb.fit.final)
```

# 7a)  Model: Neural Network
```{r}
##https://www.datacamp.com/community/tutorials/neural-network-models-r
#A neural network is not going to work because most of our variables are nominal instead of numeric.
```

# 8) Model: Ensembles model

```{r Helper packages}
# Helper packages
library(rsample)   # for creating our train-test splits
library(recipes)   # for minor feature engineering tasks

# Modeling packages
library(h2o)       # for fitting stacked models
gc()
```

```{r Load and split the data}
# Load and split the data
set.seed(123)  # for reproducibility
split <- initial_split(all_data, strata = "Match_Status")
train <- training(split)
test <- testing(split)

# Ma ke sure we have consistent categorical levels
blueprint <- recipe(Match_Status~., data = train) %>%
  step_other(all_nominal(), threshold = 0.005)

# Create training & test sets for h2o
library(h2o)
h2o::h2o.init()
train_h2o <- prep(blueprint, training = train, retain = TRUE) %>%
  juice() %>%
  as.h2o()
test_h2o <- prep(blueprint, training = train) %>%
  bake(new_data = test) %>%
  as.h2o()

# Get response and feature names
Y <- "Match_Status"
X <- setdiff(names(train), Y)  #Why are we using ames_train ???????
```

```{r Train & cross-validate mutliple models}
# Train & cross-validate a GLM model
 best_glm <- h2o.glm(
   x = X, y = Y, training_frame = train_h2o, alpha = 0.1,
   remove_collinear_columns = TRUE, nfolds = 10, fold_assignment = "Modulo",  #nfolds was 10 originally
   keep_cross_validation_predictions = TRUE, seed = 123
 )

# Train & cross-validate a RF model
best_rf <- h2o.randomForest(
  x = X, y = Y, training_frame = train_h2o, ntrees = 1000, mtries = 10,  #original
  max_depth = 30, min_rows = 1, sample_rate = 0.8, nfolds = 3,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "AUC",
  stopping_tolerance = 0
)
saveRDS(best_rf, (here::here("Saved_Models", "best_rf.rds")))


# Train & cross-validate a GBM model
best_gbm <- h2o.gbm(
  x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.01, #original
  max_depth = 7, min_rows = 5, sample_rate = 0.8, nfolds = 10, #original
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123, stopping_rounds = 50, stopping_metric = "AUC",
  stopping_tolerance = 0
)
saveRDS(best_gbm, (here::here("Saved_Models", "best_gbm.rds")))

# Train & cross-validate an XGBoost model
best_xgb <- h2o.xgboost(
  x = X, y = Y, training_frame = train_h2o, ntrees = 100, learn_rate = 0.05,
  #x = X, y = Y, training_frame = train_h2o, ntrees = 5000, learn_rate = 0.05, #original
  max_depth = 3, min_rows = 3, sample_rate = 0.8, categorical_encoding = "Enum",
  nfolds = 3, fold_assignment = "Modulo",
  #nfolds = 10, fold_assignment = "Modulo", #original
  keep_cross_validation_predictions = TRUE, seed = 123, stopping_rounds = 50,
  stopping_metric = "AUC", stopping_tolerance = 0
)
saveRDS(best_xgb, (here::here("Saved_Models", "best_xgb.rds")))
```


```{r Train a stacked tree ensemble}
# Train a stacked tree ensemble
ensemble_tree <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "my_tree_ensemble_2",
  base_models = list(best_rf, best_gbm, best_xgb),
  metalearner_algorithm = "drf"
)
saveRDS(ensemble_tree, (here::here("Saved_Models", "ensemble_tree.rds")))
```


```{r et results from base learners}
# Get results from base learners
get_auc <- function(model) {
  results <- h2o.performance(model, newdata = test_h2o)
  results@metrics$AUC
}
list(best_rf, best_gbm, best_xgb) %>%
  purrr::map_dbl(get_auc)

# Stacked results
h2o.performance(ensemble_tree, newdata = test_h2o)@metrics$AUC
```


```{r Creation of prections for each model}
data.frame(
  #GLM_pred = as.vector(h2o.getFrame(best_glm@model$cross_validation_holdout_predictions_frame_id$name)),
  RF_pred = as.vector(h2o.getFrame(best_rf@model$cross_validation_holdout_predictions_frame_id$name)),
  GBM_pred = as.vector(h2o.getFrame(best_gbm@model$cross_validation_holdout_predictions_frame_id$name)),
  XGB_pred = as.vector(h2o.getFrame(best_xgb@model$cross_validation_holdout_predictions_frame_id$name)))
#) %>% cor()
```


```{r Define GBM hyperparameter grid}
# Define GBM hyperparameter grid
hyper_grid <- list(
  max_depth = c(1, 3, 5),
  min_rows = c(1, 5, 10),
  learn_rate = c(0.01, 0.05, 0.1),
  learn_rate_annealing = c(0.99, 1),
  sample_rate = c(0.5, 0.75, 1),
  col_sample_rate = c(0.8, 0.9, 1)
)

# Define random grid search criteria
search_criteria <- list(
  strategy = "RandomDiscrete",
  max_models = 25
)

# Build random grid search 

#Not working, Illegal argument: training_frame of function: grid: Cannot append new models to a grid with different training input

random_grid <- h2o.grid(
  algorithm = "gbm", grid_id = "gbm_grid", x = X, y = Y,
  training_frame = train_h2o, hyper_params = hyper_grid,
  search_criteria = search_criteria, ntrees = 5000, stopping_metric = "AUC",
  stopping_rounds = 10, stopping_tolerance = 0, nfolds = 10,
  fold_assignment = "Modulo", keep_cross_validation_predictions = TRUE,
  seed = 123
)
```

```{r Sort results by RMSE}
# Sort results by RMSE
h2o.getGrid(
  grid_id = "gbm_grid", 
  sort_by = "AUC"
)
```

```{r Grab the model_id for the top model, chosen by validation error}
# Grab the model_id for the top model, chosen by validation error
#best_model_id <- random_grid_perf@model_ids[[1]]
best_model <- h2o.getModel(best_model_id)
h2o.performance(best_model, newdata = test_h2o)
```

```{r Train a stacked ensemble using the GBM grid}
# Train a stacked ensemble using the GBM grid
ensemble <- h2o.stackedEnsemble(
  x = X, y = Y, training_frame = train_h2o, model_id = "ensemble_gbm_grid",
  base_models = random_grid@model_ids, metalearner_algorithm = "gbm"
)

# Eval ensemble performance on a test set
h2o.performance(ensemble, newdata = test_h2o)
```

```{r Use AutoML to find a list of candidate models (i.e., leaderboard)}
# Use AutoML to find a list of candidate models (i.e., leaderboard)
auto_ml <- h2o.automl(
  x = X, y = Y, training_frame = train_h2o, nfolds = 5, 
  max_runtime_secs = 60 * 120, max_models = 50,
  keep_cross_validation_predictions = TRUE, sort_metric = "AUC", seed = 123,
  stopping_rounds = 50, stopping_metric = "AUC", stopping_tolerance = 0
)
class(auto_ml)

saveRDS(auto_ml, (here::here("Saved_Models", "auto_ml.rds")))

# Assess the leader board; the following truncates the results to show the top 
# and bottom 15 models. You can get the top model with auto_ml@leader
auto_ml@leaderboard %>% 
  as.data.frame() %>%
  dplyr::select(model_id, auc) %>%
  dplyr::slice(1:10)
```
\pagebreak

# Model Comparison

Given the fact that all three models have decent public scores, especially the correlation between SVM and the RF model is low. The most likely explanation is that SVM really is a different algorithm (both other models are tree-based).

```{r}
# This has never worked.  
# <!-- ```{r, include=TRUE} -->
# <!-- #https://www.kaggle.com/kernels/scriptcontent/7086189/download -->
# <!-- #adding model predictions to test dataframe -->
# <!-- test$RF <- as.numeric(solution_rf)-1 -->
# <!-- test$SVM <- as.numeric(solution_svm)-1 #NOT WORKING -->
# <!-- test$Boost <- as.numeric(solution_boost)-1 -->
# <!-- test$tree <- as.numeric(solution_tree)-1 -->
# 
# <!-- #compose correlations plot -->
# <!-- corrplot.mixed(cor(test[, c('RF', 'SVM', 'Boost')]), order="hclust", tl.col="black") -->
# <!-- ``` -->
# 
# 
# <!-- ```{r model comparison, align = 'center', include=TRUE} -->
# <!-- #Table 14 summarizes the overall in-sample and out-of-sample accuracy of each model. The best performing models (highest accuracy) was the random forest model with a test set accuracy of `r rf.accuracy.test[1]`. The Logistic regression model using backwards elimination was second with a test set accuracy of `r lr.accuracy.test[1]`. The Naive Bayes model did not perform as well as the other models. In summary, if accuracy is the most important aspect of the model and interpretion is not a priority then the best model was the random forest model. If interpretability of the model is paramount, then the logistic regression model is recommended. -->

# <!-- # Training set performance summary -->
# <!-- # x <- caret::postResample(pred = step_prob, obs = as.factor(train$Match_Status)) -->
# <!-- # a <- caret::postResample(pred = solution_tree_probability, obs = as.factor(train$Match_Status)) -->
# <!-- # c <- caret::postResample(pred = train.svm, obs = as.factor(train$Match_Status)) -->
# <!-- # e <- caret::postResample(pred = train.rf, obs = as.factor(train$Match_Status)) -->
# <!-- # g <- caret::postResample(pred = train.nbwoe, obs = as.factor(train$Match_Status)) -->
# 
# 
# <!-- # Test set performance summary -->
# <!-- # xt <- postResample(pred = back_step_prob, obs = as.factor(test$Match_Status)) -->
# <!-- # at <- postResample(pred = test.cart, obs = as.factor(test.df.binned$Match_Status)) -->
# <!-- #  -->
# <!-- # ct <- postResample(pred = test.svm, obs = as.factor(test.df.binned$Match_Status)) -->
# <!-- # et  <- postResample(pred = test.rf, obs = as.factor(test.df.binned$Match_Status)) -->
# <!-- # gt <- postResample(pred = test.nbwoe, obs = as.factor(test.df.binned$Match_Status)) -->
# <!-- # matrix <- matrix(data = c(x[1], a[1], c[1], e[1], g[1], xt[1], at[1], ct[1], et[1], gt[1]), nrow = 5, ncol = 2, byrow = FALSE) -->
# <!-- # colnames(matrix) <- c("Training Set Accuracy", "Test Set Accuracy") -->
# <!-- # rownames(matrix) <- c("LR Backwards Elimination", "CART", "Support Vector Machine", "Random Forest", "Naive Bayes") -->
# <!-- # df <- round(matrix, 3) -->
# <!-- # kable(df, "latex", booktabs = T, caption = "In-sample and out-of-sample accuracy of all models") %>% kable_styling(latex_options = "striped") -->
# <!-- ``` -->
```


\pagebreak
```{r NOMOGRAM,  fig.width=7, fig.asp=1}
###NOMOGRAM
d <- rms::datadist(test)
options(datadist = "d")
args(tm_nomogram_prep)
tm_nomogram_prep(train.lrm.with.lasso.variables)
tm_print_save("train_lrm_with_lasso_variables.tiff")
```
Annotation:  Manuscript Figure 1:  The first row called points assigned to each variable's measurement from rows 2-12, which are variables included in predictive model.  Assigned points for all variables are then summed and total can be located on line 13 (total points).  Once total points are located, draw a vertical line down to the bottom line to obtain the predicted probability of matching.  For non-linear variables (count of oral presentations, etc.) values should be erad from left to right.

# Calibration of the model based on the test data.
The ticks across the x-axis represent the frequency distribution (may be called a rug plot) of the predicted probabilities. This is a way to see where there is sparsity in your predictions and where there is a relative abundance of predictions in a given area of predicted probabilities.

The "Apparent" line is essentially the in-sample calibration.

The "Ideal" line represents perfect prediction as the predicted probabilities equal the observed probabilities.

The "Bias Corrected" line is derived via a resampling procedure to help add "uncertainty" to the calibration plot to get an idea of how this might perform "out-of-sample" and adjusts for "optimistic" (better than actual) calibration that is really an artifact of fitting a model to the data at hand. This is the line we want to look at to get an idea about generalization (until we have new data to try the model on).

When either of the two lines is above the "Ideal" line, this tells us the model underpredicts in that range of predicted probabilities. When either line is below the "Ideal" line, the model overpredicts in that range of predicted probabilities.

Applying to your specific plot, it appears most of the predicted probabilities are in the higher end (per rug plot). The model overall appears to be reasonably well calibrated based on the Bias-Corrected line closely following the Ideal line; there is some underprediction at lower predicted probabilities because the Bias-Corrected line is above the Ideal line around < 0.3 predicted probability.

The mean absolute error is the "average" absolute difference (disregard a positive or negative error) between predicted probability and actual probability. Ideally, we want this to be small (0 would be perfect indicating no error). This seems small in this plot, but may be situation dependent on how small is small.

Bootstrapping is mostly used when estimating a parameter.
Cross-Validation is the choice when choosing among different predictive models.

```{r calibration,  fig.width=7, fig.asp=1}
calib <- rms::calibrate(train.lrm.with.lasso.variables, 
                        method = "boot", boot=1000, 
                        data = test, 
                        estimates = TRUE)  #Plot test data set

calibration_plot <- plot(calib, legend = TRUE, subtitles = TRUE, xlab = "Predicted probability according to model", ylab = "Observation Proportion of Matching", main = "Calibration Plot of \n train.lrm.with.lasso.variables model")
calibration_plot

plot(calibration_plot)
tm_print_save("train_lrm_with_lasso_variables_calibration_plot.tiff")
```

  \pagebreak
# Appendix, Exploratory Data Analysis
The funModeling package will first give distributions for numerical data and finally creates cross-plots.  This also saves the output of the distributions to the results folder.

# Appendix, Supplemental Table:  Descriptive analysis of all variables considered in the training set along with their association to matching.

```{r, echo=TRUE, warning=FALSE, message=FALSE, include=TRUE, results="asis"}
table1_all_data <- arsenal::tableby(Match_Status ~
                                       white_non_white +
                                       Age +
                                       Gender +
                                       Couples_Match +
                                       #Expected_Visa_Status_Dichotomized +
                                       US_or_Canadian_Applicant +
                                       #Medical_School_Type +
                                       Medical_Education_or_Training_Interrupted +
                                       #Misdemeanor_Conviction +
                                       Alpha_Omega_Alpha +
                                       #Gold_Humanism_Honor_Society +
                                       Military_Service_Obligation +
                                       USMLE_Step_1_Score +
                                       Military_Service_Obligation +
                                       Count_of_Poster_Presentation +
                                       Count_of_Oral_Presentation +
                                       # Count_of_Peer_Reviewed_Articles_Abstracts +
                                       Count_of_Peer_Reviewed_Book_Chapter +
                                       # Count_of_Peer_Reviewed_Other_than_Published +
                                       Count_of_Online_Publications +
                                       Visa_Sponsorship_Needed +
                                       Medical_Degree,
                                    data=train, 
                                    control = arsenal::tableby.control(test = TRUE, total = TRUE, digits = 1L, digits.p = 2L, digits.count = 0L, numeric.simplify = F, numeric.stats = c("median", "q1q3"), cat.stats = c("Nmiss","countpct"), stats.labels = list(Nmiss = "N Missing", Nmiss2 ="N Missing", meansd = "Mean (SD)", medianrange = "Median (Range)", median ="Median", medianq1q3 = "Median (Q1, Q3)", q1q3 = "Q1, Q3", iqr = "IQR",range = "Range", countpct = "Count (Pct)", Nevents = "Events", medSurv ="Median Survival", medTime = "Median Follow-Up")))

summary(table1_all_data, text=T, title='Supplemental Table: Descriptive analysis of all variables considered in the training set along with their association to matching', pfootnote=TRUE)
```

Medical student #1 is a `r all_data$Age[1]`year old `r all_data$white_non_white[1]` `r all_data$Gender[1]` who is a US Senior medical graduate


Abstract
===========================================================================================
Background:  A model that predicts a medical student's chances of matching into an obstetrics and gynecology residency may facilitate improved counseling and fewer unmatched medical students.

Objective:  We sought to construct and validate a model that predicts a medical student's chance of matching into obstetrics and gynecology residency.

Study Design:  In all, `r nrow(all_data)` medical students applied to a residency in Obstetrics and Gynecology at the University of Colorado from 2015 to 2018 were analyzed.  The data set was splint into a model training cohort of `r nrow(train)` who applied in 2015, 2016, and 2017 and a separate validation cohort of `r nrow(test)` in 2018.  In all, `r ncol(all_data)` candidate predictors for matching were collected.  Multiple logistic models were fit onto the training choort to predict matching.  Variables were removed using least absolute shrinkage and selection operator reduction to find the best parsimonious model.  Model discrimination was measured using the concordance index.  The model was internally valideated using 1,000 bootstrapped samples and temporarly validated by testing the model's performance in the validation cohort.  Calibration curves were plotted to inform educators about the accuracy of predicted probabilities.

Results:  The match rate in the training cohort was `r round((prop.table(table(train$Match_Status))[[2]]*100),1)`% (I need help getting 95% CI).  The model had excellent discrimination and calibration during internal validation (bias-corrected concordance index,`r round((lrm.with.lasso.variables$stats[6]),2)`) and maintained accuracy during temportal validation using the separate validation cohort (concordance index,`r round((train.lrm.with.lasso.variables$stats[6]),2)`).

Introduction
===========================================================================================
To add in.  

Materials and Methods
===========================================================================================
This was an institutional review board exempt retrospective cohort analysis of medical students who applied to Obstetrics and Gynecology (OBGYN) residency from 2015 to 2018.  Guidelines for transparent reporting of a multivariable prediction model for individual outcomes were used in this study.(https://www.equator-network.org/reporting-guidelines/tripod-statement/).  Eligible participants were identified if they applied to OBGYN residency during the study period of 2015 to 2018.  The outcome of the model was defined as matching or not matching into residency for the specific application year.  Individual predictors of successfully* matching were compiled from a literature review, expert opinion, and judgment then collected from the Electronic Residency Application Service materials.

Once the data set was complete it was divided into a model training and test set.  *When an external validation data set is unavailable to test a new model but an existing modeling data set is sufficiently large, as in this case, it is recommended to split by time and develop the model using data from one period and evaluate its performance from data from a future period.  There was a large case imbalance with matching rates split by year therefore we split the data randomly.

In all, 18 candidate risk factors were considered for fitting on the training data set (supplmental table).  Variable selection was done using a penalized logistic regression called least absolute shrinkage and selection operator (LASSO).  The LASSO model is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  We elected to use LASSO to choose which covariates to include over stepwise selection because the latter only improves prediction accuracy in certain cases, such as when only a few covariates have a strong relationship with the outcome.

The logistic model’s discriminative ability was measured by the area under the curve (AUC) for the receiver operating characteristic curve based on the sensitivity and specificity of the model.  An AUC value closer to 1 indicates a better prediction of the outcome and an AUC value of 0.5 indicates that the model predicts no better than chance. The AUC is also a representation of the concordance index and measures the model’s ability to generate a higher predicted probability of a successful match* occurring in a medical student who has a ????.   For example, if we have a pair of medical students, in which one medical student matches and the other does not, the concordance index measures the model’s ability to assign a higher risk of not matching to the medical student who successfully matches. All concordance indices and receiver operating characteristic curves were internally validated using a 1,000 bootstrap resample to correct for bias and overfitting within the model. The bootstrapping method of validation has been shown to be superior to other approaches to estimate internal validity. Calibration curves were also plotted to depict the relationship between the model’s predicted outcomes against the cohort’s observed outcome, where a perfectly calibrated model follows a 45° line.

After the best model was selected and internally validated, the model was compared with the best currently available method of estimating risk, that is, an expert medical educator’s predictions. To perform these comparisons, a subset of 50 participants was randomly selected for comparing the probability of matching between the model and the panel of experts. These ?? participants were used to compare predictions of the models with experts’ predictions and not as a true independent validation subset. The model was rebuilt using the remaining participants in the data set excluding the 50 randomly selected participants. The candidate risk factors of these 50 participants were given to 20 “expert” medical educators with representation from each of the *** for review resulting in 1,000 expert predictions and 50 model predictions for each outcome. All medical educators were considered to be experienced in counseling medical students regarding OBGYN matching. Each of the 20 experts were asked to consider each medical student’s data from all ??? variables among the 50 randomly selected students and provide their best estimated outcome by answering the following question: “Out of 100 medical students with these exact characteristics, estimate the number of medical students who would not matching into OBGYN during the 2019 application year.” Individual medical educators’ predictions were not averaged to yield a single value because incorporating each medical educator’s predictions substantially increased statistical power. The model’s predictions were compared with the experts’ predictions, which included all risk factors, to determine which was most accurate. The difference in accuracy was determined by using a bootstrap method from their respective receiver operating characteristic curves. Change.  

All analyses were performed using R 3.6.1.  However, considering the large amount of data, processing the data was taking too long and that was compromising the efficiency of the study. So, the platform h2o.ai was used, integrated with R, to explore the grid mode and parallel processing models. The grid allowed to combine different parameters to build different models. The parallel processing allowed those models to be built at the same time. This efficiency increase made it possible to build more models, with better tuning parameters.

Results
===========================================================================================
A total of `r nrow(all_data)` applied to obstetrics and gynecology residency at the University of Colorado from 2015 to 2018.  The overall mean rate of matching in the training cohort was `r table(train$Match_Status)[[2]]` of `r nrow(train)` was (`r round((prop.table(table(train$Match_Status))[[2]]*100),1)`%).
The unadjusted comparison of the `r ncol(all_data)` candidate predictors in the training cohort are presented in Supplemental Table 1.  To identify predictors from the candidates we employed least absolute shrinkage and selection operator (LASSO).  Regularisation techniques change how the model is fit by adding a penalty for every additional parameter you have in the model.
`r length(variables)` variables were included within the final model.  Applicants from the United States or Canada, high USMLE Step 1 scores, female gender, White race, no visa sponsorship needed, membership in Alpha Omega Alpha, no interruption of medical training, couples matching, and allopathic medical training increased the chances of matching into OBGYN.  In contrast, more oral presentations, increasing age, a higher number of peer-reviewed online publications, an increased number of authored book chapters, and a higher count of poster presentations all decreased the probability of matching into OBGYN (table 2).  The nomogram illustrates the strength of association of the predictors to the outcome as well as the nonlinear associations between age, count of Oral Presentations, count of peer−reviewed book chapters and the chances of matching (Figure 1).

Discussion
===========================================================================================
Needed


References
===============================================================================

```{r sessionInfo, include=FALSE, echo=TRUE, cache=FALSE}
utils::sessionInfo()
```

```{r rsconnect for uploading shinyapps, include = FALSE}
# put credentials here to upload file to shinyapps.io
# rsconnect::setAccountInfo(name='mufflyt',
# 			  token='D8846CA8B32E6A5EAEA94BFD02EEEA39',
# 			  secret='dIXWOv+ud/z6dTPN2xOF9M4BKJtWKROc2cOsZS4U')
# 
# library(rsconnect)
# rsconnect::deployApp('DynNomapp/')
```

# Evaluate Algorithms to create a model
```{r}
train <- na.omit(train)
sum(is.na(train))

#https://github.com/shriyarp/IndianLiverPatient/blob/master/indianLiverPatientsReport.Rmd
control <- trainControl(method="cv", number=10)
#Quadratic Discriminant Analysis
set.seed(4)
fit.qda <- caret::train(Match_Status~., data=train, method="qda", trControl=control)
#AdaBoost Classification Trees
set.seed(4)
fit.ab <- caret::train(Match_Status~., data=train, method="adaboost", trControl=control)
#Naive Bayes
set.seed(4)
fit.nb <- caret::train(Match_Status~., data=train, method="naive_bayes", trControl=control)
# Linear Discriminant Analysis
set.seed(4)
fit.lda <- caret::train(Match_Status~., data=train, method="lda",  trControl=control)
# CART
set.seed(4)
fit.cart <- caret::train(Match_Status~., data=train, method="rpart",  trControl=control)
# K-nearest neighbours
set.seed(4)
fit.knn <- caret::train(Match_Status~., data=train, method="knn",  trControl=control)
# Support Vector Machines with Radial Basis Function Kernel
set.seed(4)
fit.svm <- caret::train(Match_Status~., data=train, method="svmRadial",  trControl=control)
# Random Forest
set.seed(4)
fit.rf <- caret::train(Match_Status~., data=train, method="rf",  trControl=control)
```

Post Building the models, we now select the best model based on accuracy
```{r}
results <- resamples(list(qda = fit.qda, ab = fit.ab, nb=fit.nb, lda=fit.lda, cart=fit.cart, knn=fit.knn, svm=fit.svm, rf=fit.rf))
summary(results)
```

```{r}
dotplot(results)
tm_print_save(filename = "dotplot_results.tiff")
```

# Make Predictions
We can now proceed to make predictions based on the RandomForest
```{r}
#predict 
y_hat <- predict(fit.rf,test)
```


```{r}
class(test$Match_Status)
test$Match_Status <- as.factor(test$Match_Status)

confusionMatrix(y_hat,test$Match_Status)
```


#https://github.com/caramirezal/floJoVIHDataAnalysis/blob/master/reports/exploratoryDataAnalysis.Rmd
#Imputation script in case we need it.  
<!-- #### Data imputation -->

<!-- Here, two independent imputation strategies were implemented. A random forest approach and a very simple imputation strategy using median values in order we can compare two imputation methods. -->

<!-- ### Random forest imputation. -->

<!-- ```{r message=FALSE, warning=FALSE, warning=FALSE} -->
<!-- ## Imputing data using random forest -->
<!-- vihImputedForest <- missForest(vihProcessed,  -->
<!--                                ntree = 300) -->
<!-- vihImputedForest$OOBerror -->
<!-- ``` -->

<!-- NRMSE corresponds to a metric of imputation accuracy. Better results are expected to be closer to zero value. Hence, taking into account this metric the imputation accuracy on data is **low**. -->

<!-- ### Imputation with median values -->

<!-- ```{r} -->
<!-- vihImputedHmisc <- as.data.frame(sapply(vihProcessed,  -->
<!--                                    impute, mean)) -->
<!-- ``` -->


#https://www.kaggle.com/nawajshaikh/neural-network-nawaj-shaikh
```{r}
# Using multilayered feed forward nueral network
# package nueralnet
install.packages("neuralnet")
library(neuralnet)

# Building model
formula_nn <- Match_Status ~ Age
train1<- as.matrix(train)

liver_model <- neuralnet(formula_nn,data=train1, hidden = 5, linear.output = FALSE)
str(liver_model)
plot(liver_model)
plot(liver_model, rep="best")
```

#https://www.kaggle.com/goldsack/titanic-solution-using-neuralnet
#https://www.kaggle.com/bdetanico/liver-disease-prediction-lr-knn-cart-and-rf

#https://www.kaggle.com/gnodyahs/penalised-logistic-regression-model-titanic
## **TRAIN MODELS**
Train multiple logistic regression models and list them for later comparison.
Create variable *error* to store the cvm (mean cross validation error) correspond to lambda.1se of each model.

```{r}
error<- vector(mode = "list", length = 0)
```

```{r}
y<-train$Match_Status
# ---------model 1---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+Couples_Match+white_non_white, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_1 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[1]<-cv$cvm[imin]

###????
# tm_glmnet_model <- function (variables, model_number, error_number) { 
#   #print("glmnet being created via custom function to test variable combinations.")
# 
#   error <- vector(mode = "list", length = 0)
#   y<-train$Match_Status
#   X<-model.matrix(Match_Status~(paste0(variables)), train )[,-1]
# cv <- cv.glmnet(X, y, alpha=1, nfolds = 10,family="binomial",type.measure = "class")
# 
# model_number <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
# imin=which(cv$lambda==cv$lambda.1se)
# 
#   error[error_number]<-cv$cvm[imin]
# }
# tm_glmnet_model(variables = "Age+Gender+Couples_Match+white_non_white", 
#                 model_number = "model_number_1", 
#                 error_number = 1L)

# ---------model 2---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+white_non_white, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_2 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[2]<-cv$cvm[imin]
# ---------model 3---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+Couples_Match+white_non_white + Military_Service_Obligation, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_3 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[3]<-cv$cvm[imin]

# ---------model 4---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+Couples_Match+white_non_white + Alpha_Omega_Alpha, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_4 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[4]<-cv$cvm[imin]

# ---------model 5---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+Couples_Match+white_non_white + Alpha_Omega_Alpha + Medical_Degree, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_5 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[5]<-cv$cvm[imin]

# ---------model 6---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender++white_non_white + Alpha_Omega_Alpha + Medical_Degree, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_6 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[6]<-cv$cvm[imin]

# ---------model 7---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender++white_non_white + Alpha_Omega_Alpha + Medical_Degree + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_7 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[7]<-cv$cvm[imin]

# ---------model 8---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender++white_non_white + Alpha_Omega_Alpha + Medical_Degree + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_8 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[8]<-cv$cvm[imin]


# ---------model 9---------------------------------------------------------------
X<-model.matrix(Match_Status~ Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_9 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[9]<-cv$cvm[imin]

# ---------model 10---------------------------------------------------------------
X<-model.matrix(Match_Status~US_or_Canadian_Applicant + USMLE_Step_1_Score + Age+Gender+white_non_white + Medical_Degree, train )[,-18]
cv <- cv.glmnet(X, y, alpha=1,nfolds = 10,family="binomial",type.measure = "class")
model_10 <- glmnet(X, y, alpha = 1, family = "binomial", lambda = cv$lambda.1se)
imin=which(cv$lambda==cv$lambda.1se)
error[10]<-cv$cvm[imin]
```

Take a look at *error*.                               
```{r}
error                               
```                  

# Evaluate Algorithms to create a model
```{r}
test <- na.omit(test)

#https://github.com/shriyarp/IndianLiverPatient/blob/master/indianLiverPatientsReport.Rmd
control <- trainControl(method="cv", number=10, )
#Quadratic Discriminant Analysis
set.seed(4)
fit.qda.test <- caret::train(Match_Status~., data=test, method="qda", trControl=control)
#AdaBoost Classification Trees
set.seed(4)
fit.ab.test <- caret::train(Match_Status~., data=test, method="adaboost", trControl=control)
#Naive Bayes
set.seed(4)
fit.nb.test <- caret::train(Match_Status~., data=test, method="naive_bayes", trControl=control)
# Linear Discriminant Analysis
set.seed(4)
fit.lda.test <- caret::train(Match_Status~., data=test, method="lda",  trControl=control)
# CART
set.seed(4)
fit.cart.test <- caret::train(Match_Status~., data=test, method="rpart",  trControl=control)
  # cp          Accuracy   Kappa    
  # 0.02068966  0.8545526  0.4925973
  # 0.02758621  0.8558680  0.4885685
  # 0.16206897  0.8116327  0.1361600  #Winner!  

# K-nearest neighbours
set.seed(4)
fit.knn.test <- caret::train(Match_Status~., data=test, method="knn",  trControl=control)
# Support Vector Machines with Radial Basis Function Kernel
set.seed(4)
fit.svm.test <- caret::train(Match_Status~., data=test, method="svmRadial",  trControl=control)
# Random Forest
set.seed(4)
fit.rf.test <- caret::train(Match_Status~., data=test, method="rf",  trControl=control)
```


# DynNom Model for Shiny Upload
```{r DynNom logistic regression model}
library(dplR)
DynNom.model.lrm  <-
  rms::lrm(Match_Status ~
             rms::rcs(Age, 5) +
             Gender +
             US_or_Canadian_Applicant +
             rms::rcs(USMLE_Step_1_Score, 4) +
             white_non_white +
             Alpha_Omega_Alpha +
             Count_of_Oral_Presentation +
             Count_of_Peer_Reviewed_Book_Chapter +
             Couples_Match +
             Medical_Degree +
             Military_Service_Obligation +
             Visa_Sponsorship_Needed,
                   data = test,
           x = TRUE,
           y= TRUE)

#### Turn this on to start the shiny model.  
#DynNom::DynNom(model = DynNom.model.lrm, data = test,  clevel = 0.95)
```

#All variables for the DynNom online tool.  
```{r}
DynNom.model.lrm.full  <-
  rms::lrm(Match_Status~ Age + white_non_white + Gender + Medical_Degree + US_or_Canadian_Applicant + Medical_Education_or_Training_Interrupted + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published + Count_of_Online_Publications + Visa_Sponsorship_Needed + Couples_Match, data=test, x=TRUE, y=TRUE)

summary(DynNom.model.lrm.full)

DynNom::DynNom(model = DynNom.model.lrm.full, data = test,  clevel = 0.95)
```


```{r generalized logistic regression, include=FALSE, echo=TRUE, cache=FALSE}
DynNom.model.glm <- stats::glm(formula = formula,
                        family = "binomial",  #Removed the relax cubic splines for age and USMLE step 1
                        data = test, x = TRUE, y= TRUE)


#### Turn this on to start the shiny model.  
# DynNom::DNbuilder(model = DynNom.model.glm, data = test, clevel = 0.95, DNtitle = "DRAFT: A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency")
```

# Caret Approach
```{r}
#https://rpubs.com/esuess/LogisticRegression

library(caret)
all_data
# Using caret package
set.seed(123)  # for reproducibility
Train <- caret::createDataPartition(all_data$Match_Status, p = 0.8, 
                               list = FALSE)
#Train <- createDataPartition(GermanCredit$Class, p=0.6, list=FALSE)
training <- all_data[ Train, ]
testing <- all_data[ -Train, ]

mod_fit <- caret::train(Match_Status ~ white_non_white + Age + Gender + Couples_Match + US_or_Canadian_Applicant + Medical_Education_or_Training_Interrupted + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published + Count_of_Online_Publications + Visa_Sponsorship_Needed  + Medical_Degree,  
                        data=training, method="glm", family="binomial")

exp(coef(mod_fit$finalModel))

stats::predict(mod_fit, newdata=testing)
stats::predict(mod_fit, newdata=testing, type="prob")

mod_fit_one <- glm(Match_Status ~ white_non_white + Age + Gender + Couples_Match + US_or_Canadian_Applicant + Medical_Education_or_Training_Interrupted + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published + Count_of_Online_Publications + Visa_Sponsorship_Needed  + Medical_Degree, 
                   data=training, family="binomial")

mod_fit_two <- glm(Match_Status ~ white_non_white + Age + Gender + Couples_Match + US_or_Canadian_Applicant + Medical_Education_or_Training_Interrupted + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Medical_Degree, 
                   data=training, family="binomial")

anova(mod_fit_one, mod_fit_two, test ="Chisq")

library(lmtest)
lrtest(mod_fit_one, mod_fit_two)

library(pscl)
pR2(mod_fit_one)  # look for 'McFadden'

# install.packages("remotes")
# remotes::install_github("stamats/MKmisc")
# library(MKmisc)
# HLgof.test(fit = fitted(mod_fit_one), obs = training$Match_Status)

library(ResourceSelection)
hoslem.test(training$Match_Status, fitted(mod_fit_one), g=10)

library(survey)
regTermTest(mod_fit_one, "Medical_Degree")
regTermTest(mod_fit_one, "USMLE_Step_1_Score")

varImp(mod_fit)

# testing$Match_Status <- as.factor(testing$Match_Status)
# pred = stats::predict(mod_fit, newdata=testing)
# dim(pred)
# class(pred)
# accuracy <- table(pred, testing[,"Match_Status"])
# dim(accuracy)
# class(accuracy)
# sum(base::diag(accuracy))/sum(accuracy)

pred = predict(mod_fit, newdata=testing)
caret::confusionMatrix(data=pred, testing$Match_Status)

library(pROC)
# Compute AUC for predicting Class with the variable CreditHistory.Critical
f1 = roc(Match_Status ~ USMLE_Step_1_Score, data=training) 
plot(f1, col="red")
## 
## Call:
## roc.formula(formula = Class ~ CreditHistory.Critical, data = training)
## 
## Data: CreditHistory.Critical in 180 controls (Class Bad) < 420 cases (Class Good).
## Area under the curve: 0.5944
library(ROCR)
# Compute AUC for predicting Class with the model
prob <- stats::predict(mod_fit_one, newdata=testing, type="response")
pred <- ROCR::prediction(prob, testing$Match_Status)
perf <- performance(pred, measure = "tpr", x.measure = "fpr")
plot(perf)
auc <- performance(pred, measure = "auc")
auc <- auc@y.values[[1]]
auc

ctrl <- trainControl(method = "repeatedcv", number = 10, savePredictions = TRUE)

mod_fit <- caret::train(Match_Status ~  white_non_white + Age + Gender + Couples_Match + US_or_Canadian_Applicant + Medical_Education_or_Training_Interrupted + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Articles_Abstracts + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Other_than_Published + Count_of_Online_Publications + Visa_Sponsorship_Needed + Medical_Degree,
                 data=all_data, method="glm", family="binomial",
                 trControl = ctrl, tuneLength = 5)

pred = stats::predict(mod_fit, newdata=testing)
caret::confusionMatrix(data=pred, testing$Match_Status)

```


```{r}
# http://appliedpredictivemodeling.com/blog/2014/2/1/lw6har9oewknvus176q4o41alqw2ow

library(caret)
 
set.seed(442)
training <- twoClassSim(n = 1000, intercept = -16)
testing <- twoClassSim(n = 1000, intercept = -16)

table(training$Class)

set.seed(949)
mod0 <- caret::train(Class ~ ., data = training,
               method = "rf",
               metric = "ROC",
               tuneGrid = data.frame(mtry = 3),
              trControl = trainControl(method = "cv",
                                        classProbs = TRUE,
                                        summaryFunction = twoClassSummary))
getTrainPerf(mod0)

fourStats <- function (data, lev = levels(data$obs), model = NULL) {
  ## This code will get use the area under the ROC curve and the
  ## sensitivity and specificity values using the current candidate
  ## value of the probability threshold.
  out <- c(twoClassSummary(data, lev = levels(data$obs), model = NULL))
 
  ## The best possible model has sensitivity of 1 and specifity of 1. 
  ## How far are we from that value?
  coords <- matrix(c(1, 1, out["Spec"], out["Sens"]), 
                   ncol = 2, 
                   byrow = TRUE)
  colnames(coords) <- c("Spec", "Sens")
  rownames(coords) <- c("Best", "Current")
  c(out, Dist = dist(coords)[1])
}

set.seed(949)
mod1 <- caret::train(Class ~ ., data = training,
              ## 'modelInfo' is a list object found in the linked
              ## source code
              method = modelInfo,  #commented this out because link to modelInfo is not working
              ## Minimize the distance to the perfect model
              metric = "Dist",
              maximize = FALSE,
              tuneLength = 20,
              trControl = trainControl(method = "cv",
                                       classProbs = TRUE,
                                       summaryFunction = fourStats))

ggplot(mod1)

```

