---
output:
  html_document:
    df_print: paged
    theme: "united"
    code_folding: hide
    dev: svg
    toc_depth: 2
  pdf_document: 
    df_print: paged
    keep_tex: yes
    fig_caption: yes
    latex_engine: xelatex
    pandoc_args:
     - --wrap=none
     - --top-level-division=chapter
fontfamily: mathpazo
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
title: "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
linestretch: 2
always_allow_html: yes
spacing: double
---

*Tyler M. Muffly, Merdith Alston, Jill Liss, Georg Kropat, Janet Corral, Christine Raffaelli, J. Eric Jelovsek*
- name: Tyler M. Muffly, MD
  affiliation: Denver Health
- name: Meredith Alston, MD
  affiliation: Denver Health
- name: Jill Liss
  affiliation: University of Colorado
- name: Georg Kropat, PhD
  affiliation: R & D Services
- name: Janet Corall, PhD
  affiliation: University of Colorado
- name: Christine Raffaelli
  affiliation: University of Colorado
- name: J. Eric Jelovsek
  affiliation: Duke University

# Objective:  We sought to construct and validate a model that predict a medical student's chances of matching into an obstetrics and gynecology residency.

# Data Source
The data source to be used for the project is the Electronic Residency Application Service (ERAS). The data is collected by the University of Colorado OBGYN residency that has both a categorical and a preliminary position.  Students who applied to the preliminary position were considered to be unmatched. The data set of years 2015, 2016, 2017, and 2018 applicants to the University of Colorado OBGYN residency. The data is contained in a data frame called 'all_data'.

```{r directory paths and packrat settings, include=FALSE}
data_folder <- paste0(getwd(), "/Data/")
results_folder <- paste0(getwd(), "/Results/")
data_file <- "all_years_filter_112.rds"

# uncomment the following line to restore packrat libraries
# library(packrat)
# packrat::packrat_mode(on = TRUE)
# packrat::restore()
# packrat::set_opts(local.repos = c("packrat/lib/x86_64-apple-darwin15.6.0/3.6.0/"))
```


```{r global_options, include=FALSE}
# https://stirlingcodingclub.github.io/Manuscripts_in_Rmarkdown/Rmarkdown_notes.html
knitr::opts_chunk$set(fig.width=8, 
                      fig.height=5,
                      fig.align="center",
                      include=FALSE,
                      echo=FALSE, 
                      warning=FALSE, 
                      message=FALSE, 
                      tidy = TRUE, 
                      comment="",
                      align = 'left',
                      cache = TRUE,
                      dev = "png",   #Will need to change for manuscript
                      dpi = 200)   #Will need to change for manuscript
```

```{r Install and load needed R packages, include=FALSE, echo=FALSE}
#Install and Load needed R packages.

#install.packages("anonymizer", type="source")
if(!require(pacman))install.packages("pacman")
pacman::p_load('caret', 'readxl', 'XML', 'reshape2', 'devtools', 'purrr', 'readr', 'ggplot2', 'dplyr', 'magick', 'janitor', 'lubridate', 'hms', 'tidyr', 'stringr', 'openxlsx', 'forcats', 'RcppRoll', 'tibble', 'bit64', 'munsell', 'scales', 'rgdal', 'tidyverse', "foreach", "PASWR", "rms", "pROC", "ROCR", "nnet", "packrat", "DynNom", "export", "caTools", "mlbench", "randomForest", "ipred", "xgboost", "Metrics", "RANN", "AppliedPredictiveModeling", "nomogramEx", "shiny", "earth", "fastAdaboost", "Boruta", "glmnet", "ggforce", "tidylog", "InformationValue", "pscl", "scoring", "DescTools", "gbm", "Hmisc", "arsenal", "pander", "moments", "leaps", "MatchIt", "car", "mice", "rpart", "beepr", "fansi", "utf8", "zoom", "lmtest", "ResourceSelection", "rmarkdown", "rattle", "rmda", "funModeling", "tinytex", "caretEnsemble", "broom", "Rmisc", "corrplot", "GGally", "alluvial", "progress", "perturb", "vctrs", "highr", "labeling", "DataExplorer", "rsconnect", "inspectdf", "ggpubr", "esquisse", "stargazer", "tableone", "knitr", "drake", "visNetwork", "woeBinning", "OneR", "rpart.plot", "RColorBrewer", "kableExtra", "kernlab", "naivebayes", "dplR", "e1071", "data.table", "formattable")

library(dplR)
p_install_gh("cran/doMC")
library("doMC")
registerDoMC(cores = detectCores()-1) #Use multiple cores for processing

#devtools::install_github("crsh/papaja")  #Would be good for reference management in the future.
```

# Load and Tidy the Data
## Below is a dataframe of the independent and the dependent variables for review. 
```{r Download cleaned data from Dropbox, echo=FALSE, include=FALSE, cache=TRUE}
#Create a dataframe of independent and dependent variables. 
## Here are the data for download

URL<- paste0("https://www.dropbox.com/s/qbykb8sl2c8z3me/", (data_file), "?raw=1")
download.file(url = URL, destfile = paste0(data_file), method = "curl")

# read in data
all_data <- 
  read_rds(paste0(data_folder, "/", data_file)) %>%
  select(-"Gold_Humanism_Honor_Society", 
         -"Sigma_Sigma_Phi", 
         -"Misdemeanor_Conviction", 
         -"Malpractice_Cases_Pending", 
         -"Citizenship", 
         -"BLS", 
         -"Positions_offered") 

all_data <- 
  all_data[c(
    'white_non_white', 
    'Age',  
    'Year', 
    'Gender', 
    'Couples_Match', 
    'US_or_Canadian_Applicant', 
    "Medical_Education_or_Training_Interrupted", 
    "Alpha_Omega_Alpha",  
    "Military_Service_Obligation", 
    "USMLE_Step_1_Score", 
    "Count_of_Poster_Presentation",  
    "Count_of_Oral_Presentation", 
    "Count_of_Peer_Reviewed_Journal_Articles_Abstracts", 
    "Count_of_Peer_Reviewed_Book_Chapter", 
    "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published", 
    "Count_of_Peer_Reviewed_Online_Publication", 
    "Visa_Sponsorship_Needed", 
    "Medical_Degree", 
    'Match_Status')]

#Rename columns with more human readable names
colnames(all_data)[colnames(all_data)=="Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published"] <- "Count_of_Other_than_Published"

colnames(all_data)[colnames(all_data)=="Count_of_Peer_Reviewed_Journal_Articles_Abstracts"] <- "Count_of_Articles_Abstracts"

colnames(all_data)[colnames(all_data)=="Medical_Education_or_Training_Interrupted"] <- 
  "Medical_Education_Interrupted"

colnames(all_data)[colnames(all_data)=="Count_of_Peer_Reviewed_Online_Publication"] <- 
  "Count_of_Online_Publications"

colnames(all_data)[colnames(all_data)=="Match_Status_Dichot"] <- 
  "Match_Status"

factor_columns <- all_data %>%  select_if(is.factor) %>% colnames()
#factor_columns

all_data$white_non_white <- forcats::fct_explicit_na(all_data$white_non_white, na_level="(Missing)")
all_data$Year <- forcats::fct_explicit_na(all_data$Year, na_level="(Missing)")
all_data$Gender <- forcats::fct_explicit_na(all_data$Gender, na_level="(Missing)")
all_data$Couples_Match <- forcats::fct_explicit_na(all_data$Couples_Match, na_level="(Missing)")
all_data$US_or_Canadian_Applicant <- forcats::fct_explicit_na(all_data$US_or_Canadian_Applicant, na_level="(Missing)")
all_data$Medical_Education_Interrupted <- forcats::fct_explicit_na(all_data$Medical_Education_Interrupted, na_level="(Missing)")
all_data$Alpha_Omega_Alpha <- forcats::fct_explicit_na(all_data$Alpha_Omega_Alpha, na_level="(Missing)")
all_data$Military_Service_Obligation <- forcats::fct_explicit_na(all_data$Military_Service_Obligation, na_level="(Missing)")
all_data$Visa_Sponsorship_Needed <- forcats::fct_explicit_na(all_data$Visa_Sponsorship_Needed, na_level="(Missing)")
all_data$Visa_Sponsorship_Needed <- forcats::fct_explicit_na(all_data$Visa_Sponsorship_Needed, na_level="(Missing)")
all_data$Medical_Degree <- forcats::fct_explicit_na(all_data$Medical_Degree, na_level="(Missing)")
all_data$Match_Status <- forcats::fct_explicit_na(all_data$Match_Status, na_level="(Missing)")

all_data<- 
  Hmisc::upData(all_data)
```

```{r data.table chunk, include=TRUE}
data.table(all_data)
```

```{r Using the Drake package here for efficiency, echo=FALSE, include=FALSE}
# Using the Drake package here for efficiency.  
create_plot_num <- 
  function(data) {
    funModeling::plot_num(data, path_out = results_folder)
  } 

create_plot_cross_plot <- 
  function(data) {
    funModeling::cross_plot(data, 
                            input=(colnames(data)), 
                            target="Match_Status", 
                            path_out = results_folder,
                            auto_binning = TRUE)
  } #, auto_binning = FALSE, #Export results

create_profiling_num <- 
  function(data) {
    funModeling::profiling_num(all_data)
  }

plan <- drake_plan(
  raw_data = read_rds(paste0(data_folder,"/", data_file)),
  data = raw_data %>%
    select(-"Gold_Humanism_Honor_Society", -"Sigma_Sigma_Phi", -"Misdemeanor_Conviction", -"Malpractice_Cases_Pending", -"Match_Status", -"Citizenship", -"BLS", -"Positions_offered"),
  function(data) {funModeling::plot_num(data, path_out = results_folder)},
  function(data) {funModeling::cross_plot(data, target="Match_Status", path_out = results_folder)},
  quiet = TRUE)

plan

config <- 
  drake_config(plan)

vis_drake_graph(config)

drake::make(plan)
```
# Description of the Data

A summary of the 19 variables are listed below:
  
1. Eleven of the variables were a factor.  All factors had two levels except for Alpha_Omega_Alpha had three levels.  The target variable is `all_data$Match_Status`.  

2. Seven of the variables were integers. 

3. One of the variables was a number.  Age was calculated as a number.  

```{r structure of data, include=TRUE, echo=FALSE}
# examine the structure of the initial data frame
inspectdf::inspect_types(all_data) %>% show_plot()
```

```{r structure data 2, include=FALSE}
all_data <- Hmisc::upData(all_data)
#all_data <- Hmisc::cleanup.import(all_data) #Not working
str(all_data)
```

```{r Describe data, include=FALSE}
describe(all_data)
```

#Evaluate missing data
```{r,EDA, results="asis", echo=FALSE, include=TRUE}
#plot_str(all_data) #COOL BUT USELESS HERE
DataExplorer::plot_missing(all_data)
plot_intro(all_data)
```
## Exploratory data analysis

#After the data check was completed, an exploratory data analysis (EDA) was conducted to look for interesting relationships among the variables. Histograms were used to visualize distributions among predictors. Since the assignment was a classification problem, relationships between predictors and the dichotomous outcome were also performed. Distributions of all variables were skewed right. Examples of histograms of seven variables: age, Count of articles and abstracts, count of oral presentations, count of poster presentations, Count of online pubications, count of non-published publications, count of peer-reviewed book chapters,  are demonstrated below.

#Description of Match_Status variable.  
```{r data check, include=TRUE}
describe(as.factor(all_data$Match_Status))
```

# General Data Description and Univariate analysis of boxplots for variables. 
```{r DataExplorer, results='asis', echo=FALSE, include=TRUE, align = 'left', cache=TRUE}
#General Data Description
inspectdf::inspect_cat(all_data) %>% show_plot()  #Please use `cols = c(data)`
DataExplorer::plot_histogram(all_data)
DataExplorer::plot_bar(all_data)

#Univariate of boxplots
DataExplorer::plot_boxplot(all_data, by = "Match_Status")
#inspectdf::inspect_num(all_data) %>% show_plot() #Breaks the Rmarkdown compiling for some reason.  Ugh.  
```


```{r ggpairs, warning=F, message=FALSE, include=FALSE, echo=FALSE, align = 'center', cache=TRUE}
all_data_ggally <- 
  as.data.frame(all_data) 

ggpairs(data = all_data_ggally, 
        progress = TRUE, 
        columns = 
          c("Match_Status" , 
            "Age", 
            "white_non_white", 
            "Gender"),
        bins = 30)

ggpairs(data = all_data_ggally, progress = TRUE, 
        columns = 
          c("Match_Status" ,  
            "Medical_Education_Interrupted", 
            "Alpha_Omega_Alpha", 
            "USMLE_Step_1_Score"))

ggpairs(data = all_data_ggally,
        progress = TRUE, 
        columns = 
          c("Match_Status" , 
            "Medical_Degree", 
            "Visa_Sponsorship_Needed",
            "Count_of_Peer_Reviewed_Book_Chapter"))

ggpairs(data = all_data_ggally, 
        progress = TRUE, 
        columns = 
          c("Match_Status" , 
            "US_or_Canadian_Applicant" , 
            "Couples_Match", 
            "Count_of_Poster_Presentation"))
```

# Numerical Summary Table of Variables
```{r funModeling1, echo=FALSE, message=FALSE, warning=TRUE, include=TRUE}
funModeling::df_status(all_data)
```

# Graphical Summary of Numerical Variables
```{r funModeling2, echo=FALSE, message=FALSE, warning=TRUE, include=TRUE}
create_plot_num(all_data)  #Uses custom function so it can be run in Drake
```

# Profile of Numerical Data
```{r funModeling3, echo=FALSE, message=FALSE, warning=TRUE, include=TRUE}
#Summary stats of the numerical data showing means, medians, skew
create_profiling_num(all_data)   #Uses custom function so it can be run in Drake
```

# Cross plots
```{r funModeling4, echo=FALSE, message=FALSE, warning=TRUE, include=TRUE}
#Shows the variable frequency charted by matching status
create_plot_cross_plot(all_data)   #Uses custom function so it can be run in Drake
```


IS THERE A WAY TO SET THE CUTPOINTS BASED ON A STATISTICAL REASONING?
I set the cutpoints based on nothing really. I like this better than the base summary command.  

```{r View the data using Hmisc, echo=FALSE, include=TRUE, warning=FALSE}
dd <- 
  rms::datadist(all_data)

options(datadist='dd')

s <- 
  summary(Match_Status ~ cut2(Age, 30:30) + 
            Gender + 
            Alpha_Omega_Alpha + 
            cut2(USMLE_Step_1_Score, 245:245) + 
            Couples_Match + 
            Medical_Education_Interrupted + 
            US_or_Canadian_Applicant + 
            Military_Service_Obligation + 
            Count_of_Oral_Presentation + 
            cut2(Count_of_Peer_Reviewed_Book_Chapter, 0:3) + 
            cut2(Count_of_Poster_Presentation, 0:3) + 
            white_non_white + 
            cut2(Count_of_Articles_Abstracts, 0:3) + 
            cut2(Count_of_Other_than_Published, 0:3), 
          data = all_data)
s
```

## Relaxed Cubic Splines For Continuous Variables
HOW DO YOU DECIDE HOW MANY KNOTS TO USE ON CONTINUOUS VARIABLES WITH RELAXED SPLINES?
```{r, echo=TRUE, include=FALSE, warning = FALSE, fig.width=7, fig.asp=1, fig.cap="Figure: Relaxing Cubic Splines for Continuous Variables."}
#Age Splines
Hmisc::rcspline.eval(x=all_data$Age, 
                     nk=5, type="logistic", 
                     inclx = TRUE, 
                     knots.only = TRUE, 
                     norm = 2, 
                     fractied=0.05)  

#tells where the knots are located
Hmisc::rcspline.plot(x = all_data$Age,
                     y = as.numeric(all_data$Match_Status), 
                     model = "logistic", 
                     nk = 5, 
                     showknots = TRUE, 
                     plotcl = TRUE, 
                     statloc = 11,
                     main = "Estimated Spline Transformation for Age", 
                     xlab = "Age (years)", 
                     ylab = "Probability",
                     noprint = TRUE, 
                     m = 500) #In the model Age should have rcs(Age, 5)

#Predictions with group size of 500 patients (triangles) and location of knot (arrows).
#USMLE_Step_1_Score Splines
Hmisc::rcspline.eval(x=all_data$USMLE_Step_1_Score, 
                     nk=4, 
                     type="logistic", 
                     inclx = TRUE, 
                     knots.only = TRUE, 
                     norm = 2, 
                     fractied=0.05)  #tells where the knots are located

Hmisc::rcspline.plot(x = all_data$USMLE_Step_1_Score, 
                     y = as.numeric(all_data$Match_Status), 
                     model = "logistic", 
                     nk=5, 
                     showknots = TRUE, 
                     plotcl = TRUE, 
                     statloc = 11, 
                     main = "Estimated Spline Transformation for USMLE Step 1 Score", 
                     xlab = "USMLE Step 1 Score", 
                     ylab = "Probability", 
                     noprint = TRUE, 
                     m = 500) #In the model USMLE_Step_1 should have rcs(USMLE_Step_1, 6)

#Count of Posters
Hmisc::rcspline.eval(x=all_data$Count_of_Poster_Presentation, 
                     nk=5, 
                     type="logistic", 
                     inclx = TRUE, 
                     knots.only = TRUE, 
                     norm = 2, 
                     fractied=0.05)  #tells where the knots are located

Hmisc::rcspline.plot(x = all_data$Count_of_Poster_Presentation, 
                     y = as.numeric(all_data$Match_Status), 
                     model = "logistic", 
                     nk=5, 
                     showknots = TRUE, 
                     plotcl = TRUE, 
                     statloc = 11, 
                     main = "Estimated Spline Transformation for Poster Presentations", 
                     xlab = "Count of Poster Presentations", ylab = "Probability", noprint = TRUE, m = 500) #In the model Count of Poster presentations should have rcs(Count of Poster Presentations, 4)

#Count of Oral Presentations
Hmisc::rcspline.eval(x=all_data$Count_of_Oral_Presentation, 
                     nk=5, type="logistic",
                     inclx = TRUE, 
                     knots.only = TRUE, 
                     norm = 2, 
                     fractied=0.05)  #tells where the knots are located

Hmisc::rcspline.plot(x = all_data$Count_of_Oral_Presentation, 
                     y = as.numeric(all_data$Match_Status), 
                     model = "logistic", 
                     nk = 5, 
                     showknots = TRUE, 
                     plotcl = TRUE, 
                     statloc = 11, 
                     main = "Estimated Spline Transformation for Oral Presentations", 
                     xlab = "Count of Oral Presentations", 
                     ylab = "Probability", 
                     noprint = TRUE, 
                     m = 1000) #In the model Count of Oral Presentations should have rcs(Count of Oral Presentations, 3)
```

```{r, echo=FALSE, include=FALSE}
all_data$Match_Status <- 
  as.numeric(all_data$Match_Status)

all_data$Match_Status

all_data$Match_Status <- 
  (all_data$Match_Status - 1)  

all_data$Match_Status #Outcome must be numeric
```

# Table 1: Applicant Descriptive Variables by Matching Success (1) or Failure (0)
```{r, echo=FALSE, warning=FALSE, message=FALSE, include=TRUE, results="asis"}
table1_all_data <- arsenal::tableby(Match_Status ~
                                      white_non_white +
                                      Age +
                                      Gender +
                                      Couples_Match +
                                      #Expected_Visa_Status_Dichotomized +
                                      US_or_Canadian_Applicant +
                                      #Medical_School_Type +
                                      Medical_Education_Interrupted +
                                      #Misdemeanor_Conviction +
                                      Alpha_Omega_Alpha +
                                      #Gold_Humanism_Honor_Society +
                                      Military_Service_Obligation +
                                      USMLE_Step_1_Score +
                                      Military_Service_Obligation +
                                      Count_of_Poster_Presentation +
                                      Count_of_Oral_Presentation +
                                      Count_of_Articles_Abstracts +
                                      Count_of_Peer_Reviewed_Book_Chapter +
                                      Count_of_Other_than_Published+
                                      Count_of_Online_Publications +
                                      Visa_Sponsorship_Needed +
                                      Medical_Degree,
                                    data=all_data,
                                    control = tableby.control(test = TRUE,
                                                              total = F,
                                                              digits = 1L,
                                                              digits.p = 2L,
                                                              digits.count = 0L,
                                                              numeric.simplify = F,
                                                              numeric.stats =
                                                                c("median",
                                                                  "q1q3"),
                                                              cat.stats =
                                                                c("Nmiss",
                                                                  "countpct"),
                                  stats.labels = list(Nmiss = "N Missing",
                                  Nmiss2 ="N Missing",
                                  meansd = "Mean (SD)",
                                  medianrange = "Median (Range)",
                                  median ="Median",
                                  medianq1q3 = "Median (Q1, Q3)",
                                  q1q3 = "Q1, Q3",
                                  iqr = "IQR",
                                  range = "Range",
                                  countpct = "Count (Pct)",
                                  Nevents = "Events",
                                  medSurv ="Median Survival",
                                  medTime = "Median Follow-Up")))

mylabels <- list(white_non_white = "Race", Age = "Age, years", Gender = "Sex", Couples_Match = "Participating in the Couples Match", US_or_Canadian_Applicant = "US or Canadian Applicant", Medical_Education_Interrupted = "Medical Education Process was Interrupted", Alpha_Omega_Alpha = "Alpha Omega Alpha", Military_Service_Obligation = "Military Service Obligation", USMLE_Step_1_Score = "USMLE Step 1 Score", Military_Service_Obligation = "Military Service Obligations", Count_of_Poster_Presentation = "Count of Poster Presentations", Count_of_Oral_Presentation = "Count of Oral Presentations", Count_of_Articles_Abstracts = "Count of Published Abstracts", Count_of_Peer_Reviewed_Book_Chapter = "Count of Peer Reviewed Book Chapters", Count_of_Other_than_Published = "Count of Other Published Products", Count_of_Online_Publications = "Count of Online Publications", Visa_Sponsorship_Needed = "Visa Sponsorship is Needed", Medical_Degree = "Medical Degree Training")

summary(table1_all_data,
        text=T,
        title = 'Table: Applicant Descriptive Variables by Matching Success or Failure from 2015 to 2018',
        labelTranslations = mylabels,
        pfootnote=TRUE)
```

```{r Descriptive summaries of all variables in the dataset are provided in the table}
#Not working
stargazer::stargazer(all_data, 
                     header=FALSE, 
                     title = "Descriptive Statistics of Match Status Data", 
                     type = 'latex',
                     nobs = TRUE, 
                     mean.sd = TRUE, 
                     median = TRUE, 
                     iqr = TRUE, 
                     digits = 1, 
                     font.size = "small", 
                     flip = FALSE)
```

# Split into train and test data sets
```{r, train vs test, warning=FALSE, echo=TRUE, message=FALSE, include=TRUE}
train <- filter(all_data, Year %in% c("2015", "2016"))  #Train on years 2015, 2016
nrow(train) 
test <- filter(all_data, Year %in%  c("2017", "2018")) #Test on 2017, 2018 data
nrow(test)
test <- test %>% select(-"Year")
train <- train %>% select(-"Year")
```

# Demographics of train
```{r, include = TRUE, fig.width=8, fig.height=3, fig.cap="Figure: Train dataset looking at continuous variables for normality."}
plot_histogram(train[, 2]) #age
plot_histogram(train[, 10:12])
plot_histogram(train[, 13:15])
```

## Normality testing in R

The D'Agostino tests for skewness and the Anscombe tests for kurtosis with numeric variables. There is kurtosis for the Step 1 score data.  Therefore only use medians in table 1 and I will stick with non-parametric tests throughout. There is skew in age.   

Quantile-Quantile plot is a way to visualize the deviation from a specific probability distribution. After analyzing these plots, it is often beneficial to apply mathematical transformation (such as log) for models like linear regression. DO WE NEED TO TRANSFORM THESE VARIABLES IN ANY WAY?  SHOULD WE USE Kolmogorov-Smirnov (K-S) normality test OR Shapiro-Wilk’s test???

The null hypothesis of these tests is that “sample distribution is normal”. If the test is significant, the distribution is non-normal.  From the output, the p-value less than 0.05 implying that the distribution of the data are  significantly different from normal distribution. In other words, we can not assume the normality.

```{r, message=FALSE, echo=FALSE}
#Examination of skewness and kurtosis for numeric values, Zhang book page 65
moments::agostino.test(all_data$Age) #D'Agostino skewness test is positive for skewness
moments::anscombe.test(all_data$USMLE_Step_1_Score)  #There is kurtosis for the Step 1 score data.  
DataExplorer::plot_qq(all_data)
shapiro.test(all_data$Age)
shapiro.test(all_data$Count_of_Oral_Presentation)
shapiro.test(all_data$Count_of_Poster_Presentation)
shapiro.test(all_data$Count_of_Peer_Reviewed_Book_Chapter)
shapiro.test(all_data$Count_of_Articles_Abstracts)
shapiro.test(all_data$USMLE_Step_1_Score)
```

This allowed for measuring the associations between continuous predictors using a matrix with correlation coefficients. The scatterplot matrix of a sample of predictors below demonstrated some associations. CAN WE INCLUDE ONLY CONTINUOUS VARIABLES?  SHOULD WE INCLUDE THE TARGET VARIABLE: MATCH_STATUS?
  
  ## Correlation overview
  
  In this kind of plot we want to look for the bright, large circles which immediately show the strong correlations (size and shading depends on the absolute values of the coefficients; color depends on direction).  This shows whether two features are connected so that one changes with a predictable trend if you change the other. The closer this coefficient is to zero the weaker is the correlation. Anything that you would have to squint to see is usually not worth seeing!  CAN ONLY CORRELATE CONTINOUS VARIABLES?  SHOULD I RUN CORRELATION ON THE TRAIN DATA OR ON THE ALL_DATA SET?
  
```{r, warning= FALSE, message=FALSE, echo=FALSE, include=TRUE}
set.seed(0)
train_corrleation <- 
  train %>% 
  select(Age, 
         USMLE_Step_1_Score, 
         Count_of_Poster_Presentation, 
         Count_of_Oral_Presentation, 
         Count_of_Articles_Abstracts, 
         Count_of_Peer_Reviewed_Book_Chapter, 
         Count_of_Other_than_Published, 
         Count_of_Online_Publications) %>%
  #mutate(Gender = fct_recode(Gender,"0" = "Male", "1" = "Female")) %>%
  #mutate(Couples_Match = fct_recode(Couples_Match,"0" = "No", "1" = "Yes")) %>%
  #mutate(US_or_Canadian_Applicant = fct_recode(US_or_Canadian_Applicant,"0" = "No", "1" = "Yes")) %>%
  #mutate(Medical_Education_Interrupted = fct_recode(Medical_Education_Interrupted,"0" = "No", "1" =      "Elections_Senior_Year", "2" = "Yes")) %>%
  mutate_all(as.integer) %>%
  stats::cor(use="pairwise.complete.obs") %>%
  corrplot::corrplot(type="lower", 
                     diag=FALSE, 
                     order = "hclust", 
                     tl.cex = 0.5, 
                     tl.col = "black", 
                     sig.level = "0.01", 
                     insig = "blank", 
                     pch = TRUE)
```

p-value associated with the null hypothesis of 0 correlation, small values indicate evidence that the true correlation is not equal to 0.

```{r, echo=FALSE}
inspect_cor(all_data, show_plot = FALSE, alpha = 0.05)
```

I'M NOT SURE WHY THE PROPORTION OF MATCHED VS UNMATCHED DATA IS DIFFERENT BETWEEN THE TRAIN VS TEST DATA SETS?  I THOUGHT ABOUT REMAKING THE TRAINING SET TO BE 2015 AND 2016 WITH THE TEST SET OF 2017 AND 2018 IF WE CAN'T FIGURE IT OUT.
```{r Match proportion vs year, align = 'left', include=TRUE}
all_data %>% 
  dplyr::count(Year, Match_Status) %>% 
  spread(key=Match_Status, value = n) %>% 
  mutate(sum = `0` + `1`) %>% 
  mutate(`0` = `0`/sum, `1` = `1`/sum) %>% 
  select(-sum) %>% 
  tidyr::gather(value=proportion, key=Match_Status, `0`, `1`) %>% 
  ggplot(aes(x = Year, y = proportion, fill = Match_Status, label = paste(round(proportion * 100,1),"%"))) +
  # geom_col(col="black", position="fill") +
  geom_bar(col = "black", stat = "identity") +
  labs(x="Year", fill = "Match Status", y = "Proportion matched/unmatched in %") +
  theme_minimal() +
  geom_text(position = "stack", size = 3,  vjust=1.15) +
  scale_y_continuous(breaks = seq(0, 1, 0.25),  labels = seq(0, 100, 25))
```

Check Proportions of Matched

```{r, results="asis", echo=FALSE}
train <- train %>% 
  mutate(Match_Status = factor(Match_Status,
                               levels=c(0,1),
                               labels=c("NoMatch", "Matched")))
test <- test %>% 
  mutate(Match_Status = factor(Match_Status,
                               levels=c(0,1),
                               labels=c("NoMatch", "Matched")))
# levels(train$Match_Status) <- 
#   c("NoMatch", 
#     "Matched")
# 
# levels(test$Match_Status) <- 
#   c("NoMatch", 
#     "Matched")

# Examine the proportions of the Match_Status class lable across the datasets.
crude_summary <- 
  prop.table(table(all_data$Match_Status))  #Original data set proportion 

prop.table(table(train$Match_Status)) #Train data set proportion

prop.table(table(test$Match_Status))  #Test data set proportion

kable(crude_summary, caption="2x2 Contingency Table on Matching for all_data", format="markdown")
```

##Compare the datasets of train and test
```{r, results="asis", include=FALSE, echo=FALSE}
summary(arsenal::comparedf(train, test))
```

IS IT NECESSARY TO CREATE A MODEL WITH ALL THE VARIABLES FIRST?  SPLINES QUESTION FROM ABOVE.  

Create a Kitchen Sink model with all factors first. This is essentially a screening model with all variables. I relaxed the cubic splines with the guidance from above.  

```{r, echo=FALSE}
d <- 
  datadist(train)

options(datadist = "d")

kitchen.sink <- 
  lrm(Match_Status ~ 
        white_non_white +  
        rms::rcs(Age, 5) + 
        Gender +  
        Couples_Match + 
        US_or_Canadian_Applicant +  
        Medical_Education_Interrupted + 
        Alpha_Omega_Alpha +  
        Military_Service_Obligation + 
        rms::rcs(USMLE_Step_1_Score, 4) + 
        rms::rcs(Count_of_Poster_Presentation,3) +  
        Count_of_Oral_Presentation + 
        Count_of_Articles_Abstracts + 
        Count_of_Peer_Reviewed_Book_Chapter + 
        Count_of_Other_than_Published + 
        Count_of_Online_Publications + 
        Visa_Sponsorship_Needed + 
        Medical_Degree, 
      data = train, 
      x = T, 
      y = T)

kitchen.sink
```

Are there predictor interactions?????  HOW WOULD I RECOGNIZED THE INTERACTIONS AND DISPLAY THE RESULTS IN A READABLE WAY?
  
```{r, echo=TRUE}
lm.fit2 <- 
  lm(Match_Status ~ 
       (white_non_white + 
          Age + Gender +  
          Couples_Match + 
          US_or_Canadian_Applicant +  
          Medical_Education_Interrupted + 
          Alpha_Omega_Alpha +  
          Military_Service_Obligation + 
          USMLE_Step_1_Score + 
          Count_of_Poster_Presentation +  
          Count_of_Oral_Presentation + 
          Count_of_Articles_Abstracts + 
          Count_of_Peer_Reviewed_Book_Chapter + 
          Count_of_Other_than_Published + 
          Count_of_Online_Publications + 
          Visa_Sponsorship_Needed + 
          Medical_Degree)^2, 
     data = all_data)
```

```{r, include=FALSE, echo=FALSE}
lm.fit3 <- 
  anova(lm.fit2)
```

```{r, echo=FALSE}
lm.fit3 %>% 
  mutate(variable_name = row.names(lm.fit3)) %>% 
  mutate(adjusted_P = p.adjust(`Pr(>F)`, n = nrow(lm.fit3))) %>% 
  filter(str_detect(variable_name, ":") & adjusted_P<0.05) %>% 
  select(variable_name,`Pr(>F)`, adjusted_P) %>% 
  arrange(`Pr(>F)`)
```

```{r, warning=FALSE, echo=FALSE}
#Shows the C-statistic and the Brier score.  
kitchen.sink.model.stats <- 
  as.data.frame(kitchen.sink$stats)

knitr::kable(kitchen.sink.model.stats, 
             caption = "Performance statistics of the Kitchen Sink Model Using All Variables", 
             digits=2)
```

## HOW DO I CHECK FOR COLLINEARITY IN THE VARIABLES?  THE VARIABLES WITH SPLINES HAVE A VERY HIGH VIF.  WHY IS THIS AND SHOULD I KEEP THEM IN?

```{r}
car::vif(kitchen.sink, echo=FALSE)
```

## Evaluating the signficance of kitchen.sink variables 

According to the ANOVA: USMLE_Step_1_Score, Age, and US_or_Canadian_Applicants are predictors.  The anova() function for the model object allows to see the null and residuals deviances. The difference between these two deviances shows how well the model is performing against the null deviance. The residuals deviance column allows to see the drop of deviance value by additional respective predictor term added.

```{r, echo=TRUE, message=FALSE,fig.width=7, fig.asp=1, fig.cap="Figure: Variance-inflation factors for matching into OBGYN."}
plot(anova(kitchen.sink, test = 'Chisq'), cex=0.9, cex.lab=0.9, cex.axis = 0.7)
```

#Factor Selection

I LIKED LASSO BECAUSE CHOOSING SOME PEOPLE MAY FIND THE PREDICTORS OR AGE, RACE, GENDER DISCRIMINATORY.  I NEED SOME STATISTICAL TEST TO STAND ON ABOUT WHY I CHOSE THESE VARIABLES.

#Factor Selection using a LASSO model (Penalized Logistic Regression)

Here, we use Lasso for simplicity and interpretability. The aim is to avoid over-parametrization and unnecessary model bias by carrying feature selection on-the-go. Key to this task will be cross-validation.  Start by creating a custom train control providing the number of cross-validations and setting the classProbs to TRUE for logistic regression. 

WOULD YOU DO MORE REPEATS IN MYCONTROL?
```{r, echo=TRUE}
# Create custom trainControl: myControl
set.seed(1978)
myControl <- 
  trainControl(
    method = "repeatedcv",
    number = 10,
    repeats = 5,
    summaryFunction = twoClassSummary,
    classProbs = TRUE, # IMPORTANT!
    verboseIter = FALSE)

# train$Match_Status <- 
#   as.factor(train$Match_Status)
# 
# test$Match_Status <- 
#   as.factor(test$Match_Status)
# 
# #Levels of the target outcome variable for glmnet need to be words and not numbers.  
# levels(train$Match_Status) <- 
#   c("NoMatch", "Matched")
# 
# levels(test$Match_Status) <- 
#   c("NoMatch", "Matched")
```

Create the LASSO using glmnet within the caret package.  Here we are solely using the train dataset to determine what varaiables predict the outcome.  

```{r}
# Train glmnet with custom trainControl and tuning: model
lasso.mod <- 
  caret::train(
    Match_Status ~ .,
    data = train,
    family = "binomial",
    tuneGrid = expand.grid(
      alpha = 0:1,
      lambda = seq(0.0001, 1, length = 20)
    ),
    method = "glmnet",
    metric = "ROC",
    trControl = myControl)
```

IS THERE A COEFFICIENT CUTOFF THAT YOU WOULD USE TO CHOSE THE FINAL PREDICTORS OR NO?
```{r, echo=TRUE, include=FALSE}
# Print model to console
print(lasso.mod)

#summary(lasso.mod)

#lasso.mod[["results"]]

lasso.mod$bestTune #Final model is more of a ridge and less of a LASSO model

best <- 
  lasso.mod$finalModel

coef(best, s=lasso.mod$bestTune$lambda) ###Look for the largest coefficient
```

Plot the results of the lasso.mod so we can see if this is more ridge or more lasso.  0 = ridge regression and 1 = LASSO regression, here ridge is better

```{r, fig.width=7, fig.asp=1, fig.cap="Figure: Plotting the results of ridge or lasso in regression"}
plot(lasso.mod)
```

DO WE NEED TO SAVE THE LASSO MODEL FOR ANY REASON IN THE FUTURE?
  
  ## Plot LASSO factors
  
  Plot the individual variables by lambda.  Saves the lasso.mod to an RDS file for later use.  

```{r,  fig.asp=1}
plot(lasso.mod$finalModel, xvar = 'lambda', label = TRUE)
#legend("topright", lwd = 1, col = 1:5, legend = colnames(train), cex = .6)
#https://www.datacamp.com/community/tutorials/tutorial-ridge-lasso-elastic-net
colnames(train[1:17])
saveRDS(lasso.mod, "best.LASSO.rds")  #save the model
```



Makes predictions of matching based on the lasso.mod using the training data.  
```{r, echo=TRUE, warning=FALSE, include=FALSE}
###
predict(lasso.mod, newx = x[1:5,], type = "prob", s = c(0.05, 0.01))
```

GLMNet to do factor selection with the previously made LASSO model
And we use the glmnet library to determine the optimal penalization parameter. Note that this must be assigned through cross validation; here, we use 50-fold cross validation (only suitable in small datasets).  GLMnet accepts data in a matrix format so the data format was changed before giving it to glmnet.cv. 

```{r, echo=TRUE, message=FALSE, include=TRUE, fig.cap="Figure: Determining optimal penalization parameters for factor selection with the least absolute shrinkage and selection operator (LASSO) model."}
`%ni%`<-Negate(`%in%`)
# save the outcome for the glmnet model, could use dummyVars with fullRan=FALSE can remove collinearity by removing male.gender so you are either male or female

x <- model.matrix(train$Match_Status~., data=train)

class(x)

x <- x[,-1]  #Removes intercept

set.seed(356)

glmnet1 <- 
  cv.glmnet(x=x,y=train$Match_Status,nfolds=10,alpha=.5, family="binomial")

plot(glmnet1,main = "Misclassification Error")
```

The left vertical line represents the minimum error, and the right vertical line represents the cross-validated error within 1 standard error of the minimum. LASSO, least absolute shrinkage and selection operator

If you look at this graph we ran the model with a range of values for lambda and saw which returned the lowest cross-validated error. You'll see that our cross-validated error remains consistent until we hit the dotted lines, where we start to see our model perform very poorly due to underfitting with misclassification error.  Cross validation is an essential step in studies to help up us not only calibrate the parameters of our model but estimate the prediction accuracy with unseen data.

Variable selection using LASSO in the train dataset

```{r, echo=TRUE, warning=FALSE, message=FALSE}
c <- 
  coef(glmnet1,s='lambda.min',exact=TRUE)

inds <-
  which(c!=0)

variables <- 
  row.names(c)[inds]

variables <- 
  variables[variables %ni% '(Intercept)']
#variables  ###What variables should be included in the model per LASSO!!!
```

```{r, results="asis"}
knitr::kable(variables, caption = "Variables Chosen by LASSO to Predict Matching into OBGYN based on the Train Data (2015, 2016, 2017)")
```


I DON'T REALLY UNDERSTAND WHAT THIS GRAPH MEANS?
  
  ## Measuring Strength and Direction of Predictors
  
  I plotted everything on a bar graph so we can easily compare the strongest predictors and the direction they affect the model:
  
```{r, fig.cap="Figure: Plot of the strongest predictors and the direction that the predictors affect matching."}
#https://amunategui.github.io/supervised-summarizer/index.html
GetSummaryPlot <- function(objdfscaled0, objdfscaled1, predictorName, plotit=TRUE) {
  require(ggplot2)
  
  stats0 <- (summary(objdfscaled0[,predictorName]))
  stats0 <- c(stats0[1:6])
  stats1 <- (summary(objdfscaled1[,predictorName]))
  stats1 <- c(stats1[1:6])
  stats <- data.frame('ind'=c(1:6), 'stats1'=stats1,'stats0'=stats0)
  
  spread <- ((stats1[[1]] - stats0[[1]]) +
               (stats1[[2]] - stats0[[2]]) +
               (stats1[[3]] - stats0[[3]]) +
               (stats1[[4]] - stats0[[4]]) +
               (stats1[[5]] - stats0[[5]]) +
               (stats1[[6]] - stats0[[6]]))
  
  if (plotit) {
    print(paste('Scaled spread:',spread))
    p <- ggplot(data=stats, aes(ind)) +
      geom_line(aes(y = stats1, colour = "stats1")) +
      geom_line(aes(y = stats0, colour = "stats0")) +
      scale_x_discrete(breaks = 1:6,
                       labels=c("min","1q","median","mean","3q","max")) +
      ylab(predictorName) + xlab(paste('Spread:',spread))
    return (p)
  } else {
    return (spread)
  }
}
#Create predictorsNames variable
outcomeName <- 'Match_Status'
predictorsNames <- names(test)[names(test) != outcomeName]  #Removes outcome from list of predictrs
# Temporarily remove the outcome variable before scaling the data set
titanicDF <- all_data
outcomeValue <- titanicDF$Match_Status
dim(titanicDF)
# binarize all factors
require(caret)
titanicDF$Match_Status <- as.numeric(titanicDF$Match_Status)
levels(titanicDF$Match_Status)
titanicDummy <- dummyVars("~.",data=titanicDF, fullRank=F)
titanicDF <- as.data.frame(predict(titanicDummy,titanicDF))
head(titanicDF)
# scale returns a matrix so we need to tranform it back to a data frame
scaled_titanicDF <- as.data.frame(scale(titanicDF))
scaled_titanicDF$Match_Status <- outcomeValue
# split your data sets
scaled_titanicDF_0 <- scaled_titanicDF[scaled_titanicDF[,outcomeName]==0,]
scaled_titanicDF_1 <- scaled_titanicDF[scaled_titanicDF[,outcomeName]==1,]

outcomeName <- 'Match_Status'
predictorNames <- names(titanicDF)[!names(titanicDF) %in% outcomeName]

for (predictorName in predictorNames){
  print(paste(predictorName,':',GetSummaryPlot(scaled_titanicDF_0,
                                               scaled_titanicDF_1, predictorName, plotit=FALSE)))
  summaryImportance <- c()
  variableName <- c()
}

for (predictorName in predictorNames) {
  summaryImportance <-  c(summaryImportance, GetSummaryPlot(scaled_titanicDF_0, scaled_titanicDF_1, predictorName, plotit=FALSE))
  variableName <- c(variableName, predictorName)
}

results <- 
  data.frame('VariableName'=variableName, 'Weight'=summaryImportance)

# display variable importance on a +/- scale 
results <- 
  results[order(results$Weight),]

results <- 
  results[(results$Weight != 0),]

par(mar=c(5,15,4,2)) # increase y-axis margin. 


barplot(results$Weight, width = 0.85, 
        main = paste("Variable Importance of all_data - Matching"), horiz = T, 
        xlab = "< (-) importance >  < neutral >  < importance (+) >", axes = FALSE, 
        col = ifelse((results$Weight > 0), 'blue', 'red')) +
  # axis(2, at=xx, labels=results$VariableName, tick=FALSE, las=2, line=-0.3, cex.axis=0.6)  
  axis(2, at= seq_along(results$VariableName), labels=results$VariableName, tick=FALSE, las=2, line=-0.3, cex.axis=0.6)  
```


Revise Model with selected factors

Creating a more parsiomonious model using the variables selected by LASSO in the train dataset. I made the model with lrm so I could fit it to a rms::nomogram function.  

```{r, echo=TRUE, results="asis", warning=FALSE}
d <- 
  datadist(test)

options(datadist = "d")

lrm.with.lasso.variables <- 
  lrm(Match_Status ~ 
        rms::rcs(Age, 5) + 
        Alpha_Omega_Alpha + 
        Count_of_Oral_Presentation + 
        Count_of_Peer_Reviewed_Book_Chapter + 
        Couples_Match + 
        Gender + 
        Medical_Degree + 
        Military_Service_Obligation + 
        US_or_Canadian_Applicant +  
        rms::rcs(USMLE_Step_1_Score, 4) + 
        Visa_Sponsorship_Needed + 
        white_non_white, 
      data = train, 
      x = T, 
      y = T)

#lrm.with.lasso.variables$stats  #Shows the C-statistic and the Brier score.  
knitr::kable(broom::tidy(lrm.with.lasso.variables$stats), digits =2, caption = "Performance statistics of the Training Model")

lrm.with.lasso.variables$stat[[6]]  #C-statistic
```

I THINK IT WAS WRONG TO DROP SOME OF THESE VARIABLES THAT HAD RELAXED CUBIC SPLINES:
  
  ## Remove variables with multicollinearity and rebuild model
  
```{r, echo=TRUE, results="asis", warning=FALSE}
d <- 
  datadist(test)

options(datadist = "d")

#Removed Age, AOA and Medical Degree, C-statistic dropped from 0.84 to 0.83
lrm.with.lasso.variables <- 
  lrm(Match_Status ~ 
        Count_of_Oral_Presentation + 
        Count_of_Peer_Reviewed_Book_Chapter + 
        Couples_Match + 
        Gender +  
        Military_Service_Obligation + 
        US_or_Canadian_Applicant +  
        rms::rcs(USMLE_Step_1_Score, 4) + 
        Visa_Sponsorship_Needed + 
        white_non_white, 
      data = train, 
      x = T, 
      y = T)

#lrm.with.lasso.variables$stats  #Shows the C-statistic and the Brier score.  
knitr::kable(broom::tidy(lrm.with.lasso.variables$stats), digits =2, caption = "Performance statistics of the Training Model")

lrm.with.lasso.variables$stats[[6]] #C-statistic
```

AUC of this pared down model is `r (lrm.with.lasso.variables$stats[[6]])`.

```{r, echo=FALSE,  fig.width=7, fig.asp=1, fig.cap="Figure: Charting the strength of the variables chosen using LASSO."}
lrm.with.lasso.variables
#dev.off()
plot(anova(lrm.with.lasso.variables), cex=0.5, cex.lab=0.4, cex.axis = 0.4)
```


```{r, include=F}
summary(lrm.with.lasso.variables)
```
## Odds ratios of the `train` dataset

#Table 2 of odds ratios in graph form in the train dataset.  
```{r, echo=TRUE,  fig.width=7, fig.asp=1, fig.cap="Figure: Odds ratios of the training data set to predict matching into OBGYN residency."}
plot(summary(lrm.with.lasso.variables), cex=0.5, cex.lab=0.7, cex.axis = 0.7)
```

Odds ratios for train data
#https://rstudio-pubs-static.s3.amazonaws.com/283447_fd922429e1f0415c89b93b6da6dc1ccc.html

```{r, results="asis"}
#For example, increase one unit in age will decrease the log odd of survival by 0.039; being a male will decrease the log odd of survival by 2.7 compared to female; and being in class2 will decrease the log odd of survival by 0.92, being in class3 will decrease the log odd of survival by 2.15. 
oddsratios <- 
  as.data.frame(exp(cbind("Adjusted Odds ratio" = coef(lrm.with.lasso.variables),
                          confint.default(lrm.with.lasso.variables, level = 0.95))))

#I'm not sure how to set the significant digits
knitr::kable(oddsratios, digits = 2)
```

Annotation for Manuscript Table 2:  A:  Nonlinear component A of the function describing the variable and the probability of matching into OBGYN.  B:  Nonlinear component B of the function describing the variable and the probability of matching into OBGYN.  C:  Nonlinear component C of the function describing the variable and the probability of matching into OBGYN.  

# Use Model to predict match for Test Data

Shift Gears: Test Accuracy of Model on Training Data, Use glmnet model on 2018 TEST data
Here the code is creating a vector called predictorsNames so that we can reuse the model by changing the variables in predictorsNames in the future prn.  Run the 2018 data through the train model.  

```{r, warning=FALSE, message=FALSE}
#Create predictorsNames variable
outcomeName <- 
  'Match_Status'

predictorsNames <- 
  names(test)[names(test) != outcomeName]  #Removes outcome from list of predictrs

# get predictions on your testing data
b <- 
  model.matrix(test$Match_Status~., 
               data=test) #x <- model.matrix(train$Match_Status~., data=train)

a <- 
  b[,-1]  #Removes intercept from the matrix as we did for model

predictions <- 
  predict(object = glmnet1, 
          newx=a, 
          s="lambda.min", 
          family = "binomial")  #What is the matrix?

#predictions
d <- 
  as.data.frame(test[,outcomeName])

levels(d$Match_Status) <- 
  c("NoMatch", "Matched")

test$Match_Status <- 
  as.numeric(test$Match_Status)-1  #pROC only accepts numeric variables, not a matrix

# test$Match_Status <- 
#   (test$Match_Status - 1)

predictions <- 
  as.numeric(predictions)
```

First, we need to fit lrm.with.lasso.variables in GLM, rather than rms to get the AUC.  There is probably a better way to do this.  Using the test data set.  Also built the same model in lrm.  

```{r, echo=TRUE, warning=FALSE}
# test$Match_Status <- 
#   as.integer(test$Match_Status+2)

test <- test %>% 
  mutate(Match_Status = factor(Match_Status,
                               levels=c(0,1),
                               labels=c("NoMatch", "Matched")))

train.glm.with.lasso.variables  <- 
  glm(Match_Status ~ 
        rms::rcs(Age, 5) + 
        Alpha_Omega_Alpha + 
        Count_of_Oral_Presentation + 
        Count_of_Peer_Reviewed_Book_Chapter + 
        Couples_Match + Gender + Medical_Degree + 
        Military_Service_Obligation + 
        US_or_Canadian_Applicant +  
        rms::rcs(USMLE_Step_1_Score, 4) + 
        Visa_Sponsorship_Needed + 
        white_non_white,
      data = test, 
      family = "binomial"(link=logit))  

train.lrm.with.lasso.variables <- 
  lrm(Match_Status ~ 
        rms::rcs(Age, 5) + 
        Alpha_Omega_Alpha + 
        Count_of_Oral_Presentation + 
        Count_of_Peer_Reviewed_Book_Chapter + 
        Couples_Match + 
        Gender + 
        Medical_Degree + 
        Military_Service_Obligation + 
        US_or_Canadian_Applicant + 
        rms::rcs(USMLE_Step_1_Score, 4) + 
        Visa_Sponsorship_Needed + 
        white_non_white, 
      data = train, 
      x = T, 
      y = T)
```

```{r}
#lrm.with.lasso.variables$stats  #Shows the C-statistic and the Brier score.  
test.model.stats <- as.data.frame(train.lrm.with.lasso.variables$stats)
knitr::kable(test.model.stats, caption = "Performance statistics of the Testing Model", digits=2)
```



#The Receiver Operating Characteristic (ROC) curve is plotted below for false positive rate (FPR) in the x-axis vs. the true positive rate (TPR) in the y-axis. It shows the detection of true positive while avoiding the false positive. This is the same as measuring the unspecificity (1 - specificity) in x-axis, against the sensitivity in y-axis. This ROC curve in particular shows that its very closed to the perfect classifier meaning that its better at identifying the positive values. 

## Use Model to predict match Status for Test Data
```{r}
#Use Model to predict match Status for Test Data
prob <- 
  predict(train.glm.with.lasso.variables, newdata = test, type="response")
dim(test)

pred <- 
  prediction(prob, test$Match_Status)  #removed na.omit
```

#ROC: Type 1 using ggplot with nice controls
```{r, include = TRUE}
# rest of this doesn't need much adjustment except for titles
perf <-
  performance(pred, measure = "tpr", x.measure = "fpr")

auc <- 
  performance(pred, measure="auc")

auc <- 
  round(auc@y.values[[1]],3)

roc.data <- 
  data.frame(fpr=unlist(perf@x.values),
             tpr=unlist(perf@y.values),
             model="GLM")

ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  labs(title = paste0("ROC Curve with area under the curve = ", auc),
       subtitle = "Model A for test data")
```


ROC Curve type 2 with nice labels on the x and y
```{r, include = TRUE}
pred <- 
  prediction(prob, test$Match_Status)

perf <- 
  performance(pred, measure = "tpr", x.measure = "fpr")

plot(perf)

auc <- 
  performance(pred, measure = "auc")

auc <- 
  auc@y.values[[1]]

auc  
```

ROC Curve Type 3 with nice diagnal line but half of the formula printed
```{r, include = TRUE}
#library(rJava)
#Deducer::rocplot(glm.with.lasso.variables, diag = TRUE, prob.label.digits = TRUE, AUC = TRUE)
```

ROC Curve Type 4, ROC in color
```{r, include = TRUE}
perf <- 
  performance(pred, 'tpr','fpr')

plot(perf, colorize = TRUE, text.adj = c(-0.2,1.7), main="Receiver-Operator Curve for Model A")

#Plots of Sensitivity and Specificity
perf1 <- 
  performance(pred, "sens", "spec")

plot(perf1, colorize = TRUE, text.adj = c(-0.2,1.7), main="Sensitivity and Specificity for Model A")

## precision/recall curve (x-axis: recall, y-axis: precision)
perf2 <- 
  performance(pred, "prec", "rec")

plot(perf2, colorize = TRUE, text.adj = c(-0.2,1.7), main="Precision and Recall for Model A")
```


```{r, include = TRUE, fig.width=8, fig.height=4, fig.cap="Figure: Evaluation of the variable interactions in the train data set."}
train.df <- 
  train

psych::pairs.panels(train.df[, c(9:15, 18)], bg=c("red","blue")[as.factor(train.df$Match_Status)], pch=21, jiggle = TRUE)
```

Since the predictors were highly skewed, binning was also explored. This facilitated visualizing associations between binned variables and the outcome using contingency plots. Supervised Weight of Evidence (WOE) binning of numeric variables were explored using the woeBinning package. Fine and coarse classing that merged granular classes and levels step by step was performed. Bins were merged and respectively split based on similar weight of evidence (WOE) values and stop via an information value (IV) based criteria. The figure below demonstrated the top five predictors ranked by information value during binning.

```{r, include = TRUE, align = 'center', echo=FALSE}
# WOE binning 
# Confirming target variable is numeric and 0 or 1
train$Match_Status <- 
  as.numeric(train$Match_Status) - 1

table(train$Match_Status)

train <- 
  as.data.frame(train)

# Bin all variables of the data frame (apart from the target variable)
# with default parameter settings
train$USMLE_Step_1_Score <- as.numeric(train$USMLE_Step_1_Score)
train$Count_of_Poster_Presentation <- as.numeric(train$Count_of_Poster_Presentation)
train$Count_of_Oral_Presentation <- as.numeric(train$Count_of_Oral_Presentation)
train$Count_of_Articles_Abstracts <- as.numeric(train$Count_of_Articles_Abstracts)
train$Count_of_Peer_Reviewed_Book_Chapter <- as.numeric(train$Count_of_Peer_Reviewed_Book_Chapter)
train$Count_of_Other_than_Published <- as.numeric(train$Count_of_Other_than_Published)
train$Count_of_Online_Publications <- as.numeric(train$Count_of_Online_Publications)


binning <- 
  woe.binning(train, 
              'Match_Status', 
              c('Age',
                'USMLE_Step_1_Score', 
                "Count_of_Poster_Presentation", 
                "Count_of_Oral_Presentation", 
                "Count_of_Articles_Abstracts", 
                "Count_of_Peer_Reviewed_Book_Chapter", 
                "Count_of_Other_than_Published", 
                "Count_of_Online_Publications"))

woe.binning.plot(binning, "1:5")
woe.binning.plot(binning)
bin.top.five <- binning[1:5,c(1,3)]
pander::pander(bin.top.five, style = "simple", justify = c('left', 'center'))
```
These top five binned variables were used for the training and test set.

```{r bin train}
# Deploy the binning solution to the data frame
# (add all binned variables and corresponding WOE variables)
train.df.with.binned.vars.added <-
  woe.binning.deploy(train, binning, add.woe.or.dum.var='woe')

train.df.binned <-
  train.df.with.binned.vars.added

# Removing char_freq_dollar raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -USMLE_Step_1_Score)

# Removing word_freq_remove raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Age)

# Removing char_freq_exclamation raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Poster_Presentation)

# Removing word_freq_hp raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Articles_Abstracts)

# Removing capital_run_length_average raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Oral_Presentation)

# Removing capital_run_length_average raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Other_than_Published)

# Removing capital_run_length_average raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Peer_Reviewed_Book_Chapter)

# Removing capital_run_length_average raw variable since binned is present
train.df.binned <-
  dplyr::select(train.df.binned, -Count_of_Online_Publications)

str(train.df.binned)
```


```{r stargazer train, include=TRUE, results = "asis"}
stargazer::stargazer(train.df.binned, header=FALSE, 
                     title = "Descriptive Statistics of Binned Training Match_Status Data", 
                     type='latex', 
                     nobs = TRUE, 
                     mean.sd = TRUE, 
                     median = TRUE, 
                     iqr = TRUE, 
                     digits = 1, 
                     font.size = "small", 
                     flip = FALSE)
```

```{r, include = TRUE, align = 'center', echo=FALSE}
# WOE binning
# Confirming target variable is numeric and 0 or 1
test$Match_Status <-
  as.numeric(test$Match_Status) - 1

table(test$Match_Status)

test <- as.data.frame(test)

test$USMLE_Step_1_Score <- as.numeric(test$USMLE_Step_1_Score)
test$Count_of_Poster_Presentation <- as.numeric(test$Count_of_Poster_Presentation)
test$Count_of_Oral_Presentation <- as.numeric(test$Count_of_Oral_Presentation)
test$Count_of_Articles_Abstracts <- as.numeric(test$Count_of_Articles_Abstracts)
test$Count_of_Peer_Reviewed_Book_Chapter <- as.numeric(test$Count_of_Peer_Reviewed_Book_Chapter)
test$Count_of_Other_than_Published <- as.numeric(test$Count_of_Other_than_Published)
test$Count_of_Online_Publications <- as.numeric(test$Count_of_Online_Publications)

# Bin all variables of the data frame (apart from the target variable)
# with default parameter settings
binning <-
  woe.binning(test,
              'Match_Status',
              c(
                'Age',
                'USMLE_Step_1_Score',
                "Count_of_Poster_Presentation",
                "Count_of_Oral_Presentation",
                "Count_of_Articles_Abstracts",
                "Count_of_Peer_Reviewed_Book_Chapter",
                "Count_of_Other_than_Published",
                "Count_of_Online_Publications"))

woe.binning.plot(binning, "1:5")
woe.binning.plot(binning)
bin.top.five <- binning[1:5,c(1,3)]
pander::pander(bin.top.five, style = "simple", justify = c('left', 'center'))
```

```{r bin test}
# Deploy the binning solution to the data frame
# (add all binned variables and corresponding WOE variables)
test <- as.data.frame(test)
# Bin all variables of the data frame (apart from the target variable)

# with default parameter settings
binning.test <-
  woe.binning(test,
              'Match_Status',
              c('Age',
                "USMLE_Step_1_Score",
                "Count_of_Poster_Presentation",
                "Count_of_Oral_Presentation",
                "Count_of_Articles_Abstracts",
                "Count_of_Peer_Reviewed_Book_Chapter",
                "Count_of_Other_than_Published",
                "Count_of_Online_Publications"))

woe.binning.plot(binning.test, "1:5")
#woe.binning.plot(binning)
bin.top.five <-
  binning.test[1:5,c(1,3)]

pander::pander(bin.top.five, style = "simple", justify = c('left', 'center'))

test.df.with.binned.vars.added <-
   woe.binning.deploy(test, binning, min.iv.total=0.5,add.woe.or.dum.var='woe')

test.df.with.binned.vars.added <-
  woe.binning.deploy(test, binning, add.woe.or.dum.var='woe')

test.df.binned <-
  test.df.with.binned.vars.added

# test.df.binned <-
#   test.df.with.binned.vars.added[-18]

# Removing char_freq_dollar raw variable since binned is present
test.df.binned <-
  dplyr::select(test.df.binned, -"USMLE_Step_1_Score")

# Removing word_freq_remove raw variable since binned is present
test.df.binned <-
  dplyr::select(test.df.binned, -Age)

# Removing char_freq_exclamation raw variable since binned is present
test.df.binned <-
  dplyr::select(test.df.binned, -Count_of_Poster_Presentation)

# Removing word_freq_hp raw variable since binned is present
test.df.binned <-
  dplyr::select(test.df.binned, -Count_of_Articles_Abstracts)

# Removing capital_run_length_average raw variable since binned is present
test.df.binned <-
  dplyr::select(test.df.binned, -Count_of_Oral_Presentation)

# Removing capital_run_length_average raw variable since binned is present
test.df.binned <- dplyr::select(test.df.binned, -Count_of_Other_than_Published)

# Removing capital_run_length_average raw variable since binned is present
test.df.binned <- dplyr::select(test.df.binned, -Count_of_Peer_Reviewed_Book_Chapter)

# Removing capital_run_length_average raw variable since binned is present
test.df.binned <- dplyr::select(test.df.binned, -Count_of_Online_Publications)

str(test.df.binned)
```

```{r stargazer test, include=TRUE, results = "asis"}

stargazer::stargazer(test.df.binned, 
                     header=FALSE, 
                     title = "Descriptive Statistics of Binned Training Match_Status Data", 
                     type='html', 
                     nobs = TRUE, 
                     mean.sd = TRUE, 
                     median = TRUE, 
                     iqr = TRUE, 
                     digits = 1, 
                     font.size = "small", 
                     flip = FALSE)
```

Relationships between binned variables and `Match_Status` were explored using mosaic plots to look for interesting bins that aided in discrimination. An example of several binned variables are shown in the plots below.

```{r OneR chunk}
# Fit a model to a single attribute;
model.1 <-
  OneR(Match_Status ~
         Age.binned,
       data=train.df.binned,
       verbose=TRUE);

model.2 <-
  OneR(Match_Status ~
         USMLE_Step_1_Score.binned,
       data=train.df.binned,
       verbose=TRUE);

model.3 <-
  OneR(Match_Status ~
         Count_of_Poster_Presentation.binned,
       data=train.df.binned,
       verbose=TRUE);

model.4 <-
  OneR(Match_Status ~
         Count_of_Articles_Abstracts.binned,
       data=train.df.binned,
       verbose=TRUE);

model.5 <-
  OneR(Match_Status ~
         Count_of_Oral_Presentation.binned,
       data=train.df.binned,
       verbose=TRUE);

model.6 <-
  OneR(Match_Status ~
         Count_of_Other_than_Published.binned,
       data=train.df.binned,
       verbose=TRUE);

model.7 <-
  OneR(Match_Status ~
         Count_of_Peer_Reviewed_Book_Chapter.binned,
       data=train.df.binned,
       verbose=TRUE);

model.8 <-
  OneR(Match_Status ~
         Count_of_Online_Publications.binned,
       data=train.df.binned,
       verbose=TRUE);
```

```{r OneR diagnostic plots, fig.height=4, fig.width=8, align='center', include=TRUE, fig.cap="Figure: Diagnostic plots to visualize classifier accuracy."}
# Commonly used to visualize classifier accuracy;
par(mfrow=c(1,2))
plot(model.1)
plot(model.2)
plot(model.3)
plot(model.4)
plot(model.5)
plot(model.6)
plot(model.7)
plot(model.8)
par(mfrow = c(1,1))
```


A simple decision tree model was used for exploration. The variable importance summary from the simple tree was used to explore important relationships. The variables a, b, c, and d  were the top four variables in importance.

CAN WE USE ONLY FACTORS IN THIS TREE MODEL?
  
```{r rpart EDA}
t.model <-
  rpart(as.factor(Match_Status) ~.,
        data = train.df.binned)

t.model$variable.importance
```

The simple tree was plotted below. The a, b and c variables were near the roots of the tree demonstrating importance. DO ALL THE VARIABLES NEED TO BE BINNED AS FACTORS TO RUN A TREE PLOT?
  
```{r fancyR plot rpart EDA, include=TRUE}
# plot
fancyRpartPlot(t.model)
```


Exploratory random forest was also performed. The variable importance for the random forest model was summarized in the figure below. The variables capital_run_length_longest, capital_run_length_total, char_freq_dollar.binned, word_freq_free and word_freq_your were the top five using accuracy and the Gini index.

```{r RF EDA, cache=TRUE, include=TRUE}
# Random forest for EDA
set.seed(12345)

Y <-
  !is.na(as.factor(train.df.binned$Match_Status))

X <-
  !is.na(train.df.binned[,-10])



#train default model and the most regularized model with same predictive performance
rf.eda = randomForest(X,Y,sampsize=25,ntree=5000,mtry=4,
                      keep.inbag = T,
                      keep.forest = T,
                      importance = TRUE)
varImpPlot(rf.eda, main = "Random Forest EDA Variable Importance", cex = 0.75)
```


## 3) The Model Build

All models were fit using the data labeled train and validated using the data labeled test. 10-fold cross-validation was performed for variable selection and parameter estimation was performed using cross-validation where appropriate.

**(1) Logistic regression using backwards variable selection model**
  
  A logistic regression model using backwards variable selection was fit. The summary of the model coefficients for the final model is presented in Table 3. Table 4 demonstrates the confusion matrix for the in-sample performance of the model and Table 5 demonstrates the confusion matrix? or AUC? for the out-of-sample performance.


#I'M NOT SURE IF I INCLUDED ALL THE RIGHT VARIABLES
```{r Backwards LR model, message=FALSE, warning=FALSE, cache=TRUE}
full.model <- glm(as.factor(Match_Status) ~
                    white_non_white + woe.Age.binned +
                    Gender +  Couples_Match +
                    US_or_Canadian_Applicant + Medical_Education_Interrupted +
                    Alpha_Omega_Alpha +
                    Military_Service_Obligation +
                    Visa_Sponsorship_Needed + Medical_Degree  +
                    woe.USMLE_Step_1_Score.binned + woe.Count_of_Poster_Presentation.binned +
                    woe.Count_of_Articles_Abstracts.binned +
                    woe.Count_of_Oral_Presentation.binned +
                    woe.Count_of_Other_than_Published.binned +
                    woe.Count_of_Peer_Reviewed_Book_Chapter.binned +
                    woe.Count_of_Online_Publications.binned,
                  data = train.df.binned, family = binomial)
backwards = step(full.model) # Backwards selection is the default

backwards_results = backwards$coefficients

```

```{r,Stargazer backwards LR, include=TRUE, results='asis'}
stargazer::stargazer(backwards, 
                     title="Backwards Logistic Regression Model Results", 
                     no.space=TRUE, 
                     header=FALSE, 
                     type='latex',  
                     ci=TRUE, 
                     ci.level=0.95, 
                     single.row=TRUE)

stargazer::stargazer(backwards, title="Backwards Logistic Regression Model Results")
```



```{r train backwards, align= 'center'}
train.backwards <-
  predict(backwards, newdata = train.df.binned, type = "response")

train.backwards <-
  ifelse(train.backwards > 0.50, 1, 0)

lr.accuracy <-
  caret::postResample(pred = train.backwards, obs = as.factor(train.df.binned$Match_Status))

#cat("\n","----- Accuracy of Backwards LR on train set -----","\n")
lr.accuracy[1]
```

```{r conf matrix backwards LR, align= 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of Backwards LR on train set -----","\n")
t <-
  table(as.factor(train.df.binned$Match_Status),train.backwards)

row.totals <-
  apply(t,MAR=1,FUN=sum)

#t/row.totals
dt <-
  t/row.totals


kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Backwards LR on train set") %>% kable_styling(latex_options = "striped")
```


```{r accuracy test backwards LR, align = 'center'}
test.backwards <-
  predict(backwards, newdata = test.df.binned, type = "response")

test.backwards <-
  ifelse(test.backwards > 0.50, 1, 0)

#cat("\n","----- Accuracy of Backwards LR on test set -----","\n")
lr.accuracy.test <-
  caret::postResample(pred = test.backwards, obs = as.factor(test.df.with.binned.vars.added$Match_Status))

lr.accuracy <-
  caret::postResample(pred = train.backwards, obs = as.factor(train.df.with.binned.vars.added$Match_Status))

lr.accuracy.test[1]
```


```{r Conf matrix test, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of Backwards LR on test set -----","\n")
t <- table(as.factor(test.df.with.binned.vars.added$Match_Status),test.backwards)

row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals
kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Backwards LR on test set") %>% kable_styling(latex_options = "striped")
```

The in-sample accuracy was `r lr.accuracy[1]` and the out-of-sample accuracy was `r lr.accuracy.test[1]`.

\pagebreak

**(2) Tree model**
  
  A CART model was fit using the rpart package. The final model is presented in the figure below. The majority of final predictors were derived from the binning process. The variables char_free_exclamation.binned, word_freq_removed.binned, and woe.char_freq_dollar.binned had significant influence in the model. Table 6 demonstrates the confusion matrix for the in-sample performance of the model and Table 7 demonstrates the confusion matrix for the out-of-sample performance.

```{r cart model, cache=TRUE}
#CART

# Timer on
ptm = proc.time()
set.seed(12345)

fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 2)

cart.model <-    train(as.factor(Match_Status) ~ .
                       ,
                       data = train.df.binned,
                       method = 'rpart',
                       trControl = fitControl,
                       tuneLength = 8,
                       preProc = c("center", "scale"),
                       metric = "Accuracy"
)


# Timer off
proc.time() - ptm
cart.model
```

```{r CART plot , align = 'center', include=TRUE}
fancyRpartPlot(cart.model$finalModel, caption = NULL)
```

```{r Train cart accuracy}
train.cart <- predict(cart.model, train.df.binned)
#cat("\n","----- Performance of cart on train set -----","\n")
cart.accuracy <- caret::postResample(pred = train.cart, obs = as.factor(train.df.binned$Match_Status))
cart.accuracy[1]
```

```{r conf matrix train cart, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of CART model on train set -----","\n")
t <- table(as.factor(train.df.binned$Match_Status),train.cart)
row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals

kable(dt, "latex", booktabs = T, caption = "Confusion matrix of CART on train set") %>% kable_styling(latex_options = "striped")
```


```{r cart test accuracy}


#test.cart <- predict(cart.model, test.df.with.binned.vars.added)  #ISSUE HERE?????????????????

# Check to ensure the dfs are matching columns
colnames(train.df.binned)
colnames(test.df.binned)



cat("\n","----- Performance of cart on test set -----","\n")
# cart.accuracy.test <- caret::postResample(pred = test.cart, obs = as.factor(test.df.with.binned.vars.added$Match_Status))
# cart.accuracy.test[1]
```

```{r CART test conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of CART model on test set -----","\n")
#t <- table(as.factor(test.df.binned$Match_Status),test.cart)
row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals
library()
kable(dt, "latex", booktabs = T, caption = "Confusion matrix of CART on test set") %>% kable_styling(latex_options = "striped")

##For the CART model, the in-sample accuracy was `r cart.accuracy[1]` and out-of-sample accuracy was `r cart.accuracy.test[1]`.

```



\pagebreak

**(3) a Support Vector Machine model**
  
  The support vector machine model was fit. Cross validation identified a cost C = 1 using a linear kernel and a sigma =  ???  using ??? support vectors.

```{r SVM model, cache=TRUE}
#SVM

# Timer on
ptm = proc.time()
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 2)
svm.model <-    train(as.factor(Match_Status) ~ .
                      ,
                      data = train.df.binned,
                      method = 'svmLinear',
                      trControl = fitControl,
                      tuneLength = 8,
                      metric = "Accuracy", scale = FALSE #added scale = FALSE to make it work
)
# Timer off
proc.time() - ptm; rm(ptm)
svm.model
```

```{r medians of values}
# Select numeric columns
data.numcols <- train.df.binned[, sapply(train.df.binned, is.numeric)]
# Using apply
all.medians <- apply(data.numcols, 2, median)
# Using colMeans
all.means <- colMeans(data.numcols)
```


```{r SVM model one, eval=FALSE, include=FALSE}
# Fit the SVM using C value of 1

train.df.binned <- train.df.binned %>%
  mutate(Match_Status = as.factor(Match_Status))


# train.df.binned <- train.df.binned %>% 
#   mutate(Match_Status = as.numeric(Match_Status)-1)
svm.model.one <- svm(Match_Status ~ .,
                     data=train.df.binned)

###?????????????????????????????????????????????????
plot(x = svm.model.one,
     woe.Age.binned ~ woe.USMLE_Step_1_Score.binned,
     data=train.df.binned)


```


```{r Plot SVM, eval=FALSE, include=TRUE}
# Plot SVM model
plot(svm.model.one, train.df.binned, woe.Age.binned ~ woe.USMLE_Step_1_Score.binned,
     svSymbol = 1, dataSymbol = 2, symbolPalette = rainbow(4),
     color.palette = terrain.colors)
```


```{r SVM train accuracy, align = 'center'}
train.svm <- predict(svm.model, train.df.binned)
cat("\n","----- Performance of svm on train set -----","\n")
svm.accuracy <- caret::postResample(pred = train.svm, obs = as.factor(train.df.binned$Match_Status))
svm.accuracy[1]
```

```{r SVM train conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of SVM model on train set -----","\n")
t <- table(as.factor(train.df.binned$Match_Status),train.svm)
row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals

kable(dt, "latex", booktabs = T, caption = "Confusion matrix of SVM model on train set") %>% kable_styling(latex_options = "striped")
```


```{r SVM test accuracy, align = 'center'}
#####commented out because this does not work

# test.svm <- predict(svm.model, test.df.binned)
# 
# colnames(test.df.binned)
# 
# 
# cat("\n","----- Performance of svm on test set -----","\n")
# svm.accuracy.test <- caret::postResample(pred = test.svm, obs = as.factor(test.df.binned$Match_Status))
# svm.accuracy.test[1]
```

```{r SVM test conf matrix, align = 'center', include=TRUE}
#####commented out because this does not work

# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of SVM model on test set -----","\n")
#t <- table(as.factor(test.df.binned$Match_Status),test.svm)
#row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals

kable(dt, "latex", booktabs = T, caption = "Confusion matrix of SVM model on test set") %>% kable_styling(latex_options = "striped")

#For the SVM model, the in-sample accuracy was `r svm.accuracy[1]` and out-of-sample accuracy was `r svm.accuracy.test[1]`. The in-sample confusion matrix for the SVM model is shown in Table 8 and the out-of-sample confusion matrix for the SVM model is shown in Table 9.
```




**(4) Random Forest model**
  
  A random forest model was fit to the training data. Cross-validation selected the a final value used for mtry = 12 based on optimizing accuracy. The variable importance plot for the random forest model is demonstrated below. Important predictors were similar between the CART model and the RF model. The predictors char_freq_exclamation.binned, woe.char_freq_exclamation.binned, aand woe.char_freq_dollar.binned were top three for variable importance.

```{r RF model, cache=TRUE}
#RF

# Timer on
ptm = proc.time()
set.seed(12345)
fitControl <- trainControl(method = "cv",
                           number = 10)
rf.model <- train(as.factor(Match_Status) ~ .,
                  data = train.df.binned,
                  method = 'rf',
                  trControl = fitControl,
                  # tuneLength = 8,
                  metric = "Accuracy")
# Timer off
proc.time() - ptm
rf.model
```

```{r var imp plot rf model, align = 'center', include=TRUE}

plot(varImp(rf.model), top = 10)
```


```{r RF train accuracy, align = 'center'}
train.rf <- predict(rf.model, train.df.binned)
cat("\n","----- Performance of rf on train set -----","\n")
rf.accuracy <- caret::postResample(pred = train.rf, obs = as.factor(train.df.binned$Match_Status))
rf.accuracy[1]
```

```{r RF train conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of RF model on train set -----","\n")
#t <- table(as.factor(train.df.binned$Match_Status),train.rf)
#row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
#dt <- t/row.totals

#kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Random Forest model on train set") %>% kable_styling(latex_options = "striped")
```


```{r RF test accuracy, align = 'center'}
#########?????????Not working ??????????/
# test.rf <- predict(rf.model, test.df.binned)
# cat("\n","----- Performance of rf on test set -----","\n")
# rf.accuracy.test <- caret::postResample(pred = test.rf, obs = as.factor(test.df.binned$Match_Status))
# rf.accuracy.test[1]
```

```{r RF test conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of RF model on test set -----","\n")
#t <- table(as.factor(test.df.binned$Match_Status),test.rf)
#row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
#dt <- t/row.totals

#kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Random Forest model on test set") %>% kable_styling(latex_options = "striped")

#For the random forest model, the in-sample accuracy was `r rf.accuracy[1]` and out-of-sample accuracy was `r rf.accuracy.test[1]`. The in-sample confusion matrix for the RF model is shown in Table 10 and the out-of-sample confusion matrix for the RF model is shown in Table 11.


```


\pagebreak


## 4) Naïve Bayes with WOE Binning model

Finally, a Naïve Bayes model was fit. Similar to the previous models, the top 5 WOE binned variables were also included in this model. Cross-validation demonstrated that the tuning parameter 'laplace' was held constant at a value of 0 and tuning parameter 'adjust' was held constant at a value of 1.

```{r NB model, cache=TRUE}
#Naive Bayes with WOE Binning

# Timer on
ptm = proc.time()
set.seed(12345)
fitControl <- trainControl(method = "repeatedcv",
                           number = 10,
                           repeats = 2)
nbwoe <-    train(as.factor(Match_Status) ~ .
                  ,
                  data = train.df.binned,
                  method = 'naive_bayes',
                  trControl = fitControl,
                  tuneLength = 8,
                  metric = "Accuracy"
)
# Timer off
proc.time() - ptm
nbwoe
```

```{r NB model accuracy, align = 'center'}
train.nbwoe <- predict(nbwoe, train.df.binned)
cat("\n","----- Performance of nbwoe on train set -----","\n")
nb.accuracy <- postResample(pred = train.nbwoe, obs = as.factor(train.df.binned$Match_Status))
nb.accuracy[1]
```

```{r NB model conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of Naive Bayes model on train set -----","\n")
t <- table(as.factor(train.df.binned$Match_Status),train.nbwoe)
row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
dt <- t/row.totals

kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Naive Bayes model on train set") %>% kable_styling(latex_options = "striped")
```


```{r NB test accuracy, align = 'center'}
###  Not working, commented out  ???????????? #########################

# test.nbwoe <- predict(nbwoe, test.df.binned)
# cat("\n","----- Performance of nbwoe on test set -----","\n")
# nb.accuracy.test <- postResample(pred = test.nbwoe, obs = as.factor(test.df.binned$Match_Status))
# nb.accuracy.test[1]
```

```{r NB test conf matrix, align = 'center', include=TRUE}
# Scale the confusion matrix to accuracy rates by normalizing
# by the row totals; row totals are used because true values are rows and predicted values are columns.
#cat("\n","----- Confusion matrix of Naive Bayes model on test set -----","\n")
#t <- table(as.factor(test.df.binned$Match_Status),test.nbwoe)
#row.totals <- apply(t,MAR=1,FUN=sum)
#t/row.totals
#dt <- t/row.totals

#kable(dt, "latex", booktabs = T, caption = "Confusion matrix of Naive Bayes model on test set") %>% kable_styling(latex_options = "striped")

#For the naive Bayes model, the in-sample accuracy was `r nb.accuracy[1]` and out-of-sample accuracy was `r nb.accuracy.test[1]`. The in-sample confusion matrix for the naive Bayes model is shown in Table 12 and the out-of-sample confusion matrix for the naive Bayes model is shown in Table 13.


```


\pagebreak


# Model Comparison
```{r model comparison, align = 'center', include=TRUE}
#Table 14 summarizes the overall in-sample and out-of-sample accuracy of each model. The best performing models (highest accuracy) was the random forest model with a test set accuracy of `r rf.accuracy.test[1]`. The Logistic regression model using backwards elimination was second with a test set accuracy of `r lr.accuracy.test[1]`. The Naive Bayes model did not perform as well as the other models. In summary, if accuracy is the most important aspect of the model and interpretion is not a priority then the best model was the random forest model. If interpretability of the model is paramount, then the logistic regression model is recommended.

# Training set performance summary

###???????Commented OUT #########
##x <- postResample(pred = train.backwards, obs = as.factor(train.df$Match_Status))
##a <- postResample(pred = train.cart, obs = as.factor(train.df.binned$Match_Status))
##c <- postResample(pred = train.svm, obs = as.factor(train.df.binned$Match_Status))
#e <- postResample(pred = train.rf, obs = as.factor(train.df.binned$Match_Status))

##g commented out
##g <- postResample(pred = train.nbwoe, obs = as.factor(train.df.binned$Match_Status))
# Test set performance summary
#xt <- postResample(pred = test.backwards, obs = as.factor(test.df$Match_Status))
#at <- postResample(pred = test.cart, obs = as.factor(test.df.binned$Match_Status))

##ct commented out
##ct <- postResample(pred = test.svm, obs = as.factor(test.df.binned$Match_Status))
#et  <- postResample(pred = test.rf, obs = as.factor(test.df.binned$Match_Status))
#gt <- postResample(pred = test.nbwoe, obs = as.factor(test.df.binned$Match_Status))
#matrix <- matrix(data = c(x[1], a[1], c[1], e[1], g[1], xt[1], at[1], ct[1], et[1], gt[1]), nrow = 5, ncol = 2, byrow = FALSE)
#colnames(matrix) <- c("Training Set Accuracy", "Test Set Accuracy")
#rownames(matrix) <- c("LR Backwards Elimination", "CART", "Support Vector Machine", "Random Forest", "Naive Bayes")
#df <- round(matrix, 3)
#kable(df, "latex", booktabs = T, caption = "In-sample and out-of-sample accuracy of all models") %>% kable_styling(latex_options = "striped")
```
\pagebreak

```{r NOMOGRAM,  fig.width=7, fig.asp=1}
###NOMOGRAM
#fun.at - Demarcations on the function axis: "Matching into obgyn"
#lp=FALSE so we don't have the logistic progression
d <- rms::datadist(test)
options(datadist = "d")
nomo.from.lrm.with.lasso.variables <- rms::nomogram(train.lrm.with.lasso.variables,
                                                    #lp.at = seq(-3,4,by=0.5),
                                                    fun = plogis,
                                                    fun.at = c(0.001, 0.01, 0.05, seq(0.2, 0.8, by = 0.2), 0.95, 0.99, 0.999),
                                                    funlabel = "Chance of Matching in OBGYN",
                                                    lp =FALSE,
                                                    #conf.int = c(0.1,0.7),
                                                    abbrev = F,
                                                    minlength = 9)
nomogramEx::nomogramEx(nomo=nomo.from.lrm.with.lasso.variables ,np=1,digit=2)  #Gives the polynomial formula
nomo_final <- plot(nomo.from.lrm.with.lasso.variables, lplabel="Linear Predictor",
                   cex.sub = 0.8, cex.axis=0.4, cex.main=1, cex.lab=0.3, ps=10, xfrac=.7,
                   #fun.side=c(3,3,1,1,3,1,3,1,1,1,1,1,3),
                   #col.conf=c('red','green'),
                   #conf.space=c(0.1,0.5),
                   label.every=1,
                   col.grid = gray(c(0.8, 0.95)),
                   which="Match_Status")
#print(nomo.from.lrm.with.lasso.variables)
```


#Annotation:  Manuscript Figure 1:  The first row called points assigned to each variable's measurement from rows 2-12, which are variables included in predictive model.  Assigned points for all variables are then summed and total can be located on line 13 (total points).  Once total points are located, draw a vertical line down to the bottom line to obtain the predicted probability of matching.  For non-linear variables (count of oral presentations, etc.) values should be erad from left to right.


#Calibration of the model based on the test data.
The ticks across the x-axis represent the frequency distribution (may be called a rug plot) of the predicted probabilities. This is a way to see where there is sparsity in your predictions and where there is a relative abundance of predictions in a given area of predicted probabilities.

The "Apparent" line is essentially the in-sample calibration.

The "Ideal" line represents perfect prediction as the predicted probabilities equal the observed probabilities.

The "Bias Corrected" line is derived via a resampling procedure to help add "uncertainty" to the calibration plot to get an idea of how this might perform "out-of-sample" and adjusts for "optimistic" (better than actual) calibration that is really an artifact of fitting a model to the data at hand. This is the line we want to look at to get an idea about generalization (until we have new data to try the model on).

When either of the two lines is above the "Ideal" line, this tells us the model underpredicts in that range of predicted probabilities. When either line is below the "Ideal" line, the model overpredicts in that range of predicted probabilities.

Applying to your specific plot, it appears most of the predicted probabilities are in the higher end (per rug plot). The model overall appears to be reasonably well calibrated based on the Bias-Corrected line closely following the Ideal line; there is some underprediction at lower predicted probabilities because the Bias-Corrected line is above the Ideal line around < 0.3 predicted probability.

The mean absolute error is the "average" absolute difference (disregard a positive or negative error) between predicted probability and actual probability. Ideally, we want this to be small (0 would be perfect indicating no error). This seems small in this plot, but may be situation dependent on how small is small.

```{r calibration,  fig.width=7, fig.asp=1}
calib <- rms::calibrate(train.lrm.with.lasso.variables, method = "boot", boot=1000, data = test, rule = "aic", estimates = TRUE)  #Plot test data set
plot(calib, legend = TRUE, subtitles = TRUE, xlab = "Predicted probability according to model", ylab = "Observation Proportion of Matching")
```

\pagebreak

## References
Lorrie Faith Cranor and Brian A. LaMacchia. Match_Status! Communications of the ACM. Vol. 41, No. 8 (Aug. 1998), Pages 74-83. Definitive version: http://www.acm.org/pubs/citations/journals/cacm/1998-41-8/p74-cranor/
  
  \pagebreak
# Appendix, Exploratory Data Analysis
The funModeling package will first give distributions for numerical data and finally creates cross-plots.  This also saves the output of the distributions to the results folder.

# Appendix, Supplemental Table:  Descriptive analysis of all variables considered in the training set along with their association to matching.

```{r, echo=FALSE, warning=FALSE, message=FALSE, include=TRUE, results="asis"}
table1_all_data <- arsenal::tableby(Match_Status ~
                                      white_non_white +
                                      Age +
                                      Gender +
                                      Couples_Match +
                                      #Expected_Visa_Status_Dichotomized +
                                      US_or_Canadian_Applicant +
                                      #Medical_School_Type +
                                      Medical_Education_Interrupted +
                                      #Misdemeanor_Conviction +
                                      Alpha_Omega_Alpha +
                                      #Gold_Humanism_Honor_Society +
                                      Military_Service_Obligation +
                                      USMLE_Step_1_Score +
                                      Military_Service_Obligation +
                                      Count_of_Poster_Presentation +
                                      Count_of_Oral_Presentation +
                                      # Count_of_Peer_Reviewed_Articles_Abstracts +
                                      Count_of_Peer_Reviewed_Book_Chapter +
                                      # Count_of_Peer_Reviewed_Other_than_Published +
                                      Count_of_Online_Publications +
                                      Visa_Sponsorship_Needed +
                                      Medical_Degree,
                                    data=train, control = arsenal::tableby.control(test = TRUE, total = TRUE, digits = 1L, digits.p = 2L, digits.count = 0L, numeric.simplify = F, numeric.stats = c("median", "q1q3"), cat.stats = c("Nmiss","countpct"), stats.labels = list(Nmiss = "N Missing", Nmiss2 ="N Missing", meansd = "Mean (SD)", medianrange = "Median (Range)", median ="Median", medianq1q3 = "Median (Q1, Q3)", q1q3 = "Q1, Q3", iqr = "IQR",range = "Range", countpct = "Count (Pct)", Nevents = "Events", medSurv ="Median Survival", medTime = "Median Follow-Up")))
summary(table1_all_data, text=T, title='Supplemental Table: Descriptive analysis of all variables considered in the training set along with their association to matching', pfootnote=TRUE)
```

Medical student #1 is a `r all_data$Age[1]`year old `r all_data$white_non_white[1]` `r all_data$Gender[1]` who is a US Senior medical graduate


Abstract
===========================================================================================
Background:  A model that predicts a medical student's chances of matching into an obstetrics and gynecology residency may facilitate improved counseling and fewer unmatched medical students.

Objective:  We sought to construct and validate a model that predicts a medical student's chance of matching into obstetrics and gynecology residency.

Study Design:  In all, `r nrow(all_data)` medical students applied to a residency in Obstetrics and Gynecology at the University of Colorado from 2015 to 2018 were analyzed.  The data set was splint into a model training cohort of `r nrow(train)` who applied in 2015, 2016, and 2017 and a separate validation cohort of `r nrow(test)` in 2018.  In all, `r ncol(all_data)` candidate predictors for matching were collected.  Multiple logistic models were fit onto the training choort to predict matching.  Variables were removed using least absolute shrinkage and selection operator reduction to find the best parsimonious model.  Model discrimination was measured using the concordance index.  The model was internally valideated using 1,000 bootstrapped samples and temporarly validated by testing the model's performance in the validation cohort.  Calibration curves were plotted to inform educators about the accuracy of predicted probabilities.

Results:  The match rate in the training cohort was `r round((prop.table(table(train$Match_Status))[[2]]*100),1)`% (I need help getting 95% CI).  The model had excellent discrimination and calibration during internal validation (bias-corrected concordance index,`r round((lrm.with.lasso.variables$stats[6]),2)`) and maintained accuracy during temportal validation using the separate validation cohort (concordance index,`r round((train.lrm.with.lasso.variables$stats[6]),2)`).

Introduction
===========================================================================================
To add in.  

Materials and Methods
===========================================================================================
This was an institutional review board exempt retrospective cohort analysis of medical students who applied to Obstetrics and Gynecology (OBGYN) residency from 2015 to 2018.  Guidelines for transparent reporting of a multivariable prediction model for individual outcomes were used in this study.(https://www.equator-network.org/reporting-guidelines/tripod-statement/).  Eligible students were identified if they applied to OBGYN residency during the study period.  The outcome of the model was defined as matching or not matching into residency for the specific application year.  Individual predictors of successfully* matching were compiled from a literature review, expert opinion, and judgment then collected from the Electronic Residency Application Service materials.

Once the data set was complete it was divided into a model training and test set.  *When an external validation data set is unavailable to test a new model but an existing modeling data set is sufficiently large, as in this case, it is recommended to split by time and develop the model using data from one period and evaluate its performance from data from a future period.  We arbitrarily chose to divide the cohort into a training set of 2015 to 2017 data and a training set of 2018 data.

In all, ?? candidate risk factors were considered for fitting on the training data set (supplmental table).  Variable selection was done using a peenalized logistic regression called least absolute shrinkage and selection operator (LASSO).  The LASSO model is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  We elected to use LASSO to choose which covariates to include over stepwise selection because the latter only improves prediction accuracy in certain cases, such as when only a few covariates have a strong relationship with the outcome.

The logistic model’s discriminative ability was measured by the area under the curve (AUC) for the receiver operating characteristic curve based on the sensitivity and specificity of the model.  An AUC value closer to 1 indicates a better prediction of the outcome and an AUC value of 0.5 indicates that the model predicts no better than chance. The AUC is also a representation of the concordance index and measures the model’s ability to generate a higher predicted probability of a successful match* occurring in a medical student who has a ????.   For example, if we have a pair of medical students, in which one medical student matches and the other does not, the concordance index measures the model’s ability to assign a higher risk of not matching to the medical student who successfully matches. All concordance indices and receiver operating characteristic curves were internally validated using a 1,000 bootstrap resample to correct for bias and overfitting within the model. The bootstrapping method of validation has been shown to be superior to other approaches to estimate internal validity. Calibration curves were also plotted to depict the relationship between the model’s predicted outcomes against the cohort’s observed outcome, where a perfectly calibrated model follows a 45° line.

After the best model was selected and internally validated, the model was compared with the best currently available method of estimating risk, that is, an expert medical educator’s predictions. To perform these comparisons, a subset of 50 participants was randomly selected for comparing the probability of matching between the model and the panel of experts. These ?? participants were used to compare predictions of the models with experts’ predictions and not as a true independent validation subset. The model was rebuilt using the remaining participants in the data set excluding the 50 randomly selected participants. The candidate risk factors of these 50 participants were given to 20 “expert” medical educators with representation from each of the *** for review resulting in 1,000 expert predictions and 50 model predictions for each outcome. All medical educators were considered to be experienced in counseling medical students regarding OBGYN matching. Each of the 20 experts were asked to consider each medical student’s data from all ??? variables among the 50 randomly selected students and provide their best estimated outcome by answering the following question: “Out of 100 medical students with these exact characteristics, estimate the number of medical students who would not matching into OBGYN during the 2019 application year.” Individual medical educators’ predictions were not averaged to yield a single value because incorporating each medical educator’s predictions substantially increased statistical power. The model’s predictions were compared with the experts’ predictions, which included all risk factors, to determine which was most accurate. The difference in accuracy was determined by using a bootstrap method from their respective receiver operating characteristic curves. All analyses were performed using R 3.5.

Results
===========================================================================================
A total of `r nrow(all_data)` applied to obstetrics and gynecology residency at the University of Colorado from 2015 to 2018.  The overall mean rate of matching in the training cohort was `r table(train$Match_Status)[[2]]` of `r nrow(train)` was (`r round((prop.table(table(train$Match_Status))[[2]]*100),1)`%).
The unadjusted comparison of the `r ncol(all_data)` candidate predictors in the training cohort are presented in Supplemental Table 1.  To identify predictors from the candidates we employed least absolute shrinkage and selection operator (LASSO).  Regularisation techniques change how the model is fit by adding a penalty for every additional parameter you have in the model.
`r length(variables)` variables were included within the final model.  Applicants from the United States or Canada, high USMLE Step 1 scores, female gender, White race, no visa sponsorship needed, membership in Alpha Omega Alpha, no interruption of medical training, couples matching, and allopathic medical training increased the chances of matching into OBGYN.  In contrast, more oral presentations, increasing age, a higher number of peer-reviewed online publications, an increased number of authored book chapters, and a higher count of poster presentations all decreased the probability of matching into OBGYN (table 2).  The nomogram illustrates the strength of association of the predictors to the outcome as well as the nonlinear associations between age, count of Oral Presentations, count of peer−reviewed book chapters and the chances of matching (Figure 1).

Discussion
===========================================================================================
Needed


References
===============================================================================

# Appendix, DynNom Model for Shiny Upload
```{r DynNom logistic regression model}
DynNom
library(dplR)
DynNom.model.lrm  <-
  rms::lrm(Match_Status ~
             rms::rcs(Age, 5) +
             Gender +
             US_or_Canadian_Applicant +
             rms::rcs(USMLE_Step_1_Score, 4) +
             white_non_white +
             Alpha_Omega_Alpha +
             Count_of_Oral_Presentation +
             Count_of_Peer_Reviewed_Book_Chapter +
             Couples_Match +
             Medical_Degree +
             Military_Service_Obligation +
             Visa_Sponsorship_Needed,
                   data = test,
           x = TRUE,
           y= TRUE)

# DynNom::DynNom.lrm(model = DynNom.model.lrm, data = test,  clevel = 0.95)
```


```{r generalized logistic regression}
DynNom.model.glm  <-
DynNom.model.glm <- glm(Match_Status ~
                          Age +
                          Alpha_Omega_Alpha +
                          Count_of_Oral_Presentation +
                          Count_of_Peer_Reviewed_Book_Chapter +
                          Couples_Match +
                          Gender +
                          Medical_Degree +
                          Military_Service_Obligation +
                          US_or_Canadian_Applicant +
                          USMLE_Step_1_Score +
                          Visa_Sponsorship_Needed +
                          white_non_white,
                        family = "binomial",  #Removed the relax cubic splines for age and USMLE step 1
                        data = test, x = TRUE, y= TRUE)
DynNom::DNbuilder(model = DynNom.model.glm, data = train)
```



```{r rsconnect for uploading shinyapps}
# put credentials here to upload file to shinyapps.io
# Tutorial: https://docs.rstudio.com/shinyapps.io/getting-started.html#CreateAccount
# rsconnect::setAccountInfo(name='georgkropat',
#                           token='F4ECA8D490AB629FB79416DF01048F3F',
#                           secret='TriCqLW2h1XRf12m7bPMZbIh7ZZgSjVg8nHUVbTZ')


# 
# rsconnect::deployApp('DynNomapp/')
```

```{r sessionInfo}
sessionInfo()
```