---
title: "04-GLM_and_CART_models: A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: hide
    dev: svg
    df_print: paged
    theme: journal
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---
I removed the `USMLE_Step_1_Score` given the impending pass/fail in 2020.  I also removed `USMLE_Step_2_CK_Score` because most students do not have this test completed by the time that they apply.  

```{r, fig.align="center"}
knitr::include_graphics("images/pass.jpg")
```

Read in the original `all_data`, `train`, and `test`
```{r, include = FALSE, echo=FALSE, warning=FALSE}
rm(list = ls())

library(magrittr)
all_data <- readr::read_csv("~/Dropbox/Nomogram/nomogram/all_data_output_of_split_correlation.csv") %>%
  dplyr::select(-USMLE_Step_1_Score,
                -USMLE_Step_2_CK_Score)
readr::write_csv(all_data, "~/Dropbox/Nomogram/nomogram/all_data_no_usmle.csv")

train <- readr::read_csv("~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv") %>%
  dplyr::select("ACLS", "Age", "Alpha_Omega_Alpha",  "BLS", "Citizenship",  "Count_of_Peer_Reviewed_Journal_Articles_Abstracts", "Count_of_Poster_Presentation",
                "Count_of_Peer_Reviewed_Book_Chapter",
                "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published","Couples_Match", "Gender", "Medical_Education_or_Training_Interrupted", "Medical_Degree", "NIH_dollars", "PALS",  "Type_of_medical_school", "US_or_Canadian_Applicant", #"USMLE_Step_1_Score",
                #"USMLE_Step_2_CK_Score", 
                "Visa_Sponsorship_Needed", "white_non_white", "Match_Status")

test <- readr::read_csv("~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv") %>%
 dplyr::select("ACLS", "Age", "Alpha_Omega_Alpha",  "BLS", "Citizenship",  "Count_of_Peer_Reviewed_Journal_Articles_Abstracts", "Count_of_Poster_Presentation",
                "Count_of_Peer_Reviewed_Book_Chapter",
                "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published","Couples_Match", "Gender", "Medical_Education_or_Training_Interrupted", "Medical_Degree", "NIH_dollars", "PALS",  "Type_of_medical_school", "US_or_Canadian_Applicant", #"USMLE_Step_1_Score",
                #"USMLE_Step_2_CK_Score", 
                "Visa_Sponsorship_Needed", "white_non_white", "Match_Status")

train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "0", Match = "1"))

test <- test %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "0", Match = "1"))

train$Match_Status <- as.numeric(train$Match_Status)

# https://rpubs.com/TELOVE/project1_demo_2019-432
#See section 11
```

# Final selection of variables from Boruta - I removed the variables that Boruta threw out.  
```{r, include=TRUE, warning=FALSE}
boruta_drivers <- c(
                                                                     "Age",
                                                       "Alpha_Omega_Alpha",
                                                                    "ACLS",
                                                                     "BLS",
                                                             "Citizenship",
                           #"Count_of_Non_Peer_Reviewed_Online_Publication",
                                      #        "Count_of_Oral_Presentation",
                                     #            "Count_of_Other_Articles",
                                    "Count_of_Peer_Reviewed_Book_Chapter",
                      "Count_of_Peer_Reviewed_Journal_Articles_Abstracts",
 "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published",
  #                            "Count_of_Peer_Reviewed_Online_Publication",
                                           "Count_of_Poster_Presentation",
   #                                       "Count_of_Scientific_Monograph",
                                                          "Couples_Match",
                                                                 "Gender",
                                                         "Medical_Degree",
                              "Medical_Education_or_Training_Interrupted",
                                              #"Medical_Licensure_Problem",
                                            #"Military_Service_Obligation",
                                                 #"Misdemeanor_Conviction",
                                                            "NIH_dollars",
                                                                   "PALS",
                                                        #"Sigma_Sigma_Phi",
                                                 "Type_of_medical_school",
                                               "US_or_Canadian_Applicant",
                                                     #"USMLE_Step_1_Score",
                                                 # "USMLE_Step_2_CK_Score",
                                                "Visa_Sponsorship_Needed",
                                                        "white_non_white")

#Writing the name of dependent variable seperately
dependentVar <- "Match_Status"

#Bringing all the column names under one term
final_formula <- paste(dependentVar, "~", paste(boruta_drivers, collapse = " + "))
final_formula
```

**(1) Generalized Linear Models**

# 1) lrm Model 1 (m.1):  Draw a Nomogram Representing a Regression Fit
I inserted the variables from Boruta (`final_formula`) into the `rms::lrm` model.  

One problem is that the `rms:nomogram` function only accepts an `lrm` model.  The package `hdnom` only plots survival analysis for nomograms.  


```{r, include = FALSE, , fig.width = 15, fig.height = 15, dpi = 1200}
#Using rmarkdown with images and outputing at different sizes and resolutions
#https://yihui.org/knitr/options
#https://www.zevross.com/blog/2017/06/19/tips-and-tricks-for-working-with-images-and-figures-in-r-markdown-documents/

library(rms)
```


```{r, include = TRUE, , fig.width = 15, fig.height = 15, dpi = 1200}
dd <- datadist(test)
options(datadist = "dd")
set.seed(123456)
m.1 <- 
  rms::lrm(formula = Match_Status ~ Age + Alpha_Omega_Alpha + ACLS + BLS + Citizenship + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Peer_Reviewed_Journal_Articles_Abstracts + Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published + Count_of_Poster_Presentation + Couples_Match + Gender + Medical_Degree + Medical_Education_or_Training_Interrupted + NIH_dollars + PALS + US_or_Canadian_Applicant + Visa_Sponsorship_Needed + white_non_white, 
      data = train, #Does this need to be centered and scaled?
      penalty = 0.0,
      #method = "lrm.fit",
      tol = 1e-15,
      x = T, 
      y = T,
      maxit=10000)

#Only thing that fixed this was decreasing the singularity criteria set by tol, tol = 1e-15 works
#https://grokbase.com/t/r/r-help/08a9xjk7rk/r-singular-information-matrix-in-lrm-fit
#https://stats.stackexchange.com/questions/70699/qualitative-variable-coding-in-regression-leads-to-singularities#70700

summary(m.1)
```

There is a singular information matrix for the variable: `Type_of_medical_school=Osteopathic School ` so that variable was removed.  

```{r NOMOGRAM1,  fig.width=7, fig.asp=1}
#https://www.kaggle.com/pjmcintyre/titanic-first-kernel#final-checks
#https://www.ncbi.nlm.nih.gov/pmc/articles/PMC5451623/
# #https://rpubs.com/TELOVE/project1_demo_2019-432
tm_print_save <- function (filename) {
  print("Function Sanity Check: Saving TIFF of what is in the viewer")
  grDevices::dev.print(tiff, (here::here("results", filename)), compression = "lzw",width=2000, height=2000, bg="transparent", res = 200, units = "px" )
  dev.off()
}

nomo <- rms::nomogram(m.1,  #requires an lrm model from rms package
                #lp.at = seq(-3,4,by=0.5),
                fun = plogis,
                fun.at = c(0.001, 0.01, 0.05, seq(0.2, 0.8, by = 0.2), 0.95, 0.99, 0.999),
                funlabel = "Chance of Matching in OBGYN",
                lp =FALSE,
                #conf.int = c(0.1,0.7),
                abbrev = F,
                minlength = 9)

plot(nomo)
```


```{r NOMOGRAM2,  fig.width=15, fig.asp=1.5}
plot(anova(m.1))
```

Variables of interest:  
Age + Alpha_Omega_Alpha + 
BLS + Citizenship + 
Couples_Match + Gender + Medical_Degree + Medical_Education_or_Training_Interrupted + NIH_dollars + PALS + US_or_Canadian_Applicant + 
white_non_white

Variables that are not significant:  
Count_of_Peer_Reviewed_Book_Chapter + 
Count_of_Peer_Reviewed_Journal_Articles_Abstracts + Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published + Count_of_Poster_Presentation + 
ACLS 
Visa_Sponsorship_Needed 

```{r NOMOGRAM3,  fig.width=7, fig.asp=1}
#https://rpubs.com/TELOVE/project1_demo_2019-432
plot(nomo, lplabel="Linear Predictor",
       cex.sub = 0.3, cex.axis=0.4, cex.main=1, cex.lab=0.2, ps=10, xfrac=1,
       col.conf=c('red','green'),
       conf.space=c(0.1,0.5),
       label.every=1,
       col.grid = gray(c(0.8, 0.95)),
       which="Match_Status")

tm_print_save("train_lrm_with_lrm_m_1.tiff")
```
Annotation:  Manuscript Figure 1:  The first row called points assigned to each variable's measurement from rows 2-19, which are variables included in predictive model.  Assigned points for all variables are then summed and total can be located on line 19 (total points).  Once total points are located, draw a vertical line down to the bottom line to obtain the predicted probability of matching.  For non-linear variables (count of oral presentations, etc.) values should be erad from left to right.


```{r}
#https://rpubs.com/TELOVE/project1_demo_2019-432
plot(rms::calibrate(fit = m.1,
                    method=c("boot","crossvalidation",".632","randomization"),
                    B=1000, #Upper limit of resamples
                    bw=FALSE, 
                    rule=c("aic","p"),
                    add=FALSE,
                    legend = TRUE))


tm_print_save("train_m_1_plot.tiff")
```
`m.1` appears to overpredict a bit, especially at the lower and the higher predicted values.

## Validation of `m.1`, with backwards elimination
What if we try a validation step, with a stepwise backwards elimination approach?
```{r, include=FALSE}
set.seed(432)
a <- rms::validate(m.1, bw = TRUE, B = 1000)

#https://rpubs.com/TELOVE/project1_demo_2019-432
```
Backwards regression showed these variables to be the final factors:
[1] Age                                       Alpha_Omega_Alpha
[3] BLS                                       Citizenship
[5] Gender                                    Medical_Degree
[7] Medical_Education_or_Training_Interrupted US_or_Canadian_Applicant
[9] white_non_white                          

Note that the cross-validated Somers’ Dxy statistic is `r a[[1,5]]`. To calculate the cross-validated area under the ROC curve, called C, we use

C=0.5+Dxy2=0.5+.44542=0.5+0.2227=0.722

which is a cross-validated estimate of the value of the C statistic we might reasonably expect using this model. Compare this to the nominal C = 0.85 (`r m.1[[3]]`) shown in the initial `m.1` summary


# 2) Model 2 (m.2): a smaller model with promising predictors from backwards regression

We’ll start with a model motivated by the Spearman ρ2 plot developed above, and repeated below.
```{r}
train <- readr::read_csv("~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
 "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")


test <- readr::read_csv("~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
 "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")
```


```{r}
dd <- datadist(train)
options(datadist = "dd")
m.2 <- lrm(Match_Status ~ Age + Alpha_Omega_Alpha + 
BLS +  Citizenship + 
#Couples_Match + 
  Gender + 
  Medical_Degree + 
  Medical_Education_or_Training_Interrupted + #NIH_dollars + PALS +
  US_or_Canadian_Applicant + 
white_non_white,
           data = train, x = T, y = T)
m.2
```

Feedback from LRM shows no singular information matrix in variables.  

Continue with variables:
Age + Alpha_Omega_Alpha + BLS + Citizenship + Gender + Medical_Degree + Medical_Education_or_Training_Interrupted + US_or_Canadian_Applicant + white_non_white


##ANOVA for model 2
```{r}
anova(m.2)

plot(anova(m.2))
tm_print_save("model_2_fewers_variables_plot.tiff")
```

##Effects summary for m.2
```{r}
summary(m.2)
plot(summary(m.2))

tm_print_save("model_2_fewers_odds_ratio_plot.tiff")
```

The calibration plot does not look much different.  
```{r}
plot(rms::calibrate(m.2))
tm_print_save("model_2_fewers_calibration_plot.tiff")
```


##Plotting Model 2 Predictions
Here is a plot of the predictions across the two levels of `Citizenship` for patients with Age ranging from 26 to 50

```{r}
ggplot(rms::Predict(m.2, Age = 26:50, 
               white_non_white = c("White", "Not_White"),
               fun=plogis)) +
    theme_bw() +
    labs(x = "Age",
         y = "Pr(matching into OBGYN residency)",
         title = "Model 2 Predictions")
```

## Plotting the ROC curve for Model 2
Again, we need to fit Model 3 in `glm`, rather than `rms.`
```{r, include=TRUE}
#https://rpubs.com/TELOVE/project1_demo_2019-432
train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = 0L, Match = 1L))


test <- test %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = 0L, Match = 1L))

model.2 <- stats::glm(Match_Status ~ Age + Alpha_Omega_Alpha + BLS + Citizenship + Gender + Medical_Degree + Medical_Education_or_Training_Interrupted + US_or_Canadian_Applicant + white_non_white,
                      data = train, 
               family = binomial(link = logit))
```

```{r}
readr::write_csv(train, "~/Dropbox/Nomogram/nomogram/data/train_from_04.csv")
```


```{r, include = FALSE}
# requires ROCR package
train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, "0" = "Did.not.match",  "1" = "Match"))

test <- test %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, "0" = "Did.not.match",  "1" = "Match"))
```

## Make Predictions
Most prediction methods have an argument `newdata` specifying the first place to look for explanatory variables to be used for prediction. 

SHOULD THIS BE DONE WITH TEST, TRAIN, or ALL_DATA?????
```{r}
library(ROCR)
prob <- stats::predict(object = model.2, 
                       newdata = test, 
                       type="response")
```

###Function to create prediction objects in ROCR
Every classifier evaluation using ROCR starts with creating a prediction object. This function is used to transform the input data (which can be in vector, matrix, data frame, or list form) into a standardized format.
```{r}
sapply(c(is.vector, is.matrix, is.list, is.data.frame), do.call, list(prob)) #confirm that predictions and labels are a vector, matrix, list, or data frame" for both predictions and labels arguments

sapply(c(is.vector, is.matrix, is.list, is.data.frame), do.call, list(train$Match_Status))

pred <- ROCR::prediction(predictions = prob,   #Contains the predictions
                         labels = test$Match_Status)  #contains the truth
# rest of this doesn't need much adjustment except for titles
```

###Function to create performance objects in ROCR
Allows you to measure performance of the model like: accuracy, true positive rate, false positive rate, auc.  
```{r}
perf <- ROCR::performance(pred, measure = "tpr", x.measure = "fpr")
auc <- ROCR::performance(pred, measure="auc")
auc <- round(auc@y.values[[1]],3)
roc.data <- data.frame(fpr=unlist(perf@x.values),
                       tpr=unlist(perf@y.values),
                       model="GLM")
ggplot2::ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
    geom_ribbon(alpha=0.2) +
    geom_line(aes(y=tpr)) +
    labs(title = paste0("ROC Curve w/ AUC=", auc),
         subtitle = "Model 2 for test data")
```


### Nomogram using lrm: lrm package train.glm.with.boruta.variables
```{r DynNom logistic regression model}
# rsconnect::setAccountInfo(name='mufflyt',
# 			  token='D8846CA8B32E6A5EAEA94BFD02EEEA39',
# 			  secret='dIXWOv+ud/z6dTPN2xOF9M4BKJtWKROc2cOsZS4U')
# rsconnect::accountInfo(name = "mufflyt")
#                        
# DynNom::DynNom(model = train.glm.with.boruta.variables, 
#                clevel = 0.95,
#                 m.summary = c("raw", "formatted"),
#                 DNtitle = "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency",
#                 DNxlab = "Probability of Matching into OBGYN Residency")
```


```{r generalized logistic regression, include=FALSE, echo=TRUE, cache=FALSE}
#### Turn this on to start the shiny model.  
# DynNom::DNbuilder(model = DynNom.model.glm, data = test, clevel = 0.95, DNtitle = "DRAFT: A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency")
```

# 3) Model: Caret for Fitting Generalized Linear Models

It is important to know what type of modeling a particular model supports. This can be done using the caret function getModelInfo.  This tells us that gbm supports both regression and classification. As this is a binary classification, we need to force glm into using the classification mode. We do this by changing the outcome variable to a factor. 

```{r}
knitr::include_graphics("images/caret.png")
```

```{r, echo=TRUE, warning=FALSE, include=TRUE}
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI
caret::getModelInfo()$glm$type
train$Match_Status <- as.factor(train$Match_Status)
```

##Model: Caret, trainControl
Caret offers many tuning functions to help you get as much as possible out of your models; the trainControl function allows you to control the resampling of your data. This will split the training data set internally and do it’s own train/test runs to figure out the best settings for your model. In this case, we’re going to cross-validate the data 10 times, therefore training it 10 times on different portions of the data before settling on the best tuning parameters. 
```{r}
#https://topepo.github.io/caret/model-training-and-tuning.html#an-example
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI
set.seed(123)
#train$Match_Status <- as.factor(train$Match_Status)

train
class(train)
class(train$Match_Status)
head((train$Match_Status), 10)

train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "Did_Not_Match", Match = "Match"))

test <- test %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "Did_Not_Match", Match = "Match"))

head((train$Match_Status), 10)

fitControl <- caret::trainControl(## 10-fold CV
                           method = "repeatedcv",
                           number = 10,
                           ## repeated ten times
                           repeats = 10, 
                           classProbs = TRUE, 
                           savePredictions=TRUE) 

# train$ACLS <- as.factor(train$ACLS)
# train$Alpha_Omega_Alpha <- as.factor(train$Alpha_Omega_Alpha)
# train$BLS <- as.factor(train$BLS)
# train$Citizenship <- as.factor(train$Citizenship)
# train$Couples_Match <- as.factor(train$Couples_Match)
# train$Gender <- as.factor(train$Gender)
# train$Medical_Education_or_Training_Interrupted <- as.factor(train$Medical_Education_or_Training_Interrupted)
# train$PALS <- as.factor(train$PALS)
# train$Type_of_medical_school <- as.factor(train$Type_of_medical_school)
# train$US_or_Canadian_Applicant <- as.factor(train$US_or_Canadian_Applicant)
# train$Visa_Sponsorship_Needed <- as.factor(train$Visa_Sponsorship_Needed)
# train$white_non_white <- as.factor(train$white_non_white)
```

## Model: Caret, Teaching the Model
This is the heart of our modeling adventure, time to teach our model how to recognize applicants who match. Because this is a classification model, we’re requesting that our metrics use ROC instead of the default RMSE:
```{r}
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI
cv_model3 <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  metric = "Accuracy",
  preProc = c("center", "scale", "nzv"), 
  trControl = fitControl
)

cv_model3
summary(cv_model3)
```

## Model: Caret, Evaluate the cv_model3
There are two types of evaluation we can do here, raw or prob. Raw gives you a class prediction, in our case Match and Did.Not.Match, while prob gives you the probability on how sure the model is about it’s choice. I always use prob, as I like to be in control of the threshold and also like to use AUC score which requires probabilities, not classes. 

We now call the predict function and pass it our trained model and our testing data. Let’s start by looking at class predictions and using the caret postResample function to get an accuracy score:

```{r}
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI
predictions <- stats::predict(object=cv_model3, #Model we want to use to predict
                              newdata = test, 
                              type='raw')
head(predictions)

a<- (postResample(pred=predictions, obs=as.factor(test$Match_Status)))
print(a)
#  Accuracy     Kappa 
# 0.8631415 0.5426890 
```
The accuracy tells us that our model is correct `r a[[1]]` of the time - not bad

```{r}
knitr::include_graphics("images/predictions.png")
```

Now let’s look at probabilities:
```{r}
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI
# probabilites 
library(pROC)
predictions <- stats::predict(object=cv_model3, 
                              newdata = test, 
                              type='prob')
head(predictions)
```

## Model: Caret, The Receiver Operating Characteristic (ROC) 
The Receiver Operating Characteristic (ROC) curve is plotted below for false positive rate (FPR) in the x-axis vs. the true positive rate (TPR) in the y-axis. It shows the detection of true positive while avoiding the false positive. This is the same as measuring the unspecificity (1 - specificity) in x-axis, against the sensitivity in y-axis. This ROC curve in particular shows that its very closed to the perfect classifier meaning that its better at identifying the positive values. 
```{r}
knitr::include_graphics("images/ROC_curve.png")
```

To get the AUC score, you need to pass the yes column to the roc function (each row adds up to 1 but we’re interested in the yes, the survivors):
```{r}
#https://www.viralml.com/video-content.html?v=-nai4NBx5zI

auc <- pROC::roc(ifelse(test$Match_Status=="Match",1,0), predictions[[2]])  #Prediction vs. actual
print(auc$auc)
pROC::plot.roc(auc, add = FALSE, identity = TRUE, reuse.auc=TRUE,
axes=TRUE, legacy.axes=FALSE)
```
The AUC is telling us that our model has a `r auc$auc [[1]]` AUC score (remember that an AUC ranges between 0.5 and 1, where 0.5 is random and 1 is perfect).

# Model: Base General logistic regression model Use Model to predict match for Test Data
Shift Gears.

We need to build the model using the generic or base packages in R because that is what will allow us to create a nomogram.  This will also serve as a confirmation of the cv_model3 from the caret package.  

```{r}
knitr::include_graphics("images/R logo.png")
```

## Model: Base, GLM Model fit with base glm package
```{r, echo=TRUE, warning=FALSE, include=TRUE}
train.glm.with.boruta.variables  <- 
  stats::glm(Match_Status ~ .,
      data = train, 
      family = "binomial"(link=logit),
      x = TRUE, y= TRUE,#Must have this line to use with DynNom later
      weights = NULL)  

summary(train.glm.with.boruta.variables)  #check the model
#exp(train.glm.with.boruta.variables$coefficients)
confint(train.glm.with.boruta.variables)

#Zhang book page 75
Predprob <- predict(train.glm.with.boruta.variables, type = "response")
```

```{r, echo=TRUE, warning=FALSE, include=FALSE}
#library(Deducer)
plot(train.glm.with.boruta.variables, main = "ROC curve", colorize = T)
```

## Model: Base, Use Model to predict `test$Match_Status` for `test` Data
```{r}
#Use Model to predict match Status for Test Data
pred_class <- rms::Predict(train.glm.with.boruta.variables)

prob <- 
  predict(train.glm.with.boruta.variables, newdata = test, type="response")
#dim(test)
plot(prob, main = "Probability of Matching based on \n train.glm.with.lasso.variables model")

pred <- 
  ROCR::prediction(prob, test$Match_Status)  #removed na.omit
```

## Model: Base, Accuracy of Model
```{r}
knitr::include_graphics("images/accuracy.png")
```

```{r}
#https://github.com/mufflyt/Diabetes-Patient-Readdmission-Prediction/blob/master/Patient%20Readmission%20Risk%20Prediction.r

# Accuracy of Model
GLM_Pred <- ifelse(prob > 0.5, 1, 0)
pred2 <- ifelse(GLM_Pred == 1, "Match", "No.Match")
table(pred2)["Match"]

#Number True and Number False
#table(pred2)["TRUE"]

#Accuracy Equation
GLM_Accuracy <- ifelse(GLM_Pred == test$Match_Status,"Match", "No.Match")

#table for accuracy
table(GLM_Accuracy)
```


## Model: Base, Receiver-Operator Curve ggplot with nice controls
```{r, include = TRUE}
# rest of this doesn't need much adjustment except for titles
perf <-
  ROCR::performance(pred, measure = "tpr", x.measure = "fpr")

auc <- 
  ROCR::performance(pred, measure="auc")

auc <- 
  round(auc@y.values[[1]],3)

roc.data <- 
  data.frame(fpr=unlist(perf@x.values),
             tpr=unlist(perf@y.values),
             model="GLM")
auc
roc_plot <- ggplot2::ggplot(data = roc.data,
                            mapping = ggplot2::aes(x=fpr, ymin=0, ymax=tpr)) +
  ggplot2::geom_ribbon(alpha=0.2) +
  ggplot2::geom_line(aes(y=tpr)) +
  ggplot2::labs(title = paste0("ROC Curve with area under the curve = ", auc),
       subtitle = "Model: train.glm.with.boruta.variables")
roc_plot

#tm_ggsave(roc_plot, "roc_plot.tiff")
```


# Comparing logistic regression model with penalized regression model. 

## Caret Logistic Regression model vs. Penalized Model
```{r, include=FALSE}
# train logistic regression model
set.seed(123)
glm_mod <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10)
  )
```

## Penalized Model
The standard linear model performs poorly in a situation, where you have a large multivariate data set containing a number of variables superior to the number of samples.  A better alternative is the penalized regression allowing to create a linear regression model that is penalized, for having too many variables in the model, by adding a constraint in the equation (James et al. 2014,P. Bruce and Bruce (2017)). This is also known as shrinkage or regularization methods.

The consequence of imposing this penalty, is to reduce (i.e. shrink) the coefficient values towards zero. This allows the less contributive variables to have a coefficient close to zero or equal zero.

```{r}
knitr::include_graphics("images/regularized logistic regression model.png")
```

```{r, include=FALSE}
# train regularized logistic regression model
set.seed(123)
penalized_mod <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glmnet",
  family = "binomial",
  preProc = c("zv", "center", "scale"),
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 10
  )
```

## Evaluate the logistic regression models side-by-side
```{r}
# extract out of sample performance measures
# summary(resamples(list(
#   caret_logistic_model = cv_model3,
#   logistic_model = glm_mod, 
#   penalized_model = penalized_mod
#   #pls = cv_model_pls
#   )))$statistics$Accuracy
```
The output shows minimal improvement with the penalized model (median accuracy = 0.809) compared to the logistic model (median accuracy = 0.803). 

\pagebreak

**Regression Tree model**

# 4)  Model: A CART model was fit using the rpart package. 

##Backwards regression showed these variables to be the final factors:
[1] Age                                       Alpha_Omega_Alpha
[3] BLS                                       Citizenship
[5] Gender                                    Medical_Degree
[7] Medical_Education_or_Training_Interrupted US_or_Canadian_Applicant
[9] white_non_white    

```{r, include=FALSE}
train <- readr::read_csv("~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
 "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")


test <- readr::read_csv("~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
 "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")
```

```{r, include=FALSE, warning=FALSE}
boruta_and_backwards_drivers <- c(
                                                                     "Age",
                                                       "Alpha_Omega_Alpha",
                                                                    #"ACLS",
                                                                     "BLS",
                                                             "Citizenship",
                           #"Count_of_Non_Peer_Reviewed_Online_Publication",
                                      #        "Count_of_Oral_Presentation",
                                     #            "Count_of_Other_Articles",
                                    #"Count_of_Peer_Reviewed_Book_Chapter",
                      #"Count_of_Peer_Reviewed_Journal_Articles_Abstracts",
# "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published",
  #                            "Count_of_Peer_Reviewed_Online_Publication",
                                           #"Count_of_Poster_Presentation",
   #                                       "Count_of_Scientific_Monograph",
                                                          #"Couples_Match",
                                                                 "Gender",
                                                         "Medical_Degree",
                              "Medical_Education_or_Training_Interrupted",
                                              #"Medical_Licensure_Problem",
                                            #"Military_Service_Obligation",
                                                 #"Misdemeanor_Conviction",
                                                            #"NIH_dollars",
                                                                   #"PALS",
                                                        #"Sigma_Sigma_Phi",
                                                 #"Type_of_medical_school",
                                               "US_or_Canadian_Applicant",
                                                     #"USMLE_Step_1_Score",
                                                 # "USMLE_Step_2_CK_Score",
                                                "Visa_Sponsorship_Needed",
                                                        "white_non_white")

#Writing the name of dependent variable seperately
dependentVar <- "Match_Status"

#Bringing all the column names under one term
final_final_formula <- paste(dependentVar, "~", paste(boruta_and_backwards_drivers, collapse = " + "))
final_final_formula
```

```{r}
knitr::include_graphics("images/Decision Tree.gif")
```

## Trees can handle both factors and continuous variables 
Use the divide-and-conquer (aka recursive partinioning e.g. rpart) technique to create two homogenous groups.  Leaf nodes are at the bottom and denote final decisions on matching success. Random Forest is a more powerful algorithm over just a single tree. However, the Decision Tree classification preserve the interpretability which the random forest algorithm lacks.  A simple decision tree model was used for exploration. The Decision Tree does not require feature scaling but can overfit data by fitting the noise instead of the whole data.  It is very important to evaluate decision trees on data it has not seen before.

##Here we intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.
```{r rpart EDA}
#https://bradleyboehmke.github.io/HOML/DT.html
#https://www.datacamp.com/community/tutorials/decision-trees-R 
#https://exploratory.io/note/kei/Visualizing-a-decision-tree-using-R-packages-7185899605672132
t.model <-
  rpart::rpart(as.factor(Match_Status) ~.,
        data = train,     
        method = "class", #Builds a classification tree
        control = rpart::rpart.control(cp = 0, maxdepth = 6), #Decreasing this CP control made the tree much more complicated
        minsplit = 20) 

t.model$variable.importance
```

The biggest factor is the type of US_or_Canadian_applicant=international.  The Decision Tree rules run from the top to the bottom. If you look at the top of the tree, the 1st rule is “International applicant”. “Yes” goes to the left hand side, and “No” goes to the right. 

```{r fancyR plot rpart EDA, include=TRUE}
# Tree visualization
tm_rpart_plot = function(df){
  print("Function Sanity Check: Plot Decision Trees using package rpart with Leaves")
  tm_rpart_plot_leaves <- rpart.plot::rpart.plot(df, yesno = 2, type = 5, extra = +100, fallen.leaves = TRUE, varlen = 0, faclen = 0, roundint = TRUE, clip.facs = TRUE, shadow.col = "gray", main = "Tree Model of Medical Students Matching into OBGYN Residency\n(Matched or Unmatched)", box.palette = c("red", "green"))  
  tm_fancy_rpart <- rattle::fancyRpartPlot(df)
  return(c(tm_rpart_plot_leaves, tm_fancy_rpart))
}

tm_rpart_plot(t.model)

tm_print_save <- function (filename) {
  print("Function Sanity Check: Saving TIFF of what is in the viewer")
  grDevices::dev.print(tiff, (here::here("results", filename)), compression = "lzw",width=2000, height=2000, bg="transparent", res = 200, units = "px" )
  dev.off()
}
tm_print_save(filename = "tm_rpart_plot.tiff")
```

Using the model to make Match_Status predictions on the test dataframe
```{r}
#using the model to make Match_Status predictions on the test dataframe
solution_tree_probability <- predict(t.model, newdata = test, type="prob")  #predict(model made with training data, test data)

solution_tree_class <- predict(t.model, newdata = test, type="class")  #predict(model made with training data, test data)

table(solution_tree_class, test$Match_Status)
# Compute the accuracy on the test dataset
round(mean(solution_tree_class == test$Match_Status), digits = 3)
```
The accuracy or correct classification rate of the decision tree called `t.model` is `r round(mean(solution_tree_probability == test$Match_Status), digits = 3)`.  

Look at pruning the tree branches to determine the optimal complexity vs. accuracy point for stopping branch growth.  Pruning complexity parameter (cp) plot illustrating the relative cross validation error (y-axis) for various cp values (lower x-axis). Smaller cp values lead to larger trees (upper x-axis). Using the 1-SE rule, a tree size of 10-12 provides optimal cross validation results.
```{r}
rpart::plotcp(t.model)
tm_print_save(filename = "plotcp.tiff")

rpart::printcp(t.model)
```
##The figure shows the pruning complexity parameter plot for a fully grown tree. Significant reduction in the cross validation error is achieved with tree sizes 88-100 and then the cross validation error levels off with minimal or no additional improvements.

```{r}
train <- readr::read_csv("~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
"Couples_Match",  "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted", "NIH_dollars", "PALS",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")

test <- readr::read_csv("~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv") %>%
  dplyr::select("Age", "Alpha_Omega_Alpha", "BLS", "Citizenship",  
"Couples_Match",  "Gender",  "Medical_Degree",  "Medical_Education_or_Training_Interrupted", "NIH_dollars", "PALS",  "US_or_Canadian_Applicant",  
"white_non_white", "Match_Status")

train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "0", Match = "1"))

test <- test %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "0", Match = "1"))

t.model2 <- rpart::rpart(
    formula = final_formula,
    data    = train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

rpart::plotcp(t.model2)
abline(v = 34, lty = "dashed")  #Not sure whhat number is ideal.  
#https://bradleyboehmke.github.io/HOML/DT.html
tm_print_save("plotcp_for_t_model2.tiff")
```

##Graph the complexity parameter
```{r}
# rpart cross validation results
t.model2$cptable

# caret cross validation results
t.model3 <- caret::train(
  Match_Status ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

t.model3.plot <- ggplot(t.model3)
t.model3.plot

tm_ggsave <- function (object, filename, ...){  #make sure the file name has quotation marks around it.  
  print("Function Sanity Check: Saving a ggplot image as a TIFF")
  ggplot2::ggsave(here::here("results", filename), object, device = "tiff", width = 10, height = 7, dpi = 200)
}

tm_ggsave(t.model3.plot, "t_model3_plot.tiff")
```

## Prune the decision tree down based on complexity graph above
Based on the complexity plot, prune the tree to a complexity of 0.005 using the prune() function with the tree and the complexity parameter.  Now Compute the accuracy of the pruned tree. 

Even though the Decision Tree algorithm is relatively simple and basic, it still provides a great explainability. Without any background in statistics or machine learning, you can understand what are the conditions to have the target outcomes by just looking at the tree diagram.
```{r}
pruned.t.model <- rpart::prune(t.model, cp = 0.005) #Set the cp where the complexity to accuraty rate plateaued.  
tm_rpart_plot(pruned.t.model)
tm_print_save("pruned_t_model.tiff")

summary(pruned.t.model)
```
 
## Validate the results on a test dataset
```{r}
# pred = stats::predict(pruned.t.model, test, type = "class")
# 
# head((pred), 10)
# class(pred)
# colnames(pred)
# 
# 
# caret::confusionMatrix(as.factor(test$Match_Status), pred)
# (pruned.t.model)
```


Abstract
===========================================================================================
Background:  A model that predicts a medical student's chances of matching into an obstetrics and gynecology residency may facilitate improved counseling and fewer unmatched medical students.

Objective:  We sought to construct and validate a model that predicts a medical student's chance of matching into obstetrics and gynecology residency.

Study Design:  In all, `r nrow(all_data)` medical students applied to a residency in Obstetrics and Gynecology at the University of Colorado from 2015 to 2018 were analyzed.  The data set was splint into a model training cohort of `r nrow(train)` who applied in 2015 to 2018 and a separate random validation cohort of `r nrow(test)`.    In all, `r ncol(all_data) - 1` candidate predictors for matching were collected.  Multiple logistic models were fit onto the training cohort to predict matching.  Variables were removed using random forest iterative models to find the best parsimonious model.  Model discrimination was measured using the concordance index.  The model was internally valideated using 1,000 bootstrapped samples and temporarly validated by testing the model's performance in the validation cohort.  Calibration curves were plotted to inform educators about the accuracy of predicted probabilities.

Results:  The match rate in the training cohort was `r round((prop.table(table(train$Match_Status))[[2]]*100),1)`% (I need help getting 95% CI).  The model had excellent discrimination and calibration during internal validation (bias-corrected concordance index,`r round((m.2$stats[6]),2)`) and maintained accuracy during temportal validation using the separate validation cohort (concordance index,`r round((m.2$stats[6]),2)`).

Introduction
===========================================================================================
To add in.  

Materials and Methods
===========================================================================================
This was an institutional review board exempt retrospective cohort analysis of medical students who applied to Obstetrics and Gynecology (OBGYN) residency from 2015 to 2018.  Guidelines for transparent reporting of a multivariable prediction model for individual outcomes were used in this study.(https://www.equator-network.org/reporting-guidelines/tripod-statement/).  Eligible participants were identified if they applied to OBGYN residency during the study period of 2015 to 2018.  The outcome of the model was defined as matching or not matching into residency for the specific application year.  Individual predictors of successfully* matching were compiled from a literature review, expert opinion, and judgment then collected from the Electronic Residency Application Service materials.

Once the data set was complete it was divided into a model training and test set.  *When an external validation data set is unavailable to test a new model but an existing modeling data set is sufficiently large, as in this case, it is recommended to split by time and develop the model using data from one period and evaluate its performance from data from a future period.  There was a large case imbalance with matching rates split by year therefore we split the data randomly.

In all, 18 candidate risk factors were considered for fitting on the training data set (supplmental table).  Variable selection was done using a penalized logistic regression called least absolute shrinkage and selection operator (LASSO).  The LASSO model is a regression analysis method that performs both variable selection and regularization in order to enhance the prediction accuracy and interpretability of the statistical model it produces.  We elected to use LASSO to choose which covariates to include over stepwise selection because the latter only improves prediction accuracy in certain cases, such as when only a few covariates have a strong relationship with the outcome.

The logistic model’s discriminative ability was measured by the area under the curve (AUC) for the receiver operating characteristic curve based on the sensitivity and specificity of the model.  An AUC value closer to 1 indicates a better prediction of the outcome and an AUC value of 0.5 indicates that the model predicts no better than chance. The AUC is also a representation of the concordance index and measures the model’s ability to generate a higher predicted probability of a successful match* occurring in a medical student who has a ????.   For example, if we have a pair of medical students, in which one medical student matches and the other does not, the concordance index measures the model’s ability to assign a higher risk of not matching to the medical student who successfully matches. All concordance indices and receiver operating characteristic curves were internally validated using a 1,000 bootstrap resample to correct for bias and overfitting within the model. The bootstrapping method of validation has been shown to be superior to other approaches to estimate internal validity. Calibration curves were also plotted to depict the relationship between the model’s predicted outcomes against the cohort’s observed outcome, where a perfectly calibrated model follows a 45° line.

After the best model was selected and internally validated, the model was compared with the best currently available method of estimating risk, that is, an expert medical educator’s predictions. To perform these comparisons, a subset of 50 participants was randomly selected for comparing the probability of matching between the model and the panel of experts. These ?? participants were used to compare predictions of the models with experts’ predictions and not as a true independent validation subset. The model was rebuilt using the remaining participants in the data set excluding the 50 randomly selected participants. The candidate risk factors of these 50 participants were given to 20 “expert” medical educators with representation from each of the *** for review resulting in 1,000 expert predictions and 50 model predictions for each outcome. All medical educators were considered to be experienced in counseling medical students regarding OBGYN matching. Each of the 20 experts were asked to consider each medical student’s data from all ??? variables among the 50 randomly selected students and provide their best estimated outcome by answering the following question: “Out of 100 medical students with these exact characteristics, estimate the number of medical students who would not matching into OBGYN during the 2019 application year.” Individual medical educators’ predictions were not averaged to yield a single value because incorporating each medical educator’s predictions substantially increased statistical power. The model’s predictions were compared with the experts’ predictions, which included all risk factors, to determine which was most accurate. The difference in accuracy was determined by using a bootstrap method from their respective receiver operating characteristic curves. Change.  

All analyses were performed using R 3.6.1.  However, considering the large amount of data, processing the data was taking too long and that was compromising the efficiency of the study. So, the platform h2o.ai was used, integrated with R, to explore the grid mode and parallel processing models. The grid allowed to combine different parameters to build different models. The parallel processing allowed those models to be built at the same time. This efficiency increase made it possible to build more models, with better tuning parameters.

Results
===========================================================================================
A total of `r nrow(all_data)` applied to obstetrics and gynecology residency at the University of Colorado from 2015 to 2018.  The overall mean rate of matching in the training cohort was `r table(train$Match_Status)[[2]]` of `r nrow(train)` was (`r round((prop.table(table(train$Match_Status))[[2]]*100),1)`%).
The unadjusted comparison of the `r ncol(all_data)` candidate predictors in the training cohort are presented in Supplemental Table 1.  To identify predictors from the candidates we employed least absolute shrinkage and selection operator (LASSO).  Regularisation techniques change how the model is fit by adding a penalty for every additional parameter you have in the model.
`r length(final_formula[2])` variables were included within the final model.  Applicants from the United States or Canada, high USMLE Step 1 scores, female gender, White race, no visa sponsorship needed, membership in Alpha Omega Alpha, no interruption of medical training, couples matching, and allopathic medical training increased the chances of matching into OBGYN.  In contrast, more oral presentations, increasing age, a higher number of peer-reviewed online publications, an increased number of authored book chapters, and a higher count of poster presentations all decreased the probability of matching into OBGYN (table 2).  The nomogram illustrates the strength of association of the predictors to the outcome as well as the nonlinear associations between age, count of Oral Presentations, count of peer−reviewed book chapters and the chances of matching (Figure 1).

Discussion
===========================================================================================
Needed


References
===============================================================================
