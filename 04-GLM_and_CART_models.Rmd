---
title: "A Model to Predict Chances of Matching into Obstetrics and Gynecology Residency"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: show
    dev: svg
    df_print: paged
    theme: journal
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

# Read in the original `all_data`, `train`, and `test`
```{r}
library(magrittr)
all_data <- readr::read_csv("~/Dropbox/Nomogram/nomogram/all_data_output_of_split_correlation.csv")
train <- readr::read_csv("~/Dropbox/Nomogram/nomogram/train_output_of_split_correlation.csv")
test <- readr::read_csv("~/Dropbox/Nomogram/nomogram/test_output_of_split_correlation.csv")

train <- train %>%
dplyr::mutate(Match_Status = dplyr::recode(Match_Status, `Did Not Match` = "0", Match = "1"))

train$Match_Status <- as.numeric(train$Match_Status)
class(train$Match_Status)
```

# Review the variables
"Malpractice_Cases_Pending", Removed because this is due to the fact that all people had 0 cases/zero variance
```{r}
#https://github.com/manojmukkamala/DiabeticPatientsReadmission/blob/master/Project_V4.R
#Just writing the names of all the variables under one term to avoid re-writing when we apply model multiple times

#"Malpractice_Cases_Pending", Removed because this is due to the fact that all people had 0 cases/zero variance

drivers <- c("ACLS",
                                                                     "Age",
                                                       "Alpha_Omega_Alpha",
                                                         # "Applicant_Name",
                                                                     "BLS",
                                                             "Citizenship",
                           "Count_of_Non_Peer_Reviewed_Online_Publication",
                                              "Count_of_Oral_Presentation",
                                                 "Count_of_Other_Articles",
                                    "Count_of_Peer_Reviewed_Book_Chapter",
                      "Count_of_Peer_Reviewed_Journal_Articles_Abstracts",
 "Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published",
                              "Count_of_Peer_Reviewed_Online_Publication",
                                           "Count_of_Poster_Presentation",
                                          "Count_of_Scientific_Monograph",
                                                          "Couples_Match",
                                                                 "Gender",
                                                         "Medical_Degree",
                              "Medical_Education_or_Training_Interrupted",
                                              "Medical_Licensure_Problem",
                                            "Military_Service_Obligation",
                                                 "Misdemeanor_Conviction",
                                                            "NIH_dollars",
                                                                   "PALS",
                                                        "Sigma_Sigma_Phi",
                                                 "Type_of_medical_school",
                                               "US_or_Canadian_Applicant",
                                                     "USMLE_Step_1_Score",
                                                  "USMLE_Step_2_CK_Score",
                                                "Visa_Sponsorship_Needed",
                                                        "white_non_white")

#Writing the name of dependent variable seperately
dependentVar <- "Match_Status"

#Bringing all the column names under one term
formula <- paste(dependentVar, "~", paste(drivers, collapse = " + "))
formula
```


# 1) Model: General logistic regression model Use Model to predict match for Test Data
Shift Gears: Test Accuracy of Model on Training Data, Use glmnet model on 207 and 2018 `test` data.   Run the 2017, 2018 data through the train model.  

Build both a glm (train.glm.with.lasso.variables) and a lrm model (train.lrm.with.lasso.variables) here with the same predictor variables.  

```{r, echo=TRUE, warning=FALSE}
#It would be nice to create a character variable that we drop features from.  
train.glm.with.lasso.variables  <- 
  stats::glm(Match_Status ~ white_non_white + Alpha_Omega_Alpha + ACLS + BLS + PALS + Citizenship + Gold_Humanism_Honor_Society + Misdemeanor_Conviction + Sigma_Sigma_Phi + Age + Medical_Education_or_Training_Interrupted + Gender + Couples_Match + US_or_Canadian_Applicant + Alpha_Omega_Alpha + Military_Service_Obligation + USMLE_Step_1_Score + Visa_Sponsorship_Needed + Count_of_Poster_Presentation + Count_of_Oral_Presentation + Count_of_Peer_Reviewed_Book_Chapter + Count_of_Peer_Reviewed_Journal_Articles_Abstracts + Count_of_Peer_Reviewed_Journal_Articles_Abstracts_Other_than_Published + Count_of_Peer_Reviewed_Online_Publication + Count_of_Non_Peer_Reviewed_Online_Publication + Count_of_Scientific_Monograph + Count_of_Other_Articles + Visa_Sponsorship_Needed + Medical_School_Type + Medical_Degree + USMLE_Step_2_CK_Score,
      data = train, 
      family = "binomial"(link=logit))  

summary(train.glm.with.lasso.variables)  #check the model
exp(train.glm.with.lasso.variables$coefficients)
confint(train.glm.with.lasso.variables)

#Zhang book page 75
Predprob <- predict(train.glm.with.lasso.variables, type = "response")
library(Deducer)
plot(train.glm.with.lasso.variables, main = "ROC curve", colorize = T)
```


```{r, echo=TRUE, warning=FALSE}
train.lrm.with.lasso.variables <- 
  rms::lrm(formula = Match_Status ~ 
                                    white_non_white + 
                                    Age +
                                    Gender +
                                    Couples_Match +
                                    US_or_Canadian_Applicant +
                                    Medical_Education_or_Training_Interrupted + 
                                    Alpha_Omega_Alpha + 
                                    USMLE_Step_1_Score + 
                                    Count_of_Oral_Presentation + 
                                    Visa_Sponsorship_Needed,
                                    #Medical_Degree, #Removed due to VIF issues of collinearity
              data = train,
           x=TRUE, y=TRUE)
```

First, we need to fit lrm.with.lasso.variables in GLM, rather than rms, to get the AUC.  There is probably a better way to do this.  Using the test data set.  Also built the same model in lrm.  

The Receiver Operating Characteristic (ROC) curve is plotted below for false positive rate (FPR) in the x-axis vs. the true positive rate (TPR) in the y-axis. It shows the detection of true positive while avoiding the false positive. This is the same as measuring the unspecificity (1 - specificity) in x-axis, against the sensitivity in y-axis. This ROC curve in particular shows that its very closed to the perfect classifier meaning that its better at identifying the positive values. 
Use Model to predict match Status for Test Data
```{r}
#Use Model to predict match Status for Test Data
##For prediction
# predict class
pred_class <- rms::Predict(train.glm.with.lasso.variables)

# create confusion matrix
confusionMatrix(
  data = relevel(pred_class, ref = "Matched"), 
  reference = relevel(train$Match_Status, ref = "Matched")
)

prob <- 
  predict(train.glm.with.lasso.variables, newdata = test, type="response")
dim(test)
plot(prob, main = "Probability of Matching based on \n train.glm.with.lasso.variables model")

pred <- 
  prediction(prob, test$Match_Status)  #removed na.omit
```

```{r}
#https://github.com/mufflyt/Diabetes-Patient-Readdmission-Prediction/blob/master/Patient%20Readmission%20Risk%20Prediction.r

# Accuracy of Model
GLM_Pred <- ifelse(prob > 0.5, 1, 0)
pred2 <- ifelse(GLM_Pred == 1, "Match", "No.Match")
#Number True and Number False
table(pred2)["Match"]

# test$Match_Status1 <- as.numeric(test$Match_Status) - 1
#In GLM_Pred == test$Match_Status1 :
  # longer object length is not a multiple of shorter object length

#Accuracy Equation
# GLM_Accuracy <- ifelse(GLM_Pred == test$Match_Status1,1,0) #Not working

#table for accuracy
#table(GLM_Accuracy)

#Calculating True vs. total
# Accuracy_GLM <- (sum(GLM_Accuracy)/count(test))*100
# Accuracy_GLM

#confusion matrix  #Not working from here
#confusionMatrix(as.factor(GLM_Pred),as.factor(test$Match_Status1), positive = "1")  ##????NOT WORKING

#ROC curve for GLM
library(ROCR)
#pred_glm <- prediction(as.numeric(as.character(prob)),  ###NOT WORKING??? as.numeric(as.character(test$Match_Status)))
#perf_glm <- performance(pred, "tpr", 'fpr')    ###NOT WORKING???
# plot(perf_glm, main = "ROC curve", colorize = T)
# abline(0,1, col='gray60')
# 
# summary(perf_glm)

#auc_ROCR <- performance(pred_glm, measure = "auc") ###NOT WORKING???
# auc_ROCR <- auc_ROCR@y.values[[1]]
# auc_ROCR
```


ROC: ROC ggplot with nice controls
```{r, include = TRUE}
# rest of this doesn't need much adjustment except for titles
perf <-
  performance(pred, measure = "tpr", x.measure = "fpr")

auc <- 
  performance(pred, measure="auc")

auc <- 
  round(auc@y.values[[1]],3)

roc.data <- 
  data.frame(fpr=unlist(perf@x.values),
             tpr=unlist(perf@y.values),
             model="GLM")

roc_plot <- ggplot(roc.data, aes(x=fpr, ymin=0, ymax=tpr)) +
  geom_ribbon(alpha=0.2) +
  geom_line(aes(y=tpr)) +
  labs(title = paste0("ROC Curve with area under the curve = ", auc),
       subtitle = "Model: train.glm.with.lasso.variables")
roc_plot

tm_ggsave(roc_plot, "roc_plot.tiff")
```

ROC: ROC with nice labels on the x and y
```{r, include = TRUE}
pred <- 
  prediction(prob, test$Match_Status)

perf <- 
  performance(pred, measure = "tpr", x.measure = "fpr")

plot(perf)

auc <- 
  performance(pred, measure = "auc")

auc <- 
  auc@y.values[[1]]

auc  
```

ROC: ROC in color
```{r, include = TRUE}
perf <- 
  performance(pred, 'tpr','fpr')

plot(perf, colorize = TRUE, text.adj = c(-0.2,1.7), main="Receiver-Operator Curve for Model A")

#Plots of Sensitivity and Specificity
perf1 <- 
  performance(pred, "sens", "spec")

plot(perf1, colorize = TRUE, text.adj = c(-0.2,1.7), main="Sensitivity and Specificity for Model A")

## precision/recall curve (x-axis: recall, y-axis: precision)
perf2 <- 
  performance(pred, "prec", "rec")

plot(perf2, colorize = TRUE, text.adj = c(-0.2,1.7), main="Precision and Recall for Model A")
```

Exploratory random forest was also performed. The variable importance for the random forest model was summarized in the figure below. 
```{r}
#https://bradleyboehmke.github.io/HOML/logistic-regression.html

model3 <- glm(
  formula = formula,
  family = "binomial", 
  data = train
  )

tidy(model3)
```

```{r}
set.seed(123)
cv_model3 <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "glm",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10)
)
```

```{r}
#Error in resamples.default(list(model3 = cv_model3)) : 
  #at least two train objects are needed

# summary(
#   resamples(
#     list(
#       model3 = cv_model3
#     )
#   )
# )$statistics$Accuracy
```

```{r}
# predict class
pred_class <- stats::predict(cv_model3, train)
pred_class


factor(train$Match_Status, levels= c("Matched", "Did not match"))
levels(train$Match_Status)
class(train$Match_Status)

pred_class
levels(pred_class)

#create confusion matrix
caret::confusionMatrix(
  data = stats::relevel(pred_class, ref = "Matched"),
  reference = stats::relevel(train$Match_Status, ref = "Matched")
)
```

We can perform a partial least squares (PLS) logistic regression to assess if reducing the dimension of our numeric predictors helps to improve accuracy. There are 17 numeric features in our data set so the following code performs a 10-fold cross-validated PLS model while tuning the number of principal components to use from 1–17. The optimal model uses 1 or 5 principal components, which is not reducing the dimension. However, the mean accuracy of 0.876 is no better than the average CV accuracy of cv_model3 (0.876).
```{r}
# Perform 10-fold CV on a PLS model tuning the number of PCs to 
# use as predictors
set.seed(123)
cv_model_pls <- caret::train(
  Match_Status ~ ., 
  data = train, 
  method = "pls",
  family = "binomial",
  trControl = trainControl(method = "cv", number = 10),
  preProcess = c("zv", "center", "scale"),
  tuneLength = 10
)

# Model with lowest AUC
cv_model_pls$bestTune

# Plot cross-validated AUC
cv_model_pls_plot <- ggplot2::ggplot(cv_model_pls) + ggtitle(label = "The 10-fold cross-validation AUC \n obtained using PLS with \n 1–10 principal components.")
cv_model_pls_plot

tm_ggsave(cv_model_pls_plot, "cv_model_pls_plot.tiff")
```
The 10-fold cross-validation AUC obtained using partial least squares with 1–10 principal components.

```{r}
tm_vip(object = cv_model3, title = "CV_model3")
tm_print_save("cv_model3.tiff")
```
Using vip::vip() we can extract our top 10 influential variables. The figures illustrates that 'USMLE_Step_1_Score' is the most influential followed by 'US_or_Canadian_Applicant - International', and 'Age'.

```{r}
# https://github.com/tobiolatunji/Readmission_Prediction/blob/master/diabetes_readmission.R
# pseudo R-squared for logistic regression model
logisticPseudoR2s(train.glm.with.lasso.variables)
```


**(2) Regression Tree model**
# 2)  Model: A CART model was fit using the rpart package. 

Trees can handle both factors and continuous variables and do not need to create dummy variables.   Use the divide-and-conquer (aka recursive partinioning e.g. rpart) technique to create two homogenous groups.  Leaf nodes are at the bottom and denote final decisions on matching success. Random Forest is a more powerful algorithm over just a single tree. However, the Decision Tree classification preserve the interpretability which the random forest algorithm lacks.  A simple decision tree model was used for exploration. The Decision Tree does not require feature scaling but can overfit data by fitting the noise instead of the whole data.  It is very important to evaluate decision trees on data it has not seen before.

Here we intentionally grow a large and complex tree then prune it to be smaller and more efficient later on.
```{r rpart EDA}
#https://bradleyboehmke.github.io/HOML/DT.html
#https://www.datacamp.com/community/tutorials/decision-trees-R 
t.model <-
  rpart(as.factor(Match_Status) ~.,
        data = train,     
        method = "class", #Builds a classification tree
        control = rpart.control(cp = 0, maxdepth = 6), #Decreaseing this CP control made the tree much more complicated
        minsplit = 20) 

t.model$variable.importance
```

```{r fancyR plot rpart EDA, include=TRUE}
# Tree visualization
tm_rpart_plot(t.model)
tm_print_save(filename = "tm_rpart_plot.tiff")
```
Using the model to make Match_Status predictions on the test dataframe
```{r}
#using the model to make Match_Status predictions on the test dataframe
solution_tree_probability <- predict(t.model, newdata = test, type="prob")  #predict(model made with training data, test data)

solution_tree_class <- predict(t.model, newdata = test, type="class")  #predict(model made with training data, test data)

table(solution_tree_class, test$Match_Status)
# Compute the accuracy on the test dataset
round(mean(solution_tree_class == test$Match_Status), digits = 3)
```
The accuracy or correct classification rate of the decision tree called `t.model` is `r round(mean(solution_tree_probability == test$Match_Status), digits = 3)`.  

Look at pruning the tree branches to determine the optimal complexity vs. accuracy point for stopping branch growth.  Pruning complexity parameter (cp) plot illustrating the relative cross validation error (y-axis) for various cp values (lower x-axis). Smaller cp values lead to larger trees (upper x-axis). Using the 1-SE rule, a tree size of 10-12 provides optimal cross validation results.
```{r}
plotcp(t.model)
tm_print_save(filename = "plotcp.tiff")

printcp(t.model)
tm_print_save(filename = "tree_sizes.tiff")
```

The figure shows the pruning complexity parameter plot for a fully grown tree. Significant reduction in the cross validation error is achieved with tree sizes 88-100 and then the cross validation error levels off with minimal or no additional improvements.
```{r}
t.model2 <- rpart(
    formula = formula,
    data    = train,
    method  = "anova", 
    control = list(cp = 0, xval = 10)
)

plotcp(t.model2)
abline(v = 34, lty = "dashed")  #Not sure if 11 is ideal.  
#https://bradleyboehmke.github.io/HOML/DT.html
tm_print_save("plotcp_for_t_model2.tiff")
```

```{r}
# rpart cross validation results
t.model2$cptable

# caret cross validation results
t.model3 <- caret::train(
  Match_Status ~ .,
  data = train,
  method = "rpart",
  trControl = trainControl(method = "cv", number = 10),
  tuneLength = 20
)

t.model3.plot <- ggplot(t.model3)
t.model3.plot

tm_ggsave(t.model3.plot, "t_model3_plot.tiff")
```

Feature interpretation of Decision Trees
Variable importance based on the total reduction in accuracy for the Match_Status decision tree.
```{r}
tm_vip(t.model3)
tm_print_save("vip_for_t.model3.plot.tiff")
```

```{r}
#########NOT WORKING


# # Construct partial dependence plots
# p1 <- pdp::partial(t.model3, pred.var = "Age") %>% autoplot() #Not working
# p2 <- pdp::partial(t.model3, pred.var = "USMLE_Step_1_Score") %>% autoplot()
# p3 <- partial(t.model3, pred.var = c("Gr_Liv_Area", "Year_Built")) %>% 
#   plotPartial(levelplot = FALSE, zlab = "yhat", drape = TRUE, 
#               colorkey = TRUE, screen = list(z = -20, x = -60))
# 
# # Display plots side by side
# gridExtra::grid.arrange(p1, p2, p3, ncol = 3)
```


```{r}
pruned.t.model <- rpart::prune(t.model, cp = 0.005) #Set the cp where the complexity to accuraty rate plateaued.  
tm_rpart_plot(pruned.t.model)
tm_print_save("pruned_t_model.tiff")

summary(pruned.t.model)
```
Based on the complexity plot, prune the tree to a complexity of 0.005 using the prune() function with the tree and the complexity parameter.  Now Compute the accuracy of the pruned tree.  

```{r}
pred = predict(pruned.t.model, test, type = "class")

caret::confusionMatrix(as.factor(test$Match_Status), pred)  #In Ops.factor(predictedScores, threshold) : ‘<’ not meaningful for factors

(pruned.t.model)

#Not working
# pred_ct <- prediction(as.numeric(as.character(pred)), as.numeric(as.character(test$Match_Status)))
# perf_ct <- performance(pred_ct, "tpr", 'fpr')
# plot(perf_ct, main = "ROC curve", colorize = T)
# abline(0,1, col='gray60')
```

```{r}
# Compute the accuracy of the pruned tree
solution_tree <- predict(pruned.t.model, newdata = test, type = "class")
mean(solution_tree == test$Match_Status)
```
The accuracy or correct classification rate of the PRUNED decision tree called `pruned.t.model` is `r round(mean(solution_tree == test$Match_Status), digits = 3)`.  Not a huge benefit in accuracy but the model will be more understandable and be faster to run now that it is pruned.  
