\documentclass[12pt,]{article}
\usepackage{lmodern}
\usepackage{amssymb,amsmath}
\usepackage{ifxetex,ifluatex}
\usepackage{fixltx2e} % provides \textsubscript
\ifnum 0\ifxetex 1\fi\ifluatex 1\fi=0 % if pdftex
  \usepackage[T1]{fontenc}
  \usepackage[utf8]{inputenc}
\else % if luatex or xelatex
  \ifxetex
    \usepackage{mathspec}
  \else
    \usepackage{fontspec}
  \fi
  \defaultfontfeatures{Ligatures=TeX,Scale=MatchLowercase}
\fi
% use upquote if available, for straight quotes in verbatim environments
\IfFileExists{upquote.sty}{\usepackage{upquote}}{}
% use microtype if available
\IfFileExists{microtype.sty}{%
\usepackage{microtype}
\UseMicrotypeSet[protrusion]{basicmath} % disable protrusion for tt fonts
}{}
\usepackage[margin=1in]{geometry}
\usepackage{hyperref}
\hypersetup{unicode=true,
            pdftitle={DRAFT: A model to predict chances of matching into Obstetrics and Gynecology Residency},
            pdfauthor={Tyler M. Muffly, MD},
            pdfborder={0 0 0},
            breaklinks=true}
\urlstyle{same}  % don't use monospace font for urls
\usepackage{color}
\usepackage{fancyvrb}
\newcommand{\VerbBar}{|}
\newcommand{\VERB}{\Verb[commandchars=\\\{\}]}
\DefineVerbatimEnvironment{Highlighting}{Verbatim}{commandchars=\\\{\}}
% Add ',fontsize=\small' for more characters per line
\usepackage{framed}
\definecolor{shadecolor}{RGB}{248,248,248}
\newenvironment{Shaded}{\begin{snugshade}}{\end{snugshade}}
\newcommand{\AlertTok}[1]{\textcolor[rgb]{0.94,0.16,0.16}{#1}}
\newcommand{\AnnotationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\AttributeTok}[1]{\textcolor[rgb]{0.77,0.63,0.00}{#1}}
\newcommand{\BaseNTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\BuiltInTok}[1]{#1}
\newcommand{\CharTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\CommentTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\CommentVarTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ConstantTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ControlFlowTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\DataTypeTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{#1}}
\newcommand{\DecValTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\DocumentationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\ErrorTok}[1]{\textcolor[rgb]{0.64,0.00,0.00}{\textbf{#1}}}
\newcommand{\ExtensionTok}[1]{#1}
\newcommand{\FloatTok}[1]{\textcolor[rgb]{0.00,0.00,0.81}{#1}}
\newcommand{\FunctionTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\ImportTok}[1]{#1}
\newcommand{\InformationTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\newcommand{\KeywordTok}[1]{\textcolor[rgb]{0.13,0.29,0.53}{\textbf{#1}}}
\newcommand{\NormalTok}[1]{#1}
\newcommand{\OperatorTok}[1]{\textcolor[rgb]{0.81,0.36,0.00}{\textbf{#1}}}
\newcommand{\OtherTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{#1}}
\newcommand{\PreprocessorTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textit{#1}}}
\newcommand{\RegionMarkerTok}[1]{#1}
\newcommand{\SpecialCharTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\SpecialStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\StringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\VariableTok}[1]{\textcolor[rgb]{0.00,0.00,0.00}{#1}}
\newcommand{\VerbatimStringTok}[1]{\textcolor[rgb]{0.31,0.60,0.02}{#1}}
\newcommand{\WarningTok}[1]{\textcolor[rgb]{0.56,0.35,0.01}{\textbf{\textit{#1}}}}
\usepackage{longtable,booktabs}
\usepackage{graphicx,grffile}
\makeatletter
\def\maxwidth{\ifdim\Gin@nat@width>\linewidth\linewidth\else\Gin@nat@width\fi}
\def\maxheight{\ifdim\Gin@nat@height>\textheight\textheight\else\Gin@nat@height\fi}
\makeatother
% Scale images if necessary, so that they will not overflow the page
% margins by default, and it is still possible to overwrite the defaults
% using explicit options in \includegraphics[width, height, ...]{}
\setkeys{Gin}{width=\maxwidth,height=\maxheight,keepaspectratio}
\IfFileExists{parskip.sty}{%
\usepackage{parskip}
}{% else
\setlength{\parindent}{0pt}
\setlength{\parskip}{6pt plus 2pt minus 1pt}
}
\setlength{\emergencystretch}{3em}  % prevent overfull lines
\providecommand{\tightlist}{%
  \setlength{\itemsep}{0pt}\setlength{\parskip}{0pt}}
\setcounter{secnumdepth}{0}
% Redefines (sub)paragraphs to behave more like sections
\ifx\paragraph\undefined\else
\let\oldparagraph\paragraph
\renewcommand{\paragraph}[1]{\oldparagraph{#1}\mbox{}}
\fi
\ifx\subparagraph\undefined\else
\let\oldsubparagraph\subparagraph
\renewcommand{\subparagraph}[1]{\oldsubparagraph{#1}\mbox{}}
\fi

%%% Use protect on footnotes to avoid problems with footnotes in titles
\let\rmarkdownfootnote\footnote%
\def\footnote{\protect\rmarkdownfootnote}

%%% Change title format to be more compact
\usepackage{titling}

% Create subtitle command for use in maketitle
\providecommand{\subtitle}[1]{
  \posttitle{
    \begin{center}\large#1\end{center}
    }
}

\setlength{\droptitle}{-2em}

  \title{DRAFT: A model to predict chances of matching into Obstetrics and
Gynecology Residency}
    \pretitle{\vspace{\droptitle}\centering\huge}
  \posttitle{\par}
    \author{Tyler M. Muffly, MD}
    \preauthor{\centering\large\emph}
  \postauthor{\par}
      \predate{\centering\large\emph}
  \postdate{\par}
    \date{07 June, 2019}

\usepackage{booktabs}
\usepackage{longtable}
\usepackage{array}
\usepackage{multirow}
\usepackage{wrapfig}
\usepackage{float}
\usepackage{colortbl}
\usepackage{pdflscape}
\usepackage{tabu}
\usepackage{threeparttable}
\usepackage{threeparttablex}
\usepackage[normalem]{ulem}
\usepackage{makecell}
\usepackage{xcolor}

\begin{document}
\maketitle

{
\setcounter{tocdepth}{2}
\tableofcontents
}
Objective: We sought to construct and validate a model that predict a
medical student's chances of matching into an obstetrics and gynecology
residency.

\hypertarget{introduction}{%
\section{Introduction}\label{introduction}}

The data set of years 2015, 2016, 2017, and 2018 at University of
Colorado. Applicants who were in the SOAP and applied to the preliminary
spot were determined to be unmatched. The data is contained in a data
frame called all\_data.

\hypertarget{install-and-load-packages.-see-session-information-at-the-end.}{%
\subsection{Install and Load packages. See Session information at the
end.}\label{install-and-load-packages.-see-session-information-at-the-end.}}

\hypertarget{create-a-dataframe-of-independent-and-dependent-variables.-download-cleaned-data-from-dropbox.}{%
\subsection{Create a dataframe of independent and dependent variables.
Download cleaned data from
Dropbox.}\label{create-a-dataframe-of-independent-and-dependent-variables.-download-cleaned-data-from-dropbox.}}

\#Potential code with Drake. I'm not sure if using drake is necessary or
if it is overkill.

\hypertarget{data-quality-check-of-all_data}{%
\subsection{\texorpdfstring{Data Quality Check of
\texttt{all\_data}}{Data Quality Check of all\_data}}\label{data-quality-check-of-all_data}}

A summary of the 19 variables are listed below:

\begin{enumerate}
\def\labelenumi{\arabic{enumi}.}
\item
  Eleven of the variables were a factor. All factors had two levels
  except for Alpha\_Omega\_Alpha had three levels. The target variable
  is \texttt{all\_data\$Match\_Status}.
\item
  Seven of the variables were integers.
\item
  One of the variables was a number. Age was calculated as a number.
\end{enumerate}

\#cleanup.import is not working.

Data size and structure

\hypertarget{univariate-analysis}{%
\subsection{Univariate Analysis}\label{univariate-analysis}}

IS THERE SOMETHING BETTER THAN THESE GGPAIRS PLOTS?
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-5-1.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-5-2.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-5-3.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-5-4.pdf}
Univariate analysis of the data.

IS THERE A WAY TO SET THE CUTPOINTS BASED ON A STATISTICAL REASONING?

I set the cutpoints based on nothing really. I like this better than the
base summary command. See the Appendix at the end of the document for
univariate distributions.

\hypertarget{relaxed-cubic-splines-for-continuous-variables}{%
\subsection{Relaxed Cubic Splines For Continuous
Variables}\label{relaxed-cubic-splines-for-continuous-variables}}

HOW DO YOU DECIDE HOW MANY KNOTS TO USE ON CONTINUOUS VARIABLES WITH
RELAXED SPLINES?

Set the Match\_Status variable to be a number and a factor.

\hypertarget{table-1}{%
\subsection{Table 1}\label{table-1}}

Table 1: Applicant Descriptive Variables by Matching Success (1) or
Failure (0)

ARSENAL VS. STARGAZER TO CREATE A TABLE 1 OF DESCRIPTIVE STATISTICS?

Descriptive summaries of all variables in the dataset are provided in
the table.

\textbackslash{}begin\{table\}{[}!htbp{]} \centering 
\textbackslash{}caption\{Descriptive Statistics of Match\_Status Data\}
\label{} \small 

\begin{tabular}{@{\extracolsep{5pt}}lcccccccc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
Statistic & \multicolumn{1}{c}{N} & \multicolumn{1}{c}{Mean} & \multicolumn{1}{c}{St. Dev.} & \multicolumn{1}{c}{Min} & \multicolumn{1}{c}{Pctl(25)} & \multicolumn{1}{c}{Median} & \multicolumn{1}{c}{Pctl(75)} & \multicolumn{1}{c}{Max} \\ 
\hline \\[-1.8ex] 
\hline \\[-1.8ex] 
\end{tabular}

\textbackslash{}end\{table\}

\hypertarget{exploratory-data-analysis}{%
\subsection{2) Exploratory data
analysis}\label{exploratory-data-analysis}}

After the data check was completed, an exploratory data analysis (EDA)
was conducted to look for interesting relationships among the variables.
Histograms were used to visualize distributions among predictors. Since
the assignment was a classification problem, relationships between
predictors and the dichotomous outcome were also performed.
Distributions of all variables were skewed right. Examples of histograms
of seven variables: age, Count of articles and abstracts, count of oral
presentations, count of poster presentations, Count of online
pubications, count of non-published publications, count of peer-reviewed
book chapters, are demonstrated below.

\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-11-1.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-11-2.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-11-3.pdf}

\hypertarget{normality-testing-in-r}{%
\subsection{Normality testing in R}\label{normality-testing-in-r}}

The D'Agostino tests for skewness and the Anscombe tests for kurtosis
with numeric variables. There is kurtosis for the Step 1 score data.
Therefore only use medians in table 1 and I will stick with
non-parametric tests throughout. There is skew in age.

Quantile-Quantile plot is a way to visualize the deviation from a
specific probability distribution. After analyzing these plots, it is
often beneficial to apply mathematical transformation (such as log) for
models like linear regression. DO WE NEED TO TRANSFORM THESE VARIABLES
IN ANY WAY? SHOULD WE USE Kolmogorov-Smirnov (K-S) normality test OR
Shapiro-Wilk's test???

The null hypothesis of these tests is that ``sample distribution is
normal''. If the test is significant, the distribution is non-normal.
From the output, the p-value less than 0.05 implying that the
distribution of the data are significantly different from normal
distribution. In other words, we can not assume the normality.

This allowed for measuring the associations between continuous
predictors using a matrix with correlation coefficients. The scatterplot
matrix of a sample of predictors below demonstrated some associations.
CAN WE INCLUDE ONLY CONTINUOUS VARIABLES? SHOULD WE INCLUDE THE TARGET
VARIABLE: MATCH\_STATUS?

\#\# Correlation overview

In this kind of plot we want to look for the bright, large circles which
immediately show the strong correlations (size and shading depends on
the absolute values of the coefficients; color depends on direction).
This shows whether two features are connected so that one changes with a
predictable trend if you change the other. The closer this coefficient
is to zero the weaker is the correlation. Anything that you would have
to squint to see is usually not worth seeing! CAN ONLY CORRELATE
CONTINOUS VARIABLES? SHOULD I RUN CORRELATION ON THE TRAIN DATA OR ON
THE ALL\_DATA SET?

p-value associated with the null hypothesis of 0 correlation, small
values indicate evidence that the true correlation is not equal to 0.

I'M NOT SURE WHY THE PROPORTION OF MATCHED VS UNMATCHED DATA IS
DIFFERENT BETWEEN THE TRAIN VS TEST DATA SETS? I THOUGHT ABOUT REMAKING
THE TRAINING SET TO BE 2015 AND 2016 WITH THE TEST SET OF 2017 AND 2018
IF WE CAN'T FIGURE IT OUT.

Check Proportions of Matched

\#\#Compare the datasets of train and test

IS IT NECESSARY TO CREATE A MODEL WITH ALL THE VARIABLES FIRST? SPLINES
QUESTION FROM ABOVE.

Create a Kitchen Sink model with all factors first. This is essentially
a screening model with all variables. I relaxed the cubic splines with
the guidance from above.

Are there predictor interactions????? HOW WOULD I RECOGNIZED THE
INTERACTIONS AND DISPLAY THE RESULTS IN A READABLE WAY?

\hypertarget{how-do-i-check-for-collinearity-in-the-variables-the-variables-with-splines-have-a-very-high-vif.-why-is-this-and-should-i-keep-them-in}{%
\subsection{HOW DO I CHECK FOR COLLINEARITY IN THE VARIABLES? THE
VARIABLES WITH SPLINES HAVE A VERY HIGH VIF. WHY IS THIS AND SHOULD I
KEEP THEM
IN?}\label{how-do-i-check-for-collinearity-in-the-variables-the-variables-with-splines-have-a-very-high-vif.-why-is-this-and-should-i-keep-them-in}}

\hypertarget{evaluating-the-signficance-of-kitchen.sink-variables}{%
\subsection{Evaluating the signficance of kitchen.sink
variables}\label{evaluating-the-signficance-of-kitchen.sink-variables}}

According to the ANOVA: USMLE\_Step\_1\_Score, Age, and
US\_or\_Canadian\_Applicants are predictors. The anova() function for
the model object allows to see the null and residuals deviances. The
difference between these two deviances shows how well the model is
performing against the null deviance. The residuals deviance column
allows to see the drop of deviance value by additional respective
predictor term added.

\#Factor Selection

I LIKED LASSO BECAUSE CHOOSING SOME PEOPLE MAY FIND THE PREDICTORS OR
AGE, RACE, GENDER DISCRIMINATORY. I NEED SOME STATISTICAL TEST TO STAND
ON ABOUT WHY I CHOSE THESE VARIABLES.

\#Factor Selection using a LASSO model (Penalized Logistic Regression)

Here, we use Lasso for simplicity and interpretability. The aim is to
avoid over-parametrization and unnecessary model bias by carrying
feature selection on-the-go. Key to this task will be cross-validation.
Start by creating a custom train control providing the number of
cross-validations and setting the classProbs to TRUE for logistic
regression.

WOULD YOU DO MORE REPEATS IN MYCONTROL?

Create the LASSO using glmnet within the caret package. Here we are
solely using the train dataset to determine what varaiables predict the
outcome.

IS THERE A COEFFICIENT CUTOFF THAT YOU WOULD USE TO CHOSE THE FINAL
PREDICTORS OR NO?

Plot the results of the lasso.mod so we can see if this is more ridge or
more lasso. 0 = ridge regression and 1 = LASSO regression, here ridge is
better

DO WE NEED TO SAVE THE LASSO MODEL FOR ANY REASON IN THE FUTURE?

\#\# Plot LASSO factors

Plot the individual variables by lambda. Saves the lasso.mod to an RDS
file for later use.

Makes predictions of matching based on the lasso.mod using the training
data.

GLMNet to do factor selection with the previously made LASSO model And
we use the glmnet library to determine the optimal penalization
parameter. Note that this must be assigned through cross validation;
here, we use 50-fold cross validation (only suitable in small datasets).
GLMnet accepts data in a matrix format so the data format was changed
before giving it to glmnet.cv.

\begin{Shaded}
\begin{Highlighting}[]
\StringTok{`}\DataTypeTok{%ni%}\StringTok{`}\NormalTok{<-}\KeywordTok{Negate}\NormalTok{(}\StringTok{`}\DataTypeTok{%in%}\StringTok{`}\NormalTok{)}
\CommentTok{# save the outcome for the glmnet model, could use dummyVars with fullRan=FALSE can remove collinearity by removing male.gender so you are either male or female}

\NormalTok{x <-}\StringTok{ }\KeywordTok{model.matrix}\NormalTok{(train}\OperatorTok{$}\NormalTok{Match_Status}\OperatorTok{~}\NormalTok{., }\DataTypeTok{data=}\NormalTok{train)}

\KeywordTok{class}\NormalTok{(x)}
\end{Highlighting}
\end{Shaded}

\begin{verbatim}
[1] "matrix"
\end{verbatim}

\begin{Shaded}
\begin{Highlighting}[]
\NormalTok{x <-}\StringTok{ }\NormalTok{x[,}\OperatorTok{-}\DecValTok{1}\NormalTok{]  }\CommentTok{#Removes intercept}

\KeywordTok{set.seed}\NormalTok{(}\DecValTok{356}\NormalTok{)}

\NormalTok{glmnet1 <-}\StringTok{ }
\StringTok{  }\KeywordTok{cv.glmnet}\NormalTok{(}\DataTypeTok{x=}\NormalTok{x,}\DataTypeTok{y=}\NormalTok{train}\OperatorTok{$}\NormalTok{Match_Status,}\DataTypeTok{nfolds=}\DecValTok{10}\NormalTok{,}\DataTypeTok{alpha=}\NormalTok{.}\DecValTok{5}\NormalTok{, }\DataTypeTok{family=}\StringTok{"binomial"}\NormalTok{)}

\KeywordTok{plot}\NormalTok{(glmnet1,}\DataTypeTok{main =} \StringTok{"Misclassification Error"}\NormalTok{)}
\end{Highlighting}
\end{Shaded}

\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-31-1.pdf}

The left vertical line represents the minimum error, and the right
vertical line represents the cross-validated error within 1 standard
error of the minimum. LASSO, least absolute shrinkage and selection
operator

If you look at this graph we ran the model with a range of values for
lambda and saw which returned the lowest cross-validated error. You'll
see that our cross-validated error remains consistent until we hit the
dotted lines, where we start to see our model perform very poorly due to
underfitting with misclassification error. Cross validation is an
essential step in studies to help up us not only calibrate the
parameters of our model but estimate the prediction accuracy with unseen
data.

Variable selection using LASSO in the train dataset

I DON'T REALLY UNDERSTAND WHAT THIS GRAPH MEANS?

\#\# Measuring Strength and Direction of Predictors

I plotted everything on a bar graph so we can easily compare the
strongest predictors and the direction they affect the model:

Revise Model with selected factors

Creating a more parsiomonious model using the variables selected by
LASSO in the train dataset. I made the model with lrm so I could fit it
to a rms::nomogram function.

I THINK IT WAS WRONG TO DROP SOME OF THESE VARIABLES THAT HAD RELAXED
CUBIC SPLINES:

\#\# Remove variables with multicollinearity and rebuild model

AUC of this pared down model is 0.8261587.

\hypertarget{odds-ratios-of-the-train-dataset}{%
\subsection{\texorpdfstring{Odds ratios of the \texttt{train}
dataset}{Odds ratios of the train dataset}}\label{odds-ratios-of-the-train-dataset}}

\#Table 2 of odds ratios in graph form in the train dataset.

Odds ratios for train data

\#\url{https://rstudio-pubs-static.s3.amazonaws.com/283447_fd922429e1f0415c89b93b6da6dc1ccc.html}

Annotation for Manuscript Table 2: A: Nonlinear component A of the
function describing the variable and the probability of matching into
OBGYN. B: Nonlinear component B of the function describing the variable
and the probability of matching into OBGYN. C: Nonlinear component C of
the function describing the variable and the probability of matching
into OBGYN.

\hypertarget{use-model-to-predict-match-for-test-data}{%
\section{Use Model to predict match for Test
Data}\label{use-model-to-predict-match-for-test-data}}

Shift Gears: Test Accuracy of Model on Training Data, Use glmnet model
on 2018 TEST data Here the code is creating a vector called
predictorsNames so that we can reuse the model by changing the variables
in predictorsNames in the future prn. Run the 2018 data through the
train model.

First, we need to fit lrm.with.lasso.variables in GLM, rather than rms
to get the AUC. There is probably a better way to do this. Using the
test data set. Also built the same model in lrm.

The Receiver Operating Characteristic (ROC) curve is plotted below for
false positive rate (FPR) in the x-axis vs.~the true positive rate (TPR)
in the y-axis. It shows the detection of true positive while avoiding
the false positive. This is the same as measuring the unspecificity (1 -
specificity) in x-axis, against the sensitivity in y-axis. This ROC
curve in particular shows that its very closed to the perfect classifier
meaning that its better at identifying the positive values.

\hypertarget{use-model-to-predict-match-status-for-test-data}{%
\subsection{Use Model to predict match Status for Test
Data}\label{use-model-to-predict-match-status-for-test-data}}

\#ROC: Type 1 using ggplot with nice controls

ROC Curve type 2 with nice labels on the x and y

ROC Curve Type 3 with nice diagnal line but half of the formula printed

ROC Curve Type 4, ROC in color

I DO NOT UNDERSTAND WOE BINNING AT ALL.

\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-49-1.pdf}

Since the predictors were highly skewed, binning was also explored. This
facilitated visualizing associations between binned variables and the
outcome using contingency plots. Supervised Weight of Evidence (WOE)
binning of numeric variables were explored using the woeBinning package.
Fine and coarse classing that merged granular classes and levels step by
step was performed. Bins were merged and respectively split based on
similar weight of evidence (WOE) values and stop via an information
value (IV) based criteria. The figure below demonstrated the top five
predictors ranked by information value during binning.

\begin{verbatim}

   0    1 
 364 1240 
\end{verbatim}

\begin{longtable}[]{@{}ll@{}}
\toprule
{[}{]}(Tyler-2\_files/figure-latex/ & unnamed-chunk-50-1.pdf)
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-2.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-3.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-4.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-5.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-6.pdf}
\includegraphics{Tyler-2_files/figure-latex/unnamed-chunk-50-7.pdf}\tabularnewline
\midrule
\endhead
Age & 0.7613197\tabularnewline
USMLE\_Step\_1\_Score & 0.5566617\tabularnewline
Count\_of\_Poster\_Presentation & 0.0608892\tabularnewline
Count\_of\_Articles\_Abstracts & 0.01508961\tabularnewline
Count\_of\_Oral\_Presentation & 0.01043028\tabularnewline
\bottomrule
\end{longtable}

These top five binned variables were used for the training and test set.

\begin{verbatim}

   0    1 
 560 1277 
\end{verbatim}

Relationships between binned variables and \texttt{Match\_Status} were
explored using mosaic plots to look for interesting bins that aided in
discrimination. An example of several binned variables are shown in the
plots below.

\includegraphics{Tyler-2_files/figure-latex/OneR diagnostic plots-1.pdf}
\includegraphics{Tyler-2_files/figure-latex/OneR diagnostic plots-2.pdf}
\includegraphics{Tyler-2_files/figure-latex/OneR diagnostic plots-3.pdf}
\includegraphics{Tyler-2_files/figure-latex/OneR diagnostic plots-4.pdf}

A simple decision tree model was used for exploration. The variable
importance summary from the simple tree was used to explore important
relationships. The variables a, b, c, and d were the top four variables
in importance.

CAN WE USE ONLY FACTORS IN THIS TREE MODEL?

The simple tree was plotted below. The a, b and c variables were near
the roots of the tree demonstrating importance. DO ALL THE VARIABLES
NEED TO BE BINNED AS FACTORS TO RUN A TREE PLOT?

\includegraphics{Tyler-2_files/figure-latex/fancyR plot rpart EDA-1.pdf}

Exploratory random forest was also performed. The variable importance
for the random forest model was summarized in the figure below. The
variables capital\_run\_length\_longest, capital\_run\_length\_total,
char\_freq\_dollar.binned, word\_freq\_free and word\_freq\_your were
the top five using accuracy and the Gini index.

\includegraphics{Tyler-2_files/figure-latex/RF EDA-1.pdf}

\hypertarget{the-model-build}{%
\subsection{3) The Model Build}\label{the-model-build}}

All models were fit using the data labeled train and validated using the
data labeled test. 10-fold cross-validation was performed for variable
selection and parameter estimation was performed using cross-validation
where appropriate.

\textbf{(1) Logistic regression using backwards variable selection
model}

A logistic regression model using backwards variable selection was fit.
The summary of the model coefficients for the final model is presented
in Table 3. Table 4 demonstrates the confusion matrix for the in-sample
performance of the model and Table 5 demonstrates the confusion matrix?
or AUC? for the out-of-sample performance.

\#I'M NOT SURE IF I INCLUDED ALL THE RIGHT VARIABLES

\begin{table}[!htbp] \centering 
  \caption{Backwards Logistic Regression Model Results} 
  \label{} 
\begin{tabular}{@{\extracolsep{5pt}}lc} 
\\[-1.8ex]\hline 
\hline \\[-1.8ex] 
 & \multicolumn{1}{c}{\textit{Dependent variable:}} \\ 
\cline{2-2} 
\\[-1.8ex] & as.factor(Match\_Status) \\ 
\hline \\[-1.8ex] 
 white\_non\_whiteWhite & 0.413$^{***}$ (0.128, 0.697) \\ 
  woe.Age.binned & $-$0.005$^{***}$ ($-$0.007, $-$0.004) \\ 
  GenderMale & $-$0.249 ($-$0.564, 0.066) \\ 
  Couples\_MatchYes & 0.824$^{**}$ (0.036, 1.611) \\ 
  US\_or\_Canadian\_ApplicantYes & 1.536$^{***}$ (1.203, 1.868) \\ 
  Medical\_Education\_InterruptedYes & $-$0.549$^{***}$ ($-$0.900, $-$0.198) \\ 
  Visa\_Sponsorship\_NeededYes & $-$0.391$^{*}$ ($-$0.814, 0.032) \\ 
  woe.USMLE\_Step\_1\_Score.binned & $-$0.007$^{***}$ ($-$0.009, $-$0.005) \\ 
  Constant & 0.247 ($-$0.068, 0.562) \\ 
 \hline \\[-1.8ex] 
Observations & 1,604 \\ 
Log Likelihood & $-$624.123 \\ 
Akaike Inf. Crit. & 1,266.246 \\ 
\hline 
\hline \\[-1.8ex] 
\textit{Note:}  & \multicolumn{1}{r}{$^{*}$p$<$0.1; $^{**}$p$<$0.05; $^{***}$p$<$0.01} \\ 
\end{tabular} 
\end{table}

\begin{table}[t]

\caption{\label{tab:conf matrix backwards LR}Confusion matrix of Backwards LR on train set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.4587912 & 0.5412088\\
1 & 0.0604839 & 0.9395161\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:Conf matrix test}Confusion matrix of Backwards LR on test set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.3571429 & 0.6428571\\
1 & 0.0469851 & 0.9530149\\
\bottomrule
\end{tabular}
\end{table}

The in-sample accuracy was 0.8304239 and the out-of-sample accuracy was
0.7713664.

\pagebreak

\textbf{(2) Tree model}

A CART model was fit using the rpart package. The final model is
presented in the figure below. The majority of final predictors were
derived from the binning process. The variables
char\_free\_exclamation.binned, word\_freq\_removed.binned, and
woe.char\_freq\_dollar.binned had significant influence in the model.
Table 6 demonstrates the confusion matrix for the in-sample performance
of the model and Table 7 demonstrates the confusion matrix for the
out-of-sample performance.

\includegraphics{Tyler-2_files/figure-latex/CART plot -1.pdf}

\begin{table}[t]

\caption{\label{tab:conf matrix train cart}Confusion matrix of CART on train set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.5109890 & 0.4890110\\
1 & 0.0491935 & 0.9508065\\
\bottomrule
\end{tabular}
\end{table}

NOT WORKING, BECAUSE TEST HAS NOT BEEN WOE BINNED???

It is not working because the training model had a different amount of
columns then the test data you're feeding into it. Just ensure that they
have the correct columns that match. I am commenting this out as it's
not something I should fix.

Georg: Should work now

\begin{table}[t]

\caption{\label{tab:CART test conf matrix}Confusion matrix of CART on test set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.5109890 & 0.4890110\\
1 & 0.0491935 & 0.9508065\\
\bottomrule
\end{tabular}
\end{table}

For the CART model, the in-sample accuracy was 0.8509975 and
out-of-sample accuracy was 0.7675558.

\pagebreak

\textbf{(3) a Support Vector Machine model}

The support vector machine model was fit. Cross validation identified a
cost C = 1 using a linear kernel and a sigma = ??? using ??? support
vectors.

I DO NOT KNOW HOW TO MAKE THIS WORK.

\begin{table}[t]

\caption{\label{tab:SVM train conf matrix}Confusion matrix of SVM model on train set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.4697802 & 0.5302198\\
1 & 0.0620968 & 0.9379032\\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[t]

\caption{\label{tab:SVM test conf matrix}Confusion matrix of SVM model on test set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.4697802 & 0.5302198\\
1 & 0.0620968 & 0.9379032\\
\bottomrule
\end{tabular}
\end{table}

For the SVM model, the in-sample accuracy was 0.8316708 and
out-of-sample accuracy was 0.7637452. The in-sample confusion matrix for
the SVM model is shown in Table 8 and the out-of-sample confusion matrix
for the SVM model is shown in Table 9.

\textbf{(4) Random Forest model}

A random forest model was fit to the training data. Cross-validation
selected the a final value used for mtry = 12 based on optimizing
accuracy. The variable importance plot for the random forest model is
demonstrated below. Important predictors were similar between the CART
model and the RF model. The predictors char\_freq\_exclamation.binned,
woe.char\_freq\_exclamation.binned, aand woe.char\_freq\_dollar.binned
were top three for variable importance.

\includegraphics{Tyler-2_files/figure-latex/var imp plot rf model-1.pdf}

For the random forest model, the in-sample accuracy was 0.8260599 and
out-of-sample accuracy was 0.7332608. The in-sample confusion matrix for
the RF model is shown in Table 10 and the out-of-sample confusion matrix
for the RF model is shown in Table 11.

\pagebreak

\hypertarget{naive-bayes-with-woe-binning-model}{%
\subsection{4) Naïve Bayes with WOE Binning
model}\label{naive-bayes-with-woe-binning-model}}

Finally, a Naïve Bayes model was fit. Similar to the previous models,
the top 5 WOE binned variables were also included in this model.
Cross-validation demonstrated that the tuning parameter `laplace' was
held constant at a value of 0 and tuning parameter `adjust' was held
constant at a value of 1.

\begin{table}[t]

\caption{\label{tab:NB model conf matrix}Confusion matrix of Naive Bayes model on train set}
\centering
\begin{tabular}{lrr}
\toprule
  & 0 & 1\\
\midrule
\rowcolor{gray!6}  0 & 0.0137363 & 0.9862637\\
1 & 0.0000000 & 1.0000000\\
\bottomrule
\end{tabular}
\end{table}

For the naive Bayes model, the in-sample accuracy was 0.7761845 and
out-of-sample accuracy was 0.6984213. The in-sample confusion matrix for
the naive Bayes model is shown in Table 12 and the out-of-sample
confusion matrix for the naive Bayes model is shown in Table 13.

\pagebreak

\hypertarget{model-comparison}{%
\subsection{5) Model Comparison}\label{model-comparison}}

Table 14 summarizes the overall in-sample and out-of-sample accuracy of
each model. The best performing models (highest accuracy) was the random
forest model with a test set accuracy of 0.7332608. The Logistic
regression model using backwards elimination was second with a test set
accuracy of 0.7713664. The Naive Bayes model did not perform as well as
the other models. In summary, if accuracy is the most important aspect
of the model and interpretion is not a priority then the best model was
the random forest model. If interpretability of the model is paramount,
then the logistic regression model is recommended.

\pagebreak

\#Annotation: Manuscript Figure 1: The first row called points assigned
to each variable's measurement from rows 2-12, which are variables
included in predictive model. Assigned points for all variables are then
summed and total can be located on line 13 (total points). Once total
points are located, draw a vertical line down to the bottom line to
obtain the predicted probability of matching. For non-linear variables
(count of oral presentations, etc.) values should be erad from left to
right.

\hypertarget{step-10-calibration-of-the-model-based-on-the-test-data.}{%
\section{Step 10: Calibration of the model based on the test
data.}\label{step-10-calibration-of-the-model-based-on-the-test-data.}}

The ticks across the x-axis represent the frequency distribution (may be
called a rug plot) of the predicted probabilities. This is a way to see
where there is sparsity in your predictions and where there is a
relative abundance of predictions in a given area of predicted
probabilities.

The ``Apparent'' line is essentially the in-sample calibration.

The ``Ideal'' line represents perfect prediction as the predicted
probabilities equal the observed probabilities.

The ``Bias Corrected'' line is derived via a resampling procedure to
help add ``uncertainty'' to the calibration plot to get an idea of how
this might perform ``out-of-sample'' and adjusts for ``optimistic''
(better than actual) calibration that is really an artifact of fitting a
model to the data at hand. This is the line we want to look at to get an
idea about generalization (until we have new data to try the model on).

When either of the two lines is above the ``Ideal'' line, this tells us
the model underpredicts in that range of predicted probabilities. When
either line is below the ``Ideal'' line, the model overpredicts in that
range of predicted probabilities.

Applying to your specific plot, it appears most of the predicted
probabilities are in the higher end (per rug plot). The model overall
appears to be reasonably well calibrated based on the Bias-Corrected
line closely following the Ideal line; there is some underprediction at
lower predicted probabilities because the Bias-Corrected line is above
the Ideal line around \textless{} 0.3 predicted probability.

The mean absolute error is the ``average'' absolute difference
(disregard a positive or negative error) between predicted probability
and actual probability. Ideally, we want this to be small (0 would be
perfect indicating no error). This seems small in this plot, but may be
situation dependent on how small is small.

\pagebreak

\hypertarget{references}{%
\subsection{References}\label{references}}

Lorrie Faith Cranor and Brian A. LaMacchia. Match\_Status!
Communications of the ACM. Vol. 41, No.~8 (Aug.~1998), Pages 74-83.
Definitive version:
\url{http://www.acm.org/pubs/citations/journals/cacm/1998-41-8/p74-cranor/}

\pagebreak \#Appendix

\hypertarget{appendix-exploratory-data-analysis}{%
\section{Appendix, Exploratory Data
Analysis}\label{appendix-exploratory-data-analysis}}

The funModeling package will first give distributions for numerical data
and finally creates cross-plots. This also saves the output of the
distributions to the results folder.

\hypertarget{appendix-supplemental-table-descriptive-analysis-of-all-variables-considered-in-the-training-set-along-with-their-association-to-matching.}{%
\section{Appendix, Supplemental Table: Descriptive analysis of all
variables considered in the training set along with their association to
matching.}\label{appendix-supplemental-table-descriptive-analysis-of-all-variables-considered-in-the-training-set-along-with-their-association-to-matching.}}

\#Appendix, Data in a private repository to be shared with the journal
\url{http://dx.doi.org/10.17632/3rtg46skbd.1}

Medical student \#1 is a 27.4year old White Male who is a US Senior
medical graduate

\url{https://denverhealth.az1.qualtrics.com/WRQualtricsControlPanel/?Section=SV_3QmslHJJmin4xBX\&SubSection=\&SubSubSection=\&PageActionOptions=\&TransactionID=1\&Repeatable=0\&restrictToBrand=1\&criteria=\&ContextSection=EditSection}

\hypertarget{abstract-draft}{%
\section{Abstract DRAFT}\label{abstract-draft}}

Background: A model that predicts a medical student's chances of
matching into an obstetrics and gynecology residency may facilitate
improved counseling and fewer unmatched medical students.

Objective: We sought to construct and validate a model that predicts a
medical student's chance of matching into obstetrics and gynecology
residency.

Study Design: In all, 3441 medical students applied to a residency in
Obstetrics and Gynecology at the University of Colorado from 2015 to
2018 were analyzed. The data set was splint into a model training cohort
of 1604 who applied in 2015, 2016, and 2017 and a separate validation
cohort of 1837 in 2018. In all, 19 candidate predictors for matching
were collected. Multiple logistic models were fit onto the training
choort to predict matching. Variables were removed using least absolute
shrinkage and selection operator reduction to find the best parsimonious
model. Model discrimination was measured using the concordance index.
The model was internally valideated using 1,000 bootstrapped samples and
temporarly validated by testing the model's performance in the
validation cohort. Calibration curves were plotted to inform educators
about the accuracy of predicted probabilities.

Results: The match rate in the training cohort was 77.3\% (I need help
getting 95\% CI). The model had excellent discrimination and calibration
during internal validation (bias-corrected concordance index,0.83) and
maintained accuracy during temportal validation using the separate
validation cohort (concordance index,0.84).

\hypertarget{prose-of-the-paper-draft}{%
\section{Prose of the paper DRAFT}\label{prose-of-the-paper-draft}}

Materials and Methods: This was an institutional review board exempt
retrospective cohort analysis of medical students who applied to
Obstetrics and Gynecology (OBGYN) residency from 2015 to 2018.
Guidelines for transparent reporting of a multivariable prediction model
for individual outcomes were used in this
study.(\url{https://www.equator-network.org/reporting-guidelines/tripod-statement/}).
Eligible students were identified if they applied to OBGYN residency
during the study period. The outcome of the model was defined as
matching or not matching into residency for the specific application
year. Individual predictors of successfully* matching were compiled from
a literature review, expert opinion, and judgment then collected from
the Electronic Residency Application Service materials.

Once the data set was complete it was divided into a model training and
test set. \emph{When an external validation data set is unavailable to
test a new model but an existing modeling data set is sufficiently
large, as in this case, it is recommended to split by time and develop
the model using data from one period and evaluate its performance from
data from a future period. We arbitrarily chose to divide the cohort
into a training set of 2015 to 2017 data and a training set of 2018
data. In all, ?? candidate risk factors were considered for fitting on
the training data set (supplmental table). Variable selection was done
using a peenalized logistic regression called least absolute shrinkage
and selection operator (LASSO). The LASSO model is a regression analysis
method that performs both variable selection and regularization in order
to enhance the prediction accuracy and interpretability of the
statistical model it produces. We elected to use LASSO to choose which
covariates to include over stepwise selection because the latter only
improves prediction accuracy in certain cases, such as when only a few
covariates have a strong relationship with the outcome. The logistic
model's discriminative ability was measured by the area under the curve
(AUC) for the receiver operating characteristic curve based on the
sensitivity and specificity of the model. An AUC value closer to 1
indicates a better prediction of the outcome and an AUC value of 0.5
indicates that the model predicts no better than chance. The AUC is also
a representation of the concordance index and measures the model's
ability to generate a higher predicted probability of a successful
match} occurring in a medical student who has a ????. For example, if we
have a pair of medical students, in which one medical student matches
and the other does not, the concordance index measures the model's
ability to assign a higher risk of not matching to the medical student
who successfully matches. All concordance indices and receiver operating
characteristic curves were internally validated using a 1,000 bootstrap
resample to correct for bias and overfitting within the model. The
bootstrapping method of validation has been shown to be superior to
other approaches to estimate internal validity. Calibration curves were
also plotted to depict the relationship between the model's predicted
outcomes against the cohort's observed outcome, where a perfectly
calibrated model follows a 45° line. After the best model was selected
and internally validated, the model was compared with the best currently
available method of estimating risk, that is, an expert medical
educator's predictions. To perform these comparisons, a subset of 50
participants was randomly selected for comparing the probability of
matching between the model and the panel of experts. These ??
participants were used to compare predictions of the models with
experts' predictions and not as a true independent validation subset.
The model was rebuilt using the remaining participants in the data set
excluding the 50 randomly selected participants. The candidate risk
factors of these 50 participants were given to 20 ``expert'' medical
educators with representation from each of the *** for review resulting
in 1,000 expert predictions and 50 model predictions for each outcome.
All medical educators were considered to be experienced in counseling
medical students regarding OBGYN matching. Each of the 20 experts were
asked to consider each medical student's data from all ??? variables
among the 50 randomly selected students and provide their best estimated
outcome by answering the following question: ``Out of 100 medical
students with these exact characteristics, estimate the number of
medical students who would not matching into OBGYN during the 2019
application year.'' Individual medical educators' predictions were not
averaged to yield a single value because incorporating each medical
educator's predictions substantially increased statistical power. The
model's predictions were compared with the experts' predictions, which
included all risk factors, to determine which was most accurate. The
difference in accuracy was determined by using a bootstrap method from
their respective receiver operating characteristic curves. All analyses
were performed using R 3.5. Results: A total of 3441 applied to
obstetrics and gynecology residency at the University of Colorado from
2015 to 2018. The overall mean rate of matching in the training cohort
was 1240 of 1604 was (77.3\%). The unadjusted comparison of the 19
candidate predictors in the training cohort are presented in
Supplemental Table 1. To identify predictors from the candidates we
employed least absolute shrinkage and selection operator (LASSO).
Regularisation techniques change how the model is fit by adding a
penalty for every additional parameter you have in the model. 12
variables were included within the final model. Applicants from the
United States or Canada, high USMLE Step 1 scores, female gender, White
race, no visa sponsorship needed, membership in Alpha Omega Alpha, no
interruption of medical training, couples matching, and allopathic
medical training increased the chances of matching into OBGYN. In
contrast, more oral presentations, increasing age, a higher number of
peer-reviewed online publications, an increased number of authored book
chapters, and a higher count of poster presentations all decreased the
probability of matching into OBGYN (table 2). The nomogram illustrates
the strength of association of the predictors to the outcome as well as
the nonlinear associations between age, count of Oral Presentations,
count of peer−reviewed book chapters and the chances of matching (Figure
1). \# Appendix, DynNom Model for Shiny Upload I WOULD LIKE HELP
UPLOADING THIS MODEL TO SHINY SERVER AS WELL PLEASE.

\hypertarget{publish-to-shiny}{%
\section{Publish to shiny}\label{publish-to-shiny}}

\hypertarget{getwd}{%
\section{getwd()}\label{getwd}}


\end{document}
