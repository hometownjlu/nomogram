---
title: "Pipeline for using h2o"
author: "Tyler M. Muffly, MD"
date: "Department of Obstetrics and Gynecology, Denver Health, Denver, CO"
output:
  html_document:
    toc: true
    toc_float: true
    toc_depth: 2
    self_contained: true
    code_folding: show
    dev: svg
    df_print: paged
    theme: yeti
  pdf_document:
    pandoc_args:
    - --wrap=none
    - --top-level-division=chapter
    df_print: paged
    fig_caption: yes
    #keep_tex: no
    latex_engine: xelatex
  word_document:
    toc_depth: '2'
fontsize: 12pt
geometry: margin=1in
header-includes:
- \usepackage{fancyhdr}
- \pagestyle{fancy}
- \fancyhead[R]{Muffly et al.}
- \usepackage{lineno}
- \linenumbers
fontfamily: mathpazo
spacing: double
always_allow_html: yes
editor_options: 
  chunk_output_type: console
---

```{r setup, include=FALSE}
invisible(gc())
dev.off()
setwd("~/Dropbox/Nomogram/nomogram/")
source(file="~/Dropbox/Nomogram/nomogram/Code/Additional_functions_nomogram.R", echo=TRUE, verbose=TRUE) #Must run by hand

library(h2o) #Loaded in "Additional_functions_nomogram.R" but this is belt and suspenders
library(recipes)
library(readxl)
library(tidyverse)
library(tidyquant)
library(stringr)
library(forcats)
library(cowplot)
library(fs)
library(glue)
library(tidyverse)
library(magrittr)
library(tidymodels)
```

```{r cars}
train_raw_tbl       <- train
test_raw_tbl        <- test
```

# Split data ----
```{r pressure, echo=FALSE}
# Resource: https://tidymodels.github.io/rsample/
#Split the data using `rsample`.  

set.seed(seed = 1978) 
all_data$Age <- as.numeric(all_data$Age)
all_data <- all_data 
data_split <- rsample::initial_split(data = all_data, # rsample::initial_split() For sampling
                                     strata = "Match_Status", 
                                     prop = 0.8)
data_split

train <- data_split %>% training() %>% glimpse()  # Extract the training dataframe
test  <- data_split %>% testing() %>% glimpse() 
```

```{r}
# Verify proportions have been maintained
train %>%
  count(Match_Status) %>%
  mutate(prop = n / sum(n))

test %>%
  count(Match_Status) %>%
  mutate(prop = n / sum(n))
```

# Preprocessing ----
```{r}
# Fix issues with data: Skew
#https://university.business-science.io/courses/246843/lectures/5021219
#Is skew present
skewed_feature_names <- train %>%
  dplyr::select_if(is.numeric) %>%
  map_df(PerformanceAnalytics::skewness) %>% #returns a single row tibble 
  tidyr::gather(factor_key = TRUE) %>% #transposes into a long data column
  arrange(desc(value)) %>% #Look for low and high values, high values have a fat tail on right, low has fat tail on left side
  filter(value>2.0) %>% #eyeballed cutoff for value cut off
  pull(key) %>%
  as.character()

png(filename="~/Dropbox/Nomogram/nomogram/Code/results/skewed_variables1.png")
skewed_variables <- train %>%
  dplyr::select(skewed_feature_names) %>% #Look to make sure these variables are not factors
  #NB:  Skew is present in everything starting with count and age
  plot_hist_facet()
plot(skewed_variables)
```


```{r}
train %>%
  dplyr::select_if(is.numeric) %>%
  plot_hist_facet()  #Variables like age and Step 1 scores have different ranges.  Age has 25 to 60 years and Step 1 score has 180 to 280.  Algorithm needs all data on the same scale.  USMLE data would dominate Age because of different ranges.  
```

# Feature Analysis ----
```{r}
#https://ryjohnson09.netlify.com/post/caret-and-tidymodels/
#Graphing the Outcomes
all_data %>%
  count(Match_Status) %>% 
  ggplot(aes(Match_Status, n, fill = Match_Status)) + 
  geom_col(width = .5, show.legend = FALSE) + 
  scale_y_continuous(labels = scales::comma) +
  #scale_fill_manual(values = custom_palette[3:2]) +
  #custom_theme + 
  labs(
    x = NULL,
    y = NULL,
    title = "Distribution of cases"
  )
```

# Plot characteristics ----
```{r}
# Create vector of predictors by removing column 19 that is Match_Status
expl <- names(train)[-19]

# Loop vector with map, moved here so we don't have to deal with dummy variables
expl_plots_box <- map(expl, ~box_fun_plot(data = train, x = "Match_Status", y = .x) )
cowplot::plot_grid(plotlist = expl_plots_box)  #Must view with zoom function

# Loop vector with map
expl_plots_density <- map(expl, ~density_fun_plot(data = train, x = "Match_Status", y = .x) )
cowplot::plot_grid(plotlist = expl_plots_density) #Must view with zoom function
```


# Recipes ----
```{r}
# Resource: https://tidymodels.github.io/recipes/

# --------------------------------------------------
#Recipe function - First, I create a recipe where I define the transformations I want to apply to my data. In this case I create a simple recipe to change all character variables to factors.
#https://ryjohnson09.netlify.com/post/caret-and-tidymodels/
#- tidymodels: https://github.com/tidymodels
#- recipes: https://tidymodels.github.io/recipes/


# --------------------------------------------------
#The order in which you infer, center, scale, etc does matter (see this post).  Artificial Neural Networks are best when the data is one-hot encoded, scaled and centered. In addition, other transformations may be beneficial as well to make relationships easier for the algorithm to identify.
#https://topepo.github.io/recipes/

# 1) Impute/Zero variance features that would add nothing to the data
# 2) Individual transformations for skewness and other issues -
# 3) Discretize (if needed and if you have no other choice)
# 4) Create dummy variables
# 5) Create interactions
# 6) Normalization steps (center, scale, range, etc)
# 7) Multivariate transformation (e.g. PCA, spatial sign, etc)

#recipe
recipe_obj <- recipe(Match_Status ~ ., data = train) %>%     
  update_role(Match_Status, new_role = "outcome") %>%
  #Using the formula sets the predictors and the outcome
  recipes::step_zv(all_predictors()) #%>%  
  #removes zero variance functions where every value is the same for every observation
  #recipes::step_YeoJohnson(skewed_feature_names) #%>%  
# The data is transformed for skewness (Age, Count of poster presentations)
#step_center(all_numeric(), -all_outcomes()) %>%
# step_scale(all_numeric(), -all_outcomes())
# #center subtracts out the means for numerics
# recipes::step_dummy(all_nominal(), -all_outcomes()))
#Process of converting categorical data to sparse data, which has columns of only zeros and ones. (a.k.a. dummy variables or a design matrix.) All Non-Numeric Data need to be coverted to Dummy Variables].
#dummying fixes factors with skew
#Helps with factor variables


# Prep the data here.  
recipe_obj %>%  
  recipes::prep() %>%
  recipes::bake(new_data = train) %>%
  dplyr::select(skewed_feature_names)

prepared_object <- recipe_obj %>%  
  recipes::prep()  #Does calculations to ready the data

prepared_object %>%
  bake(new_data = train) %>% #Can also do one hot encoding
  dplyr::select(contains("Gender")) %>%
  plot_hist_facet(ncol = 3)

# Finally bake the data
train_tbl <- bake(prepared_object, new_data = train)
train_tbl %>% glimpse()  #This is a well formed machine readable data set

test_tbl <- bake(prepared_object, new_data = test)
test_tbl %>% glimpse()  #This is a well formed machine readable data set


# --------------------Correlation ---------
args(get_cor)

train_tbl %>%
  get_cor(target = Match_Status, fct_reorder = TRUE, fct_rev = TRUE)

train_tbl %>%
  #select(Match_Status_Matched, contains("Count_of_")) %>%
  plot_cor(target = Match_Status, fct_reorder = TRUE, fct_rev = FALSE)
```


# Modeling -----
```{r}
# H2O Setup ----
# H2O Docs: http://docs.h2o.ai

# Setup instructions: 
# - Please update to the latest version of h2o using install.packages("h2o") 
#   to access the latest functionality and performance features
#   h2o.init() starts H2O in R's current working directory. h2o.importFile() looks for files from the perspective of where H2O was started.

# Finally, let's load H2O and start up an H2O cluster
library(h2o) #http://localhost:54321/flow/index.html
h2o.init(nthreads = -1, max_mem_size="8g")
h2o.getVersion()
h2o.clusterIsUp()
h2o.getConnection()
h2o.clusterStatus()
h2o.no_progress()  # Turn off progress bars
h2o.removeAll() # Cleans h2o cluster state.
options("h2o.use.data.table" = TRUE) #helpful for larger datasets
```

# Split into h2o training and h2o testing data sets ----
```{r}

# split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1978)
#No need to do split of train data into train and validations sets here because we are goig to run cross-validation

train_h2o <- as.h2o(train)
test_h2o  <- as.h2o(test)

# Take a look at the training set
h2o.describe(train_h2o) #enum is a categorical variable
h2o.hist(train_h2o$Age)
h2o.hist(train_h2o$USMLE_Step_1_Score)
```

# H2O AutoML Training ----
```{r}
start <- Sys.time()

y <- "Match_Status"  #Do not dummy the nominal variables early so that we can avoid changing the column names.  # Identify the response column

# Identify the predictor columns (remove response and ID column)
x <- setdiff(names(train_h2o), y)

#Execute an AutoML run
automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame = train_h2o, #training data set
  #validation_frame = valid_h2o, #grid tuning data set, done automatically by H2O.ai
  leaderboard_frame = test_h2o, #test data set, comment out to do cross-validation
  max_runtime_secs = 30, #originally was 60 secs #0 is no limit to time
  nfolds = 10, #K-fold cross-validation: duplicate the train data into 10 sets.  9 of the 10 are used for training and 1 of the 10 is used for validation.  Different parameters are being evaluate but NOT using a different model.  AUC is generated to measure model effectiveness for each fold and mean is used.
  seed = 1978,
  max_models = 10,
  verbosity = "info",
  keep_cross_validation_predictions = TRUE,
  balance_classes = FALSE,
  sort_metric = "AUC", #"logloss"
  project_name = "TylersLongModel"
)

automl_time <- Sys.time() - start
print(paste("Took", round(automl_time, digits = 2), units(gbm_time), "to build a automl model."))
```

```{r}
h2o.ls()
typeof(automl_models_h2o)
slotNames(automl_models_h2o)
```

# H2O AutoML Leaderboard ----
```{r}
automl_models_h2o@leaderboard
lb <- automl_models_h2o@leaderboard

# The leader model is stored at `automl_models_h2o@leader` and the leaderboard is stored at `automl_models_h2o@leaderboard`.
print(lb, n=nrow(lb)) #prints all rows instead of the standard 6 rows
```

# Look at the leading model ----
```{r}
automl_models_h2o@leader
```

# Extract the model name by position on the leaderboard ----
```{r}
args(extract_h2o_model_name_by_position)
extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=1, verbose = T)
# extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=2, verbose = T)
# extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=4, verbose = T)
# extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=5, verbose = T)
```

# Saving Model & Loading Model ----
```{r}
#Get the model name of the lead model
model_1st_position <- h2o.getModel(extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=1, verbose = T))

saveRDS(extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=1, verbose = T), "~/Dropbox/Nomogram/nomogram/04_Modeling/model_1st_position.rds")

h2o.download_mojo(automl_models_h2o@leader, path = "./") #Prep model for production

h2o.getModel(extract_h2o_model_name_by_position(automl_models_h2o@leaderboard, n=1, verbose = T)) %>% 
  h2o.saveModel(path = "04_Modeling/h2o_models/")
```


#Variable importance of leader model ----
```{r}
# We can make a simple plot that shows the relative importance of important predictors in the lead model
h2o.varimp_plot(automl_models_h2o@leader)
```


# Making Predictions with h2o model ----
```{r}
leader_model <- h2o.loadModel("04_Modeling/h2o_models/XGBoost_1_AutoML_20191214_142248")
leader_model

predictions <- h2o.predict(object = leader_model, newdata = as.h2o(test_tbl))
typeof(predictions)

predictions_tbl <- predictions %>% as.tibble()
predictions_tbl
```


# Visualizing The Leaderboard of Models ordered by AUC ----
```{r}
data_transformed <- automl_models_h2o@leaderboard %>% 
  as.tibble() %>%
  dplyr::mutate(model_type = str_split(model_id, "_", simplify = T)[,1]) %>%
  dplyr::slice(1:10) %>%
  rownames_to_column() %>%
  dplyr::mutate(
    model_id   = as_factor(model_id) %>% reorder(auc),
    model_type = as.factor(model_type)
  ) %>%
  tidyr::gather(key = key, value = value, -c(model_id, model_type, rowname), factor_key = T) %>%
  dplyr::mutate(model_id = paste0(rowname, ". ", model_id) %>% as_factor() %>% fct_rev()) 
data_transformed

data_transformed %>%
  ggplot(aes(value, model_id, color = model_type)) +
  geom_point(size = 3) +
  geom_label(aes(label = round(value, 2), hjust = "inward")) +
  facet_wrap(~ key, scales = "free_x") +
  #custom_theme () +
  #tidyquant::theme_tq() +
  tidyquant::scale_color_tq() +
  labs(title = "H2O Leaderboard Metrics",
       subtitle = paste0("Ordered by: auc"),
       y = "Model Postion, Model ID", x = "")
```

# Leaderboard is ordered by logloss ----
```{r}
#dev.off()
h2o_leaderboard <- automl_models_h2o@leaderboard
automl_models_h2o@leaderboard %>%
  plot_h2o_leaderboard(order_by = "logloss")
```


# Assessing Performance ----
```{r}
performance_h2o <- h2o.performance(model = leader_model, 
                                   newdata = as.h2o(test_tbl), 
                                   valid = FALSE)

typeof(performance_h2o)
performance_h2o %>% slotNames()
performance_h2o@metrics
```


# Classifier Summary Metrics ----
```{r}
h2o.auc(object = performance_h2o, train = T, valid = T, xval = F)
plot(performance_h2o) ## display ROC curve
h2o.giniCoef(object = performance_h2o, train = T, valid = T, xval = F)
h2o.logloss(object = performance_h2o, train = T, valid = T, xval = F)
```


# Confusion Matrix ----
```{r}
h2o.confusionMatrix(object = leader_model)
h2o.confusionMatrix(object = performance_h2o)
```

# Precision vs Recall Plot ----
```{r}
performance_tbl <- performance_h2o %>%
  h2o.metric() %>%
  as.tibble() 
performance_tbl

performance_tbl %>%
  filter(f1 == max(f1))

performance_tbl %>%
  ggplot(aes(x = threshold)) +
  geom_line(aes(y = precision), color = "blue", size = 1) +
  geom_line(aes(y = recall), color = "red", size = 1) +
  geom_vline(xintercept = h2o.find_threshold_by_max_metric(performance_h2o, "f1")) +
  #theme_tq() +
  labs(title = "Precision vs Recall", y = "value")
```


# ROC Plot ----
```{r}
model_metrics_tbl <- fs::dir_info(path = "04_Modeling/h2o_models/") %>%
  dplyr::select(path) %>%
  dplyr::mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%
  unnest()

model_metrics_tbl %>%
  dplyr::mutate(
    path = str_split(path, pattern = "/", simplify = T)[,3] %>% as_factor(),
    auc  = auc %>% round(3) %>% as.character() %>% as_factor()
  ) %>%
  ggplot(aes(fpr, tpr, color = path, linetype = auc)) +
  geom_line(size = 1) +
  #theme_tq() +
  scale_color_tq() +
  theme(legend.direction = "vertical") +
  labs(
    title = "ROC Plot",
    subtitle = "Performance of Top Performing Models"
  )
```


```{r}
model_metrics_tbl <- fs::dir_info(path = "04_Modeling/h2o_models/") %>%
  dplyr::select(path) %>%
  dplyr::mutate(metrics = map(path, load_model_performance_metrics, test_tbl)) %>%
  tidyr::unnest() 

model_metrics_tbl %>%
  dplyr::mutate(
    path = str_split(path, pattern = "/", simplify = T)[,3] %>% as_factor(),
    auc  = auc %>% round(3) %>% as.character() %>% as_factor()
  ) %>%
  ggplot(aes(recall, precision, color = path, linetype = auc)) +
  geom_line(size = 1) +
  #theme_tq() +
  scale_color_tq() +
  theme(legend.direction = "vertical") +
  labs(
    title = "Precision vs Recall Plot",
    subtitle = "Performance of Top Performing Models"
  )
```


# Gain & Lift ----
```{r}
ranked_predictions_tbl <- predictions_tbl %>%
  bind_cols(test_tbl) %>%
  dplyr::select(predict:Matched, Match_Status) %>%
  dplyr::arrange(desc(Matched))

calculated_gain_lift_tbl <- ranked_predictions_tbl %>%
  dplyr::mutate(ntile = ntile(Matched, n = 10)) %>%
  dplyr::group_by(ntile) %>%
  dplyr::summarise(
    cases = n(),
    responses = sum(Match_Status == "Matched")
  ) %>%
  dplyr::arrange(desc(ntile)) %>%
  dplyr::mutate(group = row_number()) %>%
  dplyr::select(group, cases, responses) %>%
  dplyr::mutate(
    cumulative_responses = cumsum(responses),
    pct_responses        = responses / sum(responses),
    gain                 = cumsum(pct_responses),
    cumulative_pct_cases = cumsum(cases) / sum(cases),
    lift                 = gain / cumulative_pct_cases,
    gain_baseline        = cumulative_pct_cases,
    lift_baseline        = gain_baseline / cumulative_pct_cases
  )

calculated_gain_lift_tbl 


gain_lift_tbl <- performance_h2o %>%
  h2o.gainsLift() %>%
  as.tibble()
```

# Lift Prep ----
```{r}
gain_transformed_tbl <- gain_lift_tbl %>% 
  dplyr::select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
  dplyr::select(-contains("lift")) %>%
  dplyr::mutate(baseline = cumulative_data_fraction) %>%
  dplyr::rename(gain = cumulative_capture_rate) %>%
  tidyr::gather(key = key, value = value, gain, baseline)

gain_transformed_tbl %>%
  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
  geom_line(size = 1.5) +
  #theme_tq() +
  scale_color_tq() +
  labs(
    title = "Gain Chart",
    x = "Cumulative Data Fraction",
    y = "Gain"
  )
```


# Lift ----
```{r}
lift_transformed_tbl <- gain_lift_tbl %>% 
  dplyr::select(group, cumulative_data_fraction, cumulative_capture_rate, cumulative_lift) %>%
  dplyr::select(-contains("capture")) %>%
  dplyr::mutate(baseline = 1) %>%
  dplyr::rename(lift = cumulative_lift) %>%
  tidyr::gather(key = key, value = value, lift, baseline)

lift_transformed_tbl %>%
  ggplot(aes(x = cumulative_data_fraction, y = value, color = key)) +
  geom_line(size = 1.5) +
  #theme_tq() +
  scale_color_tq() +
  labs(
    title = "Lift Chart",
    x = "Cumulative Data Fraction",
    y = "Lift"
  )

```

# Performance Visualization ----  
```{r}
h2o_leaderboard <- automl_models_h2o@leaderboard
```


# Performance Visualization with AUC ----  
```{r}
automl_models_h2o@leaderboard %>%
  plot_h2o_performance(newdata = test_tbl, order_by = "auc", 
                       size = 1, max_models = 3)
```

# Performance Visualization with logloss ----  
```{r}
automl_models_h2o@leaderboard %>%
  plot_h2o_performance(newdata = test_tbl, order_by = "logloss", 
                       size = 1, max_models = 3)
```

# Ensemble Exploration ----
```{r}
# # https://github.com/business-science/presentations/blob/master/2019_02_13_Learning_Lab_Marketing_Analytics/marketing_h2o_automl.R
# 
# # To understand how the ensemble works, let's take a peek inside the Stacked Ensemble "All Models" model.  
# # The "All Models" ensemble is an ensemble of all of the individual models in the AutoML run.  
# # This is often the top performing model on the leaderboard.
# 
# # Get model ids for all models in the AutoML Leaderboard
# model_ids <- as.data.frame(automl_models_h2o@leaderboard$model_id)[,1]
# # Get the "All Models" Stacked Ensemble model
# se <- h2o.getModel(grep("StackedEnsemble_AllModels", model_ids, value = TRUE)[1])
# # Get the Stacked Ensemble metalearner model
# metalearner <- h2o.getModel(se@model$metalearner$name)
# 
# # Examine the variable importance of the metalearner (combiner) algorithm in the ensemble.  
# # This shows us how much each base learner is contributing to the ensemble. The AutoML Stacked Ensembles 
# # use the default metalearner algorithm (GLM with non-negative weights), so the variable importance of the 
# # metalearner is actually the standardized coefficient magnitudes of the GLM. 
# h2o.varimp(metalearner)
# 
# # We can also plot the base learner contributions to the ensemble.
# h2o.varimp_plot(metalearner)
# 
# # Variable Importance ----
# 
# # Now let's look at the variable importance on the training set using the top XGBoost model
# # (Stacked Ensembles don't have variable importance yet)
# xgb <- h2o.getModel(grep("XGBoost", model_ids, value = TRUE)[1])
# 
# # Examine the variable importance of the top XGBoost model
# h2o.varimp(xgb)
# 
# # We can also plot the base learner contributions to the ensemble.
# h2o.varimp_plot(xgb)
```


# LIME ----
```{r}
#https://github.com/business-science/presentations/blob/master/2019_10_10_lab_20_explainable_machine_learning/Lab_20_-_Explaining_Machine_Learning.pdf
# Making Predictions ----

automl_leader <- automl_models_h2o@leader
predictions_tbl <- automl_leader %>% 
  h2o.predict(newdata = as.h2o(test_tbl)) %>%
  as.tibble() %>%
  bind_cols(
    test_tbl %>%
      select(Match_Status, Age, USMLE_Step_1_Score, US_or_Canadian_Applicant)
  )
predictions_tbl


test_tbl %>%
  dplyr::slice(5) %>%
  glimpse()

```


# Single Explanation ----
```{r}
explainer <- train_tbl %>%
  select(-Match_Status) %>%
  lime(
    model           = automl_leader,
    bin_continuous  = TRUE,
    n_bins          = 4,
    quantile_bins   = TRUE
  )
summary(explainer)
 
explanation <- test_tbl %>%
  dplyr::slice(5) %>%
  select(-Match_Status) %>%
  lime::explain(
    explainer = explainer,
    dist_fun = "gower", #Distance function
    n_labels   = 1,
    n_features = 8,
    n_permutations = 5000,
    kernel_width   = 1,
    feature_select = "highest_weights" #select features with ridge regression
  )

explanation %>%
  as.tibble() %>%
  dplyr::select(feature:prediction) 

#Graph of model reliability
explanation %>%
  ggplot(aes(x = model_r2, fill = label)) +
  geom_density(alpha = 0.5) +
  labs (title = "Model Reliability")

plot_features(explanation = explanation, ncol = 1)
```


# Tune LIME algorithm ----
```{r}
#https://uc-r.github.io/lime
explanation_tuned <- test_tbl %>%
  dplyr::slice(5) %>%
  select(-Match_Status) %>%
  lime::explain(
    explainer = explainer,
    dist_fun = "manhattan", #Distance function
    n_labels   = 1,
    n_features = 8,
    n_permutations = 5000,
    kernel_width   = 3,
    feature_select = "lasso_path" #Fit a lasso model and choose the n_features whose lars path converge to zero the latest.
  )
plot_features(explanation_tuned)
#Tune increases explanation fit 1 point to 0.69
```


# Multiple Explanations ----
```{r}
explanation <- test_tbl %>%
  dplyr::slice(1:10) %>%
  dplyr::filter(Match_Status == "Matched") %>%
  dplyr::select(-Match_Status) %>%
  lime::explain(
    explainer = explainer,
    n_labels   = 1,
    n_features = 8,
    n_permutations = 5000,
    kernel_width   = 1
  )

explanation %>%
  as.tibble()
#https://engineering.linecorp.com/ja/blog/neural-networks-lime-yardstick/
#https://www.r-bloggers.com/exploring-models-with-lime/
lime::plot_features(explanation, ncol = 1) +
  labs (title = " Feature Importance Visualization",
        subtitle = "Hold Out (Test) Set, First 12 Cases Shown")


lime::plot_explanations(explanation) +
  labs(title = "LIME Feature Importance Heatmap",
       subtitle = "Hold Out (Test) Set, First 12 Cases Shown") # filtered only people who matched and this was clearer.  
#You can easily show the most important variables for each of our selection methods above and we can see that they are all very consistent in the choice of the top 2-3 most influential variables in predicting matching.  

#Cross-check the lime results with correllation plot
all_data_binarized_tbl <- test_tbl %>%
  correlationfunnel::binarize(n_bins = 4, thresh_infreq = 0.01)

all_data_binarized_tbl %>% glimpse()

all_data_correlated_tbl <- all_data_binarized_tbl %>%
  correlationfunnel::correlate(target = Match_Status__Matched)

all_data_correlated_tbl

all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = TRUE, limits = c(-1, 1))

all_data_correlated_tbl %>%
  correlationfunnel::plot_correlation_funnel(interactive = FALSE, limits = c(-1, 1))
#https://www.r-bloggers.com/deep-learning-with-keras-to-predict-customer-churn/
```

```{r}
#Deploy the Model as an API if needed
#https://www.r-bloggers.com/are-you-leaking-h2o-call-plumber/

# source("api_test.R")
# rapi <- plumber::plumb("api_test.R")  # Where 'plumber.R' is the location of the code file shown above 
# rapi$run(port=8000)
```

```{r}
h2o.shutdown(prompt = FALSE)
```



```{r}
#Execute an AutoML run

automl_models_h2o <- h2o.automl(
  x = x,
  y = y,
  training_frame = train_h2o, #training data set
  #validation_frame = valid_h2o, #grid tuning data set, done automatically by H2O.ai
  leaderboard_frame = test_h2o, #test data set, comment out to do cross-validation
  max_runtime_secs = 30, #originally was 60 secs #0 is no limit to time
  nfolds = 10, #K-fold cross-validation: duplicate the train data into 10 sets.  9 of the 10 are used for training and 1 of the 10 is used for validation.  Different parameters are being evaluate but NOT using a different model.  AUC is generated to measure model effectiveness for each fold and mean is used.
  seed = 1978,
  max_models = 10,
  verbosity = "info",
  keep_cross_validation_predictions = TRUE,
  balance_classes = FALSE,
  sort_metric = "AUC", #"logloss"
  project_name = "TylersLongModel"
)


```







# Split into h2o training and h2o testing data sets ----
```{r}

# split_h2o <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1978)
#No need to do split of train data into train and validations sets here because we are goig to run cross-validation

train_h2o <- as.h2o(train)
test_h2o  <- as.h2o(test)

# Take a look at the training set
h2o.describe(train_h2o) #enum is a categorical variable
h2o.hist(train_h2o$Age)
h2o.hist(train_h2o$USMLE_Step_1_Score)
```



# Individual Models: Data Prep
```{r}
h2o.init()

data.split <- h2o.splitFrame(as.h2o(train_tbl), ratios = c(0.85), seed = 1978)
#No need to do split of train data into train and validations sets here because we are goig to run cross-validation

data.train <- data.split[[1]]
data.test <- data.split[[2]]

myY <- ("Match_Status")  #Do not dummy the nominal variables early so that we can avoid changing the column names.  # Identify the response column

# Identify the predictor columns (remove response and ID column)
myX <- setdiff(names(train_h2o), y)
```


```{r}
## Build GLM
start    <- Sys.time()
data.glm <- h2o.glm(y = myY, x = myX, training_frame = data.train, validation_frame = data.test, family = "binomial",
                    standardize=T, 
                    model_id = "glm_model", alpha = 0.5, lambda = 1e-05)
glm_time <- Sys.time() - start
print(paste("Took", round(glm_time, digits = 2), units(glm_time), "to build logistic regression model."))

## Build GBM Model
start    <- Sys.time()
data.gbm <- h2o.gbm(y = myY, x = myX, 
                    balance_classes = T, 
                    training_frame = data.train, 
                    #validation_frame = data.test,
                    ntrees = 100, max_depth = 5, model_id = "gbm_model", 
                    distribution = "bernoulli", learn_rate = .1,
                    min_rows = 2)
gbm_time <- Sys.time() - start
print(paste("Took", round(gbm_time, digits = 2), units(gbm_time), "to build a GBM model."))

## Build Random Forest Model
start    <- Sys.time()
data.drf <- h2o.randomForest(y = myY, x = myX, training_frame = data.train, validation_frame = data.test, ntrees = 150,
                             max_depth = 5, model_id = "drf_model", balance_classes = T)
drf_time <- Sys.time() - start
print(paste("Took", round(drf_time, digits = 2), units(drf_time), "to build a Random Forest model."))

## Build Deep Learning Model
start   <- Sys.time()
data.dl <- h2o.deeplearning(y = myY, x = myX, training_frame = data.train, validation_frame = data.test, hidden=c(10, 10),
                            epochs = 5, balance_classes = T, loss = "Automatic", variable_importances = T)
dl_time <- Sys.time() - start
print(paste("Took", round(dl_time, digits = 2), units(dl_time), "to build a Deep Learning model."))

## Variable Importance - For feature selection and rerunning a model build
print("GLM: Sorted Standardized Coefficient Magnitudes To Find Nonzero Coefficients")
data.glm@model$standardized_coefficient_magnitudes
print("GBM: Variable Importance")
data.gbm@model$variable_importances
print("Random Forest: Variable Importance")
data.drf@model$variable_importances
print("Deep Learning: Variable Importance")
data.dl@model$variable_importances
```

